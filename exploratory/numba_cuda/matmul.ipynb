{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43ec3352-4fc8-47b0-8e8a-32150f4c7094",
   "metadata": {},
   "source": [
    "In this notebook, I'll work through [this guide by Simon Boehm on fast matmul](https://siboehm.com/articles/22/CUDA-MMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f295efc-d39e-4b88-9f57-cf3e146eeb66",
   "metadata": {},
   "source": [
    "For simplicity, I'll use cuda from numba. According to [its documentation](https://numba.readthedocs.io/en/stable/cuda/overview.html#missing-cuda-features) it misses some cuda feature, but these features are not required for this guide:\n",
    "- [dynamic parallelism](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-dynamic-parallelism): call kernels function from other kernel functions\n",
    "- [texture memory](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-and-surface-memory): a special memory for graphics applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c18cc-8ffd-4f7a-8137-154060ff74d0",
   "metadata": {},
   "source": [
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c86aa1-0e2a-4595-803e-f09d4e9bdf1b",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72431b4f-4f00-4635-a17d-e58813d0c99d",
   "metadata": {},
   "source": [
    "### Prep 1 - Using numba cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca3bde69-e3d3-4592-897e-a40ccdf63657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fastcore.basics import tuplify\n",
    "\n",
    "#os.environ['NUMBA_ENABLE_CUDASIM']='1'  # enables simulator \n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "\n",
    "from numba import cuda, float32\n",
    "from util import to_d, to_h, array_like, cdiv\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: f'{x:.1f}'}, linewidth=200)\n",
    "\n",
    "dtype = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6ecdaa-5646-4a0b-b040-e167549de2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def f(a, b, c):\n",
    "    tid = cuda.grid(1) # like threadIdx.x + (blockIdx.x * blockDim.x)\n",
    "    if tid >= len(c): return\n",
    "    c[tid] = a[tid] + b[tid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963aa9c6-57a7-496b-8bbb-7e06e45b840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "a = to_d(np.ones(n, dtype=dtype))\n",
    "b = to_d(np.ones(n, dtype=dtype))\n",
    "c = array_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9a5ef3-bb93-4d78-b0fa-ece88f1a13e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nthreads = 8\n",
    "nblocks = (len(a) // nthreads) + 1\n",
    "f[nblocks, nthreads](a, b, c)\n",
    "to_h(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65557bc5-2789-4e6b-93f9-616c84da31c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7f079f1-60da-4a7f-8c60-210065b5d8e6",
   "metadata": {},
   "source": [
    "### Prep 2 - Profiling numba cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780f4dc-9280-4818-9020-2b3d749bb521",
   "metadata": {},
   "source": [
    "There are 2 profiling options:\n",
    "1. **Simple:** This measures the cuda runtime of a kernel\n",
    "2. **Detailled:** This performs a detailled analysis of the kernel run, and gives hints how the kernel could be improved\n",
    "\n",
    "I can do **simple** profiling in a notebook, which I'll do.\n",
    "\n",
    "For **detailled** profiling, I can use 'nsight compute' (a program by nvidia; short name 'ncu'), which requires the code to be in a separate file, so can't be done in jupyter.\n",
    "\n",
    "I can then use `ncu --set full --import-source yes -f -o <output_location> --page details python3 <python_filename>` for profiling. Then I can either look a the the console output for a quick analysis, or load the output into the nsight compute ui program (which I've downloaded) for more details.\n",
    "\n",
    "_Note: I could do detailled profiling in jupyter with `%%writefile <py_filename>`, `%run  <py_filename>` and `!ncu ...`, but I don't see the benefit._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a35bccdd-3644-431d-a769-0bcb28c22fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import measure_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac6177-9027-4c7f-9cf7-d918594b6c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5f8125f-3997-4885-bdb7-5f4681f05319",
   "metadata": {},
   "source": [
    "### Kernel 1: Naive Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fdecc2a-1a28-45c8-97cc-4cecd2ec3911",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit()\n",
    "def matmul_1(a,b,c,m,n,k):\n",
    "    x,y = cuda.grid(2)\n",
    "    if x >= m or y >= n: return \n",
    "    tmp = np.float32(0)\n",
    "    for i in range(k): tmp += a[x,i] * b[i,y]\n",
    "    c[x, y] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2830abf8-a49e-463c-b933-147dc127aa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2), (1, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,k,n = 2,3,4\n",
    "\n",
    "a = to_d(np.ones((m,k), dtype='float32'))\n",
    "b = to_d(np.ones((k,n), dtype='float32'))\n",
    "c = to_d(np.empty((m,n), dtype='float32'))\n",
    "\n",
    "nthreads = (2,2)\n",
    "nblocks = cdiv(c.shape, nthreads)\n",
    "nthreads, nblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "530a3b39-5203-4af2-b466-61e2d1eeb88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 2 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.0, 3.0, 3.0, 3.0],\n",
       "       [3.0, 3.0, 3.0, 3.0]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_1[nblocks, nthreads](a,b,c,m,n,k)\n",
    "to_h(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451cad0a-5eae-4187-8cf8-545eda26f5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf8c54a-f543-4f14-a0a3-f355d8b265d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring runtime of kernel matmul_1 for m,n,k = 4092,4092,4092, averaging over 3 runs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 14:15:16 117062:117062 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-05 14:15:20 117062:117062 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-05 14:15:20 117062:117062 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.242s\n"
     ]
    }
   ],
   "source": [
    "measure_runtime(matmul_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4719d4-4ebe-428a-b030-d1ab1399f84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92a5452d-8c89-4354-bfa5-1c8945b9e385",
   "metadata": {},
   "source": [
    "### Kernel 2: Global Memory Coalescing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da118d-1cec-43a4-840c-a486782ea562",
   "metadata": {},
   "source": [
    "We're doing naive matmul: A @ B = C.<br/>\n",
    "Sizes are (3,4) @Â (4,5) = (3,5)\n",
    "\n",
    "Let's say we have threads 0,1 and 6,7; where 0/1 and 6/7 are consecutive respectively.<br/>\n",
    "Remember, consecutive threads are (likely) in the same warp and can pool data loads. For simplicity, let's say warps are size 2 (eg threads 0/1 form a warp; threads 6/7 form a warp).<br/>\n",
    "Also, note the consecutiveness is determined by the cuda runtime; we can't influence it and it doesn't depend on the computation.<br/>\n",
    "    What we *can* decide is how threads are mapped to computation pieces. A smart choice can save data reads.\n",
    "\n",
    "Let's assume the input matrices A and B are in row-major ordering, which is very often the case.\n",
    "\n",
    "To compute C[row, col] we need A[row, :] and B[:, col].\n",
    "In row-major ordering, A[row, :] is contiguous in memory, while B[:, col] is *not*.\n",
    "\n",
    "We know gpus read memory in 'bursts'. For simplicity, let's say the burst size is 3.\n",
    "\n",
    "This means eg if we ask for A[0,0], we get A[0,0:2].\n",
    "\n",
    "So if we ask for A[0,:] = A[0,0:5] we get A[0,0:3] & A[0,3:6]. Here is the colloquial 'and'. (Also A[0,4:6] don't exist, so will be effectively random).<br/>\n",
    "-> We need 4 values, but are forced to read 2*3=6.\n",
    "\n",
    "If we ask for B[:,0] = B[0:5,0] we get B[0,0:3] & B[1,0:3] & B[2,0:3] & B[3,0:3] & B[4,0:3].<br/>\n",
    "-> We need 4 values, but are forced to read 4*3=12.\n",
    "\n",
    "So reading a col of B is way more wasteful than reading a row of A.<br/>\n",
    "Can we smartly assign threads to computation pieces in order to use this waste? Turns out: Yes!\n",
    "\n",
    "Let's think through 2 options:<br/>\n",
    "\n",
    "    1) If consecutive threads share the same row (eg warp 0 [consisting of threads 0/1] is assigned to compute C[0,0]/C[0,1]),\n",
    "        the warp needs values A[0,:] and B[:,0:2].\n",
    "        Because of bursting, it'll read:\n",
    "            A[0,0:3]\n",
    "            A[0,3:6]\n",
    "            B[0,0:3]\n",
    "            B[1,0:3]\n",
    "            B[2,0:3]\n",
    "            B[3,0:3]\n",
    "            B[4,0:3]\n",
    "        Not that waste in B is actually used here, becase the warp needs data from different cols.\n",
    "        In total, we're 7 reads (each a burst, so of size 3). \n",
    "\n",
    "    2) If consecutive threads share the same col (eg warp 0 [consisting of threads 0/1] is assigned to compute C[0,0]/C[1,0]),\n",
    "        the warp needs values A[0:2,:] and B[:,0].\n",
    "        Because of bursting, it'll read:\n",
    "            A[0,0:3]\n",
    "            A[0,3:6]\n",
    "            A[0,0:3]\n",
    "            A[0,3:6]\n",
    "            B[0,0:3]\n",
    "            B[1,0:3]\n",
    "            B[2,0:3]\n",
    "            B[3,0:3]\n",
    "            B[4,0:3]\n",
    "        In total, we're 9 reads (each a burst, so of size 3). \n",
    "'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ddc0c5-33e7-46a4-9af6-fb74cf62172d",
   "metadata": {},
   "source": [
    "Cuda groups a bunch of 'consecutive' threads into a group, called a 'warp'. This is useful, because threads in a warp can pool their data reads. E.g., if threads 1 and 2 are in the same warp, and thread 1 needs to read data from locations x and y, and thread 2 from location y and z, the warp only needs to do 3 reads (x,y,z) instead of 4 (x,y, y,z)!\n",
    "\n",
    "Different blocks are completely independent. However, inside a block, cuda has a notion of 'consecutiveness' by assigning a thread_id (tid) to each thread, like so:\n",
    "\n",
    "`tid = tidx + tidy * bsx + tidz * bsx * bsy`.\n",
    "\n",
    "Note that to increase `tid` by 1, you'd have to increase `tidx` by 1, not `tidy` or `tidz`. This means threads with consecutive `tidx`s are very likely to be consecutive. If we can make it so threads with consecutive `tidx`s need to read identical (over at least strongly overlapping) data, the pooled data read can save a lot of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f5655-0d3d-4e14-9954-b58052fd236c",
   "metadata": {},
   "source": [
    "**So we want consecutive threads to compute the same row!**\n",
    "\n",
    "In the above naive implementation, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64adef7d-8857-453c-a3f5-f2364e1a453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit()\n",
    "def matmul_2(a,b,c,m,n,k,bs):\n",
    "    # we defined blocks of size bs*bs\n",
    "    x = cuda.blockIdx.x * bs + (cuda.threadIdx.x // bs)\n",
    "    y = cuda.blockIdx.y * bs + (cuda.threadIdx.x % bs)\n",
    "    if x>=m or y>=n: return \n",
    "    tmp = np.float32(0)\n",
    "    for i in range(k): tmp += a[x,i] * b[i,y]\n",
    "    c[x, y] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5369b0c2-6172-4c2e-b1cc-aef2e9580b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, (1, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,k,n = 2,3,4\n",
    "\n",
    "a = to_d(np.ones((m,k), dtype='float32'))\n",
    "b = to_d(np.ones((k,n), dtype='float32'))\n",
    "c = to_d(np.empty((m,n), dtype='float32'))\n",
    "\n",
    "bs = 2\n",
    "nthreads = bs*bs\n",
    "nblocks = cdiv(c.shape, (bs,bs))\n",
    "nthreads, nblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c898fcc3-471a-48df-9df3-6288c81f24bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 2 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.0, 3.0, 3.0, 3.0],\n",
       "       [3.0, 3.0, 3.0, 3.0]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_2[nblocks, nthreads](a,b,c,m,n,k,bs)\n",
    "to_h(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ace62-d278-4e3c-bb9c-f9f72bcbd1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c64e3b-4199-48b6-af26-d0738dfb9e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring runtime of kernel matmul_2 for m,n,k = 4092,4092,4092, averaging over 3 runs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 14:15:22 117062:117062 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-05 14:15:24 117062:117062 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-05 14:15:24 117062:117062 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558s\n"
     ]
    }
   ],
   "source": [
    "measure_runtime(\n",
    "    matmul_2,\n",
    "    nthreads=32*32,nblocks_fn=lambda outp_shape,nthreads: cdiv(outp_shape, (32,32)),\n",
    "    kernel_args=[32]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5dbd5-9745-4aa2-aa6d-dc586b0d44df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a2cb3cf-9f65-4870-984a-2dc97a5a3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit()\n",
    "def matmul_2_bs32(a,b,c,m,n,k):\n",
    "    bs = 32\n",
    "    # we defined blocks of size bs*bs\n",
    "    x = cuda.blockIdx.x * bs + (cuda.threadIdx.x // bs)\n",
    "    y = cuda.blockIdx.y * bs + (cuda.threadIdx.x % bs)\n",
    "    if x>=m or y>=n: return \n",
    "    tmp = np.float32(0)\n",
    "    for i in range(k): tmp += a[x,i] * b[i,y]\n",
    "    c[x, y] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c65cae8-e8df-42b5-92c2-dac2a5df96d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring runtime of kernel matmul_2_bs32 for m,n,k = 4092,4092,4092, averaging over 3 runs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 14:26:01 117062:117062 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-05 14:26:03 117062:117062 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-05 14:26:03 117062:117062 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558s\n"
     ]
    }
   ],
   "source": [
    "measure_runtime(\n",
    "    matmul_2_bs32,\n",
    "    nthreads=32*32,nblocks_fn=lambda outp_shape,nthreads: cdiv(outp_shape, (32,32)),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b84122b0-6414-4bfe-91f8-fc2682358991",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit()\n",
    "def matmul_2__bs32__no_premature_return(a,b,c,m,n,k):\n",
    "    bs = 32\n",
    "    # we defined blocks of size bs*bs\n",
    "    x = cuda.blockIdx.x * bs + (cuda.threadIdx.x // bs)\n",
    "    y = cuda.blockIdx.y * bs + (cuda.threadIdx.x % bs)\n",
    "    if (x<m and y<n): \n",
    "        tmp = np.float32(0)\n",
    "        for i in range(k): tmp += a[x,i] * b[i,y]\n",
    "        c[x, y] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8963623-6fc3-4670-86fb-b08a571557ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring runtime of kernel matmul_2__bs32__no_premature_return for m,n,k = 4092,4092,4092, averaging over 3 runs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 14:38:29 117062:117062 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-05 14:38:31 117062:117062 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-05 14:38:31 117062:117062 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.557s\n"
     ]
    }
   ],
   "source": [
    "measure_runtime(\n",
    "    matmul_2__bs32__no_premature_return,\n",
    "    nthreads=32*32,nblocks_fn=lambda outp_shape,nthreads: cdiv(outp_shape, (32,32)),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037668bd-8705-4434-b99b-b578d5b735d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e74b7e1-43a9-4d1f-a8ad-97711e87f4b0",
   "metadata": {},
   "source": [
    "### Kernel 3: Shared Memory Cache-Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b9622-f1c6-4806-b3ef-2bc53c7469ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ff1f0bf-3d61-44d7-b7ee-cbcb8867e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit()\n",
    "def matmul_3_bs2(a,b,c,m,n,k):\n",
    "    bs = 2 # shared memory requires compile time constanst, so sadly can't use a bs variable\n",
    "    # index of current block\n",
    "    bx,by = cuda.blockIdx.x, cuda.blockIdx.y\n",
    "    # index of current thread inside block\n",
    "    tx,ty = cuda.threadIdx.x//bs, cuda.threadIdx.x%bs\n",
    "    # Declare 2 matrices of shape (bs,bs) in shared mem, they'll hold a piece of a/b.\n",
    "    sh_a, sh_b = cuda.shared.array((bs,bs), float32), cuda.shared.array((bs,bs), float32)        \n",
    "    tmp = np.float32(0)\n",
    "    # split k dimension into chunks of size bs, and iterate tru them\n",
    "    nk = (k+bs-1)//bs # = cdiv(k,bs)\n",
    "    for bk in range(nk):\n",
    "        # copy a piece of a & b into shared mem (if we're not out of bounds), and wait until all threads have done so (otherwise computation below will start with wrong data)\n",
    "        if (bx*bs+tx<m) and (bk*bs+ty<k) and (bk*bs+tx<k) and (by*bs+ty<n):\n",
    "            sh_a[tx,ty] = a[bx*bs+tx, bk*bs+ty] # = indexing into grid at [bx,bk] to get block, then indexing into block at [tx,ty] to get thread\n",
    "            sh_b[tx,ty] = b[bk*bs+tx, by*bs+ty] # = indexing into grid at [bk,by] to get block, then indexing into block at [tx,ty] to get thread\n",
    "        cuda.syncthreads()\n",
    "        # compute dot product, and wait until all computation is done (otherwise data copy in next iter will overwrite values we need)\n",
    "        for i in range(bs):\n",
    "            if (bx*bs+tx <m) and (bk*bs+i<k) and (by*bs+ty<n):\n",
    "                tmp += sh_a[tx,i]*sh_b[i,ty]\n",
    "        cuda.syncthreads()\n",
    "    # write output\n",
    "    if bx*bs+tx<m and by*bs+ty<n: c[bx*bs+tx,by*bs+ty] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6b9f152-cf81-44fa-982e-3262aabfe8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, (1, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,k,n = 2,3,4\n",
    "\n",
    "a = to_d(np.ones((m,k), dtype='float32'))\n",
    "b = to_d(np.ones((k,n), dtype='float32'))\n",
    "c = to_d(np.empty((m,n), dtype='float32'))\n",
    "\n",
    "bs = 2\n",
    "nthreads = bs*bs\n",
    "nblocks = cdiv(c.shape, (bs,bs))\n",
    "nthreads, nblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f9e9ff-09b1-4274-98b8-ccec924781ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 2 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.0, 3.0, 3.0, 3.0],\n",
       "       [3.0, 3.0, 3.0, 3.0]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_3_bs2[nblocks, nthreads](a,b,c,m,n,k)\n",
    "to_h(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189085a2-d677-4888-9019-dffbfc14fac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70880612-56bb-415a-aaa2-e8aac2ab0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit()\n",
    "def matmul_3_bs32(a,b,c,m,n,k):\n",
    "    bs = 32\n",
    "    bx,by = cuda.blockIdx.x, cuda.blockIdx.y\n",
    "    tx,ty = cuda.threadIdx.x//bs, cuda.threadIdx.x%bs\n",
    "    sh_a, sh_b = cuda.shared.array((bs,bs), float32), cuda.shared.array((bs,bs), float32)        \n",
    "    tmp = np.float32(0)\n",
    "    nk = (k+bs-1)//bs\n",
    "    for bk in range(nk):\n",
    "        for i in range(bs):\n",
    "            if (bx*bs+tx<m) and (bk*bs+i<k) and (by*bs+ty<n):\n",
    "                sh_a[tx,ty] = a[bx*bs+tx, bk*bs+i ]\n",
    "                sh_b[tx,ty] = b[bk*bs+i , by*bs+ty]\n",
    "        cuda.syncthreads()\n",
    "        for i in range(bs):\n",
    "            if (bx*bs+tx <m) and (bk*bs+i<k) and (by*bs+ty<n):\n",
    "                tmp += sh_a[tx,i]*sh_b[i,ty]\n",
    "        cuda.syncthreads()\n",
    "    if bx*bs+tx<m and by*bs+ty<n: c[bx*bs+tx,by*bs+ty] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "031042f2-4feb-46db-8ae0-56541ed6e2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring runtime of kernel matmul_3_bs32 for m,n,k = 4092,4092,4092, averaging over 3 runs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 14:15:27 117062:117062 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-05 14:15:31 117062:117062 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-05 14:15:31 117062:117062 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.133s\n"
     ]
    }
   ],
   "source": [
    "measure_runtime(\n",
    "    matmul_3_bs32,\n",
    "    nthreads=32*32,nblocks_fn=lambda outp_shape,nthreads: cdiv(outp_shape, (32,32))\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8722f-7447-4422-ab9f-055853826b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b23f48a8-1c28-42d3-af3c-7739cd426888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring runtime of kernel matmul_3_bs32 for m,n,k = 4092,4092,4092, averaging over 3 runs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 14:15:34 117062:117062 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-05 14:15:37 117062:117062 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-05 14:15:37 117062:117062 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.135s\n"
     ]
    }
   ],
   "source": [
    "profile_log = measure_runtime(\n",
    "    matmul_3_bs32,\n",
    "    nthreads=32*32,nblocks_fn=lambda outp_shape,nthreads: cdiv(outp_shape, (32,32)),\n",
    "    return_log=True\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78421071-e375-4de1-b7d9-854b54e0ecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "cudapy::__main__::matmul_3_bs32[abi:v5][abi:cw51cXTL...         0.00%       0.000us         0.00%       0.000us       0.000us        3.406s        98.84%        3.406s        1.135s             3  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      39.983ms         1.16%      39.983ms      13.328ms             3  \n",
      "                                         cuLaunchKernel       100.00%        3.407s       100.00%        3.407s        1.136s       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                  cudaDeviceSynchronize         0.00%       7.000us         0.00%       7.000us       7.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.407s\n",
      "Self CUDA time total: 3.446s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(profile_log.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6c274-a600-4f9c-95ed-02ebb6d19b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf9488-f067-4866-8fae-69103eb3c675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894e666-d57e-4103-8b5b-2bc300c9ba0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2912eec7-2e46-4a83-b37b-3762fb8ca847",
   "metadata": {},
   "source": [
    "### Kernel 4: 1D Blocktiling for Calculating Multiple Results per Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3c36e2c-5fa2-420d-a5be-56658cac4e4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m()\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88033ce1-1e93-443f-b665-41513f5c78a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd66390-d9b3-4395-8846-1f5386e14f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd323e-9016-441f-85c8-4c2241be1c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c896036a-ed60-44b5-97ce-625524578986",
   "metadata": {},
   "source": [
    "### Kernel 5: Increasing Arithmetic Intensity via 2D Blocktiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f5f84a-a389-4648-924e-14a971465a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "193c5be6-0d67-45de-a19d-bbff8b56c309",
   "metadata": {},
   "source": [
    "### Kernel 6: Vectorize SMEM and GMEM Accesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a9e2e-292c-41d7-a833-b5e7188ab2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20e9596c-b111-415c-ade4-4731234e6ff4",
   "metadata": {},
   "source": [
    "### Where are kernels 7 & 8??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78841719-11af-4453-bd4f-98345eefc420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "506343f8-194e-405c-a396-b81a0b3d2bb4",
   "metadata": {},
   "source": [
    "### Kernel 9: Autotuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2eb51-ff8c-4d8d-a6e6-ba55ed551dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d19e526e-e981-473e-ad27-76d3fc423349",
   "metadata": {},
   "source": [
    "### Kernel 10: Warptiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f11845-2308-486d-b4cf-8da562132187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62c2eee5-ec07-45a8-abbb-6907fe5ac554",
   "metadata": {},
   "source": [
    "### wip - Kernel 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d4fb4-d3c4-4841-bb55-524f745dacfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
