{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a7cf208-1379-4804-8f55-cbb31f7bb7f9",
   "metadata": {},
   "source": [
    "In this notebook, I profile Simon's cuda-c kernels with the torch-profiler.\n",
    "\n",
    "**Result:** Using the torch-profiler gives ~same runtimes as using cuda-events. So this is not the reasons why the runtimes of my numba-cuda kernels are so large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70eafdb-4722-4cc4-a2d3-fbf74fa7e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastcore.basics import tuplify\n",
    "from numba import cuda\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch.profiler import schedule as profiler_schedule\n",
    "from torch import allclose, tensor\n",
    "\n",
    "from util import to_d, to_h, array_like, cdiv\n",
    "\n",
    "dtype = 'float32'\n",
    "\n",
    "os.makedirs('tmp', exist_ok=True) # for tmp file from load_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9c1dbc-ae30-48ae-845e-64fed415ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit()\n",
    "def matmul_2(a,b,c,m,n,k,bs):\n",
    "    # we defined blocks of size bs*bs\n",
    "    x = cuda.blockIdx.x * bs + (cuda.threadIdx.x // bs)\n",
    "    y = cuda.blockIdx.y * bs + (cuda.threadIdx.x % bs)\n",
    "    if x>=m or y>=n: return \n",
    "    tmp = 0\n",
    "    for i in range(k): tmp += a[x,i] * b[i,y]\n",
    "    c[x, y] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe9264-61bf-469c-9bba-49f6e3a4213a",
   "metadata": {},
   "source": [
    "This is the c-code from Simon, which runs at ~220ms instead of ~1270ms like my code:\n",
    "```cpp \n",
    "template <const uint BLOCKSIZE>\n",
    "__global__ void matmul_global_mem_coalesce(int M, int N, int K, float alpha,\n",
    "                                          const float *A, const float *B,\n",
    "                                          float beta, float *C) {\n",
    "  const int cRow = blockIdx.x * BLOCKSIZE + (threadIdx.x / BLOCKSIZE);\n",
    "  const int cCol = blockIdx.y * BLOCKSIZE + (threadIdx.x % BLOCKSIZE);\n",
    "\n",
    "  // if statement is necessary to make things work under tile quantization\n",
    "  if (cRow < M && cCol < N) {\n",
    "    float tmp = 0.0;\n",
    "    for (int i = 0; i < K; ++i) {\n",
    "      tmp += A[cRow * K + i] * B[i * N + cCol];\n",
    "    }\n",
    "    C[cRow * N + cCol] = tmp;\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276c5ab2-bbed-4fdc-adf8-6c1c168ccc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n,k = 4092,4092,4092\n",
    "\n",
    "a = to_d(np.ones((m,k), dtype=dtype))\n",
    "b = to_d(np.ones((k,n), dtype=dtype))\n",
    "c = to_d(np.empty((m,n), dtype=dtype))\n",
    "bs=32\n",
    "nthreads = bs*bs # 1d block ...\n",
    "nblocks = cdiv(c.shape, (bs,bs)) # ... in 2d grid \n",
    "matmul_2[nblocks, nthreads](a,b,c,m,n,k,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a0429e-5924-427b-8658-bc7543521da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 18:44:51 2445:2445 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "cudapy::__main__::matmul_2[abi:v1][abi:cw51cXTLSUwv1...         0.00%       0.000us         0.00%       0.000us       0.000us        1.258s        98.98%        1.258s        1.258s             1  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      12.944ms         1.02%      12.944ms      12.944ms             1  \n",
      "                                         cuLaunchKernel         0.00%      27.000us         0.00%      27.000us      27.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                     cudaGetDeviceCount         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                       cudaDeviceGetStreamPriorityRange         0.87%      10.851ms         0.87%      10.851ms      10.851ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaStreamIsCapturing         0.00%       2.000us         0.00%       2.000us       0.667us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                             cudaGetDeviceProperties_v2         0.01%     108.000us         0.01%     108.000us     108.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize        99.12%        1.232s        99.12%        1.232s        1.232s       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.243s\n",
      "Self CUDA time total: 1.271s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 18:44:53 2445:2445 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-05 18:44:53 2445:2445 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "# using the standard scheduler runs the code only once\n",
    "with profile(activities=[ProfilerActivity.CUDA]) as p:\n",
    "    c = to_d(np.empty((m,n), dtype=dtype))\n",
    "    matmul_2[nblocks, nthreads](a,b,c,m,n,k,bs)\n",
    "\n",
    "print(p.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23e43fd-aeb2-4b1d-bd42-df1d20edc1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "cudapy::__main__::matmul_2[abi:v1][abi:cw51cXTLSUwv1...         0.00%       0.000us         0.00%       0.000us       0.000us        1.258s        98.98%        1.258s        1.258s             1  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      12.944ms         1.02%      12.944ms      12.944ms             1  \n",
      "                                         cuLaunchKernel         0.00%      27.000us         0.00%      27.000us      27.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                     cudaGetDeviceCount         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                       cudaDeviceGetStreamPriorityRange         0.87%      10.851ms         0.87%      10.851ms      10.851ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaStreamIsCapturing         0.00%       2.000us         0.00%       2.000us       0.667us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                             cudaGetDeviceProperties_v2         0.01%     108.000us         0.01%     108.000us     108.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize        99.12%        1.232s        99.12%        1.232s        1.232s       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.243s\n",
      "Self CUDA time total: 1.271s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wait, warmup = 1,1 # 1 wait cycle to ensure kernel is compiled, 1 warmup cycle to not have overhead of profiler start afterwards\n",
    "runs = 3\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA], schedule=profiler_schedule(wait=1, warmup=1, active=runs)) as prof:\n",
    "    for _ in range(wait+warmup+runs):\n",
    "        c = to_d(np.empty((m,n), dtype=dtype))\n",
    "        matmul_2[nblocks, nthreads](a,b,c,m,n,k,bs)\n",
    "        p.step()\n",
    "\n",
    "print(p.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18f05f1-49a0-4fe4-823d-3fdef7541adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_mean_runtime(prof_log, kernel_name, do_print=False):\n",
    "    # extract cuda mean runtime from a torch.profiler log\n",
    "    kernels = [o for o in prof_log.key_averages() if kernel_name in o.key]\n",
    "    names = [k.key for k in kernels]\n",
    "    if len(names)==0: raise RuntimeError(f\"Profiling logs have no kernel with 'f{kernel_name}' in its name\")\n",
    "    if len(names)>1: raise RuntimeError(f\"Profiling logs have multiple kernel with 'f{kernel_name}' in its name: f{names}. Please be more precise.\")\n",
    "    \n",
    "    mean_runtime = kernels[0].cuda_time/1e3 # use ms instea of µs\n",
    "    if do_print: print(f'{mean_runtime/1e3:.3f}s') # print in s\n",
    "    return mean_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc13a25-948f-479d-ae36-2c81ab48942a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.258s\n"
     ]
    }
   ],
   "source": [
    "cuda_mean_runtime(p, 'matmul',do_print=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3268a5-a339-4428-8cc3-78b7f8a50703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44d76657-5a60-4ce9-8e68-690ae8098232",
   "metadata": {},
   "source": [
    "Let's check what profiling Simon's kernel with torch-profile returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0da00-3465-42fd-b993-3054b951dd42",
   "metadata": {},
   "source": [
    "Let's first if calling kernels from Python works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "828cdfaf-3b05-4e1f-9e11-f6d7d8d54716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5caaeb39-79cb-47b7-a386-1327be6a158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  4.,  9.],\n",
      "        [16., 25., 36.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Define the CUDA kernel and C++ wrapper\n",
    "cuda_src = '''\n",
    "__global__ void square_matrix_kernel(const float* matrix, float* result, int width, int height) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (row < height && col < width) {\n",
    "        int idx = row * width + col;\n",
    "        result[idx] = matrix[idx] * matrix[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor square_matrix(torch::Tensor matrix) {\n",
    "    const auto height = matrix.size(0);\n",
    "    const auto width = matrix.size(1);\n",
    "\n",
    "    auto result = torch::empty_like(matrix);\n",
    "\n",
    "    dim3 threads_per_block(16, 16);\n",
    "    dim3 number_of_blocks((width + threads_per_block.x - 1) / threads_per_block.x,\n",
    "                          (height + threads_per_block.y - 1) / threads_per_block.y);\n",
    "\n",
    "    square_matrix_kernel<<<number_of_blocks, threads_per_block>>>(\n",
    "        matrix.data_ptr<float>(), result.data_ptr<float>(), width, height);\n",
    "\n",
    "    return result;\n",
    "}\n",
    "'''\n",
    "cpp_src = \"torch::Tensor square_matrix(torch::Tensor matrix);\"\n",
    "\n",
    "# Load the CUDA kernel as a PyTorch extension\n",
    "square_matrix_extension = load_inline(\n",
    "    name='square_matrix_extension',\n",
    "    cpp_sources=cpp_src,\n",
    "    cuda_sources=cuda_src,\n",
    "    functions=['square_matrix'],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=[\"-O2\"],\n",
    "    build_directory='./tmp',\n",
    "    # extra_cuda_cflags=['--expt-relaxed-constexpr']\n",
    ")\n",
    "\n",
    "a = torch.tensor([[1., 2., 3.], [4., 5., 6.]], device='cuda')\n",
    "\n",
    "print(square_matrix_extension.square_matrix(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c426b37b-e13a-449a-b619-acda38480167",
   "metadata": {},
   "source": [
    "It does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f883275-3aac-4132-b45d-33b2ca9619b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52fb44c4-3c4d-4b33-95a7-7aa573f68033",
   "metadata": {},
   "source": [
    "Let's now run Simon's matmul kernel 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a07c6eed-1910-4a0a-9d60-0e199bfa629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = '''\n",
    "template <const uint BLOCKSIZE>\n",
    "__global__ void matmul_global_mem_coalesce(const float *A, const float *B, float *C, int M, int N, int K) {\n",
    "  const int cRow = blockIdx.x * BLOCKSIZE + (threadIdx.x / BLOCKSIZE);\n",
    "  const int cCol = blockIdx.y * BLOCKSIZE + (threadIdx.x % BLOCKSIZE);\n",
    "\n",
    "  if (cRow < M && cCol < N) {\n",
    "    float tmp = 0.0;\n",
    "    for (int i = 0; i < K; ++i) {\n",
    "      tmp += A[cRow * K + i] * B[i * N + cCol];\n",
    "    }\n",
    "    C[cRow * N + cCol] = tmp;\n",
    "  }\n",
    "}\n",
    "\n",
    "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
    "\n",
    "torch::Tensor matmul(torch::Tensor a, torch::Tensor b) {\n",
    "    constexpr uint bs = 32;\n",
    "    \n",
    "    //CHECK_INPUT(a); CHECK_INPUT(b);\n",
    "    int m = a.size(0);\n",
    "    int n = b.size(1);\n",
    "    int k = a.size(1);\n",
    "    //TORCH_CHECK(k==b.size(0), \"Size mismatch!\");\n",
    "    auto outp = torch::zeros({m, n}, a.options());\n",
    "\n",
    "    dim3 tpb(bs*bs);\n",
    "    dim3 blocks(cdiv(m, bs), cdiv(n, bs));\n",
    "    matmul_global_mem_coalesce<bs><<<blocks, tpb>>>(\n",
    "        a.data_ptr<float>(), b.data_ptr<float>(), outp.data_ptr<float>(),\n",
    "        m, n, k\n",
    "    );\n",
    "    //C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return outp;\n",
    "}\n",
    "'''\n",
    "cpp_src = \"torch::Tensor matmul(torch::Tensor a, torch::Tensor b);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3998aa36-15d8-419f-92a6-09ed7a35864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CUDA kernel as a PyTorch extension\n",
    "matmul_2 = load_inline(\n",
    "    name='matmul',\n",
    "    cpp_sources=cpp_src,\n",
    "    cuda_sources=cuda_src,\n",
    "    functions=['matmul'],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=[\"-O2\"],\n",
    "    build_directory='tmp/matmul_2/',\n",
    "    # extra_cuda_cflags=['--expt-relaxed-constexpr']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9112a74f-0d46-4574-8ecd-f8af2318e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((2,3), device='cuda')\n",
    "b = torch.ones((3,4), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f3c336b-92a6-4d69-a6e7-0e09884043f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3., 3.],\n",
       "        [3., 3., 3., 3.]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_2.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c70b059-96c4-4420-a02e-ef1d3e2ac861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b590fea-dd38-47d2-94b9-f0ea3dc04075",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n,k = 4092,4092,4092\n",
    "\n",
    "a = torch.ones((m,k), dtype=torch.float32, device='cuda')\n",
    "b = torch.ones((k,n), dtype=torch.float32, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72b9824c-db7e-4678-9356-f33b0fea409c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4092, 4092])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = matmul_2.matmul(a,b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccb339e8-95ac-4aa0-8b47-c1cb322cabdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a@b==c).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43b35b3a-cc00-49ed-9951-2998332a9b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 18:46:23 2445:2445 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "void matmul_global_mem_coalesce<32u>(float const*, f...         0.00%       0.000us         0.00%       0.000us       0.000us     894.970ms        99.87%     894.970ms     223.743ms             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.125ms         0.13%       1.125ms     281.250us             4  \n",
      "                                       cudaLaunchKernel         0.01%      62.000us         0.01%      62.000us      10.333us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                  cudaDeviceSynchronize        99.99%        1.107s        99.99%        1.107s        1.107s       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.107s\n",
      "Self CUDA time total: 896.095ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 18:46:24 2445:2445 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-05 18:46:24 2445:2445 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "wait, warmup = 1,1 # 1 wait cycle to ensure kernel is compiled, 1 warmup cycle to not have overhead of profiler start afterwards\n",
    "runs = 3\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA], schedule=profiler_schedule(wait=1, warmup=1, active=runs)) as p:\n",
    "    for _ in range(wait+warmup+runs):\n",
    "        matmul_2.matmul(a,b)\n",
    "        p.step()\n",
    "\n",
    "print(p.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ea227-af81-4bdf-a360-6d16b5e472f7",
   "metadata": {},
   "source": [
    "Simon's kernel 2 on a T4, measured with torch-profile gives **~200ms**. So the measurement method (torch profile vs cudaevents) is not the source of weirdness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038bdfb9-fee1-40d4-8ce0-6fd6111c1557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad9a3a2-79f1-4daa-84ca-f75f960c1fcf",
   "metadata": {},
   "source": [
    "Let's also measure his kernel 1 and 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5583cce9-58be-464a-88bb-fec11ab0ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel 1\n",
    "cuda_src = '''\n",
    "__global__ void matmul_naive(const float *A, const float *B, float *C, int M, int N, int K) {\n",
    "  const uint x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  const uint y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "\n",
    "  // if statement is necessary to make things work under tile quantization\n",
    "  if (x < M && y < N) {\n",
    "    float tmp = 0.0;\n",
    "    for (int i = 0; i < K; ++i) {\n",
    "      tmp += A[x * K + i] * B[i * N + y];\n",
    "    }\n",
    "    C[x * N + y] = tmp;\n",
    "  }\n",
    "}\n",
    "\n",
    "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
    "\n",
    "torch::Tensor matmul(torch::Tensor a, torch::Tensor b) {\n",
    "    constexpr uint bs = 32;\n",
    "    \n",
    "    //CHECK_INPUT(a); CHECK_INPUT(b);\n",
    "    int m = a.size(0);\n",
    "    int n = b.size(1);\n",
    "    int k = a.size(1);\n",
    "    //TORCH_CHECK(k==b.size(0), \"Size mismatch!\");\n",
    "    auto outp = torch::zeros({m, n}, a.options());\n",
    "\n",
    "    dim3 tpb(bs,bs);\n",
    "    dim3 blocks(cdiv(m, bs), cdiv(n, bs));\n",
    "    matmul_naive<<<blocks, tpb>>>(\n",
    "        a.data_ptr<float>(), b.data_ptr<float>(), outp.data_ptr<float>(),\n",
    "        m, n, k\n",
    "    );\n",
    "    //C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return outp;\n",
    "}\n",
    "'''\n",
    "cpp_src = \"torch::Tensor matmul(torch::Tensor a, torch::Tensor b);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01d55fa0-29f0-425b-8a09-705a19ddee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul_1 = load_inline(\n",
    "    name='matmul',\n",
    "    cpp_sources=cpp_src,\n",
    "    cuda_sources=cuda_src,\n",
    "    functions=['matmul'],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=[\"-O2\"],\n",
    "    build_directory='tmp/matmul_1/',\n",
    "    # extra_cuda_cflags=['--expt-relaxed-constexpr']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f20a105-088e-43d3-bfb0-027782eb0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n,k = 4092,4092,4092\n",
    "a = torch.ones((m,k), dtype=torch.float32, device='cuda')\n",
    "b = torch.ones((k,n), dtype=torch.float32, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97bcfdc5-60a1-46fe-91da-de3cb82fec2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4092, 4092])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = matmul_1.matmul(a,b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fbcb384-6393-463a-aced-6e84c5c4b9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a@b==c).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be874d9f-3de3-4a3b-b541-86e4f9b83a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 18:47:49 2445:2445 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "matmul_naive(float const*, float const*, float*, int...         0.00%       0.000us         0.00%       0.000us       0.000us        3.351s        99.97%        3.351s     837.869ms             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.120ms         0.03%       1.120ms     280.000us             4  \n",
      "                                       cudaLaunchKernel         0.00%      46.000us         0.00%      46.000us       7.667us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                  cudaDeviceSynchronize       100.00%        4.194s       100.00%        4.194s        4.194s       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.194s\n",
      "Self CUDA time total: 3.353s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 18:47:54 2445:2445 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-05 18:47:54 2445:2445 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "wait, warmup = 1,1 # 1 wait cycle to ensure kernel is compiled, 1 warmup cycle to not have overhead of profiler start afterwards\n",
    "runs = 3\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA], schedule=profiler_schedule(wait=1, warmup=1, active=runs)) as p:\n",
    "    for _ in range(wait+warmup+runs):\n",
    "        matmul_1.matmul(a,b)\n",
    "        p.step()\n",
    "\n",
    "print(p.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630f31d-d070-4d25-8e7c-77453a5e4464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf503ca5-74d0-4016-89aa-e9c3043d1cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f133ed8b-f892-443d-b250-416a7c252102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel 3\n",
    "cuda_src = '''\n",
    "template <const int BLOCKSIZE>\n",
    "__global__ void matmul_shared_mem_block(const float *A, const float *B, float *C, int M, int N, int K) {\n",
    "  // the output block that we want to compute in this threadblock\n",
    "  const uint cRow = blockIdx.x;\n",
    "  const uint cCol = blockIdx.y;\n",
    "\n",
    "  // allocate buffer for current block in fast shared mem\n",
    "  // shared mem is shared between all threads in a block\n",
    "  __shared__ float As[BLOCKSIZE * BLOCKSIZE];\n",
    "  __shared__ float Bs[BLOCKSIZE * BLOCKSIZE];\n",
    "\n",
    "  // the inner row & col that we're accessing in this thread\n",
    "  const uint threadCol = threadIdx.x % BLOCKSIZE;\n",
    "  const uint threadRow = threadIdx.x / BLOCKSIZE;\n",
    "\n",
    "  // advance pointers to the starting positions\n",
    "  A += cRow * BLOCKSIZE * K;                    // row=cRow, col=0\n",
    "  B += cCol * BLOCKSIZE;                        // row=0, col=cCol\n",
    "  C += cRow * BLOCKSIZE * N + cCol * BLOCKSIZE; // row=cRow, col=cCol\n",
    "\n",
    "  float tmp = 0.0;\n",
    "  for (int bkIdx = 0; bkIdx < K; bkIdx += BLOCKSIZE) {\n",
    "    // Have each thread load one of the elements in A & B\n",
    "    // Make the threadCol (=threadIdx.x) the consecutive index\n",
    "    // to allow global memory access coalescing\n",
    "    As[threadRow * BLOCKSIZE + threadCol] = A[threadRow * K + threadCol];\n",
    "    Bs[threadRow * BLOCKSIZE + threadCol] = B[threadRow * N + threadCol];\n",
    "\n",
    "    // block threads in this block until cache is fully populated\n",
    "    __syncthreads();\n",
    "    A += BLOCKSIZE;\n",
    "    B += BLOCKSIZE * N;\n",
    "\n",
    "    // execute the dotproduct on the currently cached block\n",
    "    for (int dotIdx = 0; dotIdx < BLOCKSIZE; ++dotIdx) {\n",
    "      tmp += As[threadRow * BLOCKSIZE + dotIdx] *\n",
    "             Bs[dotIdx * BLOCKSIZE + threadCol];\n",
    "    }\n",
    "    // need to sync again at the end, to avoid faster threads\n",
    "    // fetching the next block into the cache before slower threads are done\n",
    "    __syncthreads();\n",
    "  }\n",
    "  C[threadRow * N + threadCol] = tmp;\n",
    "}\n",
    "\n",
    "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
    "\n",
    "torch::Tensor matmul(torch::Tensor a, torch::Tensor b) {\n",
    "    constexpr uint bs = 32;\n",
    "    \n",
    "    //CHECK_INPUT(a); CHECK_INPUT(b);\n",
    "    int m = a.size(0);\n",
    "    int n = b.size(1);\n",
    "    int k = a.size(1);\n",
    "    //TORCH_CHECK(k==b.size(0), \"Size mismatch!\");\n",
    "    auto outp = torch::zeros({m, n}, a.options());\n",
    "\n",
    "    dim3 tpb(bs,bs);\n",
    "    dim3 blocks(cdiv(m, bs), cdiv(n, bs));\n",
    "    matmul_shared_mem_block<bs><<<blocks, tpb>>>(\n",
    "        a.data_ptr<float>(), b.data_ptr<float>(), outp.data_ptr<float>(),\n",
    "        m, n, k\n",
    "    );\n",
    "    //C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return outp;\n",
    "}\n",
    "'''\n",
    "cpp_src = \"torch::Tensor matmul(torch::Tensor a, torch::Tensor b);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "531f1286-4db6-4df8-baba-b5a21c030abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul_3 = load_inline(\n",
    "    name='matmul',\n",
    "    cpp_sources=cpp_src,\n",
    "    cuda_sources=cuda_src,\n",
    "    functions=['matmul'],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=[\"-O2\"],\n",
    "    build_directory='tmp/matmul_1/',\n",
    "    # extra_cuda_cflags=['--expt-relaxed-constexpr']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dc577ac-dbab-4bcd-9456-b015e11daf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n,k = 4092,4092,4092\n",
    "a = torch.ones((m,k), dtype=torch.float32, device='cuda')\n",
    "b = torch.ones((k,n), dtype=torch.float32, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "847a7d31-b831-45d3-98bb-23f2f51ecd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4092, 4092])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = matmul_3.matmul(a,b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b5adfd6-e186-4558-bd46-7d72d04f2b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a@b==c).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d8f0062-3b79-4583-8f83-b90498115f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 18:49:19 2445:2445 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "void matmul_shared_mem_block<32>(float const*, float...         0.00%       0.000us         0.00%       0.000us       0.000us     465.614ms        99.76%     465.614ms     116.403ms             4  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.123ms         0.24%       1.123ms     280.750us             4  \n",
      "                                       cudaLaunchKernel         0.01%      48.000us         0.01%      48.000us       8.000us       0.000us         0.00%       0.000us       0.000us             6  \n",
      "                                  cudaDeviceSynchronize        99.99%     584.409ms        99.99%     584.409ms     584.409ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 584.457ms\n",
      "Self CUDA time total: 466.737ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 18:49:19 2445:2445 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-05 18:49:19 2445:2445 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "wait, warmup = 1,1 # 1 wait cycle to ensure kernel is compiled, 1 warmup cycle to not have overhead of profiler start afterwards\n",
    "runs = 3\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA], schedule=profiler_schedule(wait=1, warmup=1, active=runs)) as p:\n",
    "    for _ in range(wait+warmup+runs):\n",
    "        matmul_3.matmul(a,b)\n",
    "        p.step()\n",
    "\n",
    "print(p.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549c37b-6b0a-43dd-aeca-27f52d77fe22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3749814d-2d92-4a86-a7e2-a05ad5435797",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8cb7a4-d491-4e6a-b7d8-f526b5ba0c54",
   "metadata": {},
   "source": [
    "To generate the ptx, I want to remove all dependencies on torch. Let's assert that code doesn't have a different runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "658d29db-3566-4182-a2ce-c459d75a05ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = '''\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "template <const uint BLOCKSIZE>\n",
    "__global__ void matmul_global_mem_coalesce(const float *A, const float *B, float *C, int M, int N, int K) {\n",
    "  const int cRow = blockIdx.x * BLOCKSIZE + (threadIdx.x / BLOCKSIZE);\n",
    "  const int cCol = blockIdx.y * BLOCKSIZE + (threadIdx.x % BLOCKSIZE);\n",
    "\n",
    "  if (cRow < M && cCol < N) {\n",
    "    float tmp = 0.0;\n",
    "    for (int i = 0; i < K; ++i) { tmp += A[cRow * K + i] * B[i * N + cCol]; }\n",
    "    C[cRow * N + cCol] = tmp;\n",
    "  }\n",
    "}\n",
    "\n",
    "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b; }\n",
    "\n",
    "void matmul(int M, int N, int K) {\n",
    "    constexpr uint bs = 32;\n",
    "\n",
    "    // Allocate memory for A,B,C on device\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    cudaMalloc((void **)&d_A, M * K * sizeof(float));\n",
    "    cudaMalloc((void **)&d_B, K * N * sizeof(float));\n",
    "    cudaMalloc((void **)&d_C, M * N * sizeof(float));\n",
    "\n",
    "    // Initialize A,B to ones\n",
    "    cudaMemset(d_A, 1, M * K * sizeof(float));\n",
    "    cudaMemset(d_B, 1, K * N * sizeof(float));\n",
    "\n",
    "    // Initialize C to zeros\n",
    "    cudaMemset(d_C, 0, M * N * sizeof(float));\n",
    "\n",
    "    // Configure the grid and block dimensions\n",
    "    dim3 tpb(bs * bs);\n",
    "    dim3 blocks(cdiv(M, bs), cdiv(N, bs));\n",
    "\n",
    "    // Launch the matrix multiplication kernel\n",
    "    matmul_global_mem_coalesce<bs><<<blocks, tpb>>>(d_A, d_B, d_C, M, N, K);\n",
    "\n",
    "    // Free device memory\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "}\n",
    "'''\n",
    "cpp_src = \"void matmul(int M, int N, int K);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5f27b32-239f-41ad-bf73-a5264974850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul_2_cudac_notorch = load_inline(\n",
    "    name='matmul',\n",
    "    cpp_sources=cpp_src,\n",
    "    cuda_sources=cuda_src,\n",
    "    functions=['matmul'],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=[\"-O2\"],\n",
    "    build_directory='tmp/matmul_2/',\n",
    "    # extra_cuda_cflags=['--expt-relaxed-constexpr']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8878810e-4ce3-433f-97a2-205f78c6fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n,k = 4092,4092,4092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0562f5eb-6e2f-4db8-8724-2511e8c55752",
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul_2_cudac_notorch.matmul(m,n,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35facefb-f6e0-4b0b-876c-a82733c5d186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 18:50:34 2445:2445 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "void matmul_global_mem_coalesce<32u>(float const*, f...         0.00%       0.000us         0.00%       0.000us       0.000us     665.441ms        99.56%     665.441ms     221.814ms             3  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us       2.919ms         0.44%       2.919ms     324.333us             9  \n",
      "                                             cudaMalloc         0.25%       1.669ms         0.25%       1.669ms     185.444us       0.000us         0.00%       0.000us       0.000us             9  \n",
      "                                             cudaMemset         0.02%     112.000us         0.02%     112.000us      12.444us       0.000us         0.00%       0.000us       0.000us             9  \n",
      "                                       cudaLaunchKernel         0.00%      31.000us         0.00%      31.000us      10.333us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                               cudaFree        99.73%     678.643ms        99.73%     678.643ms      75.405ms       0.000us         0.00%       0.000us       0.000us             9  \n",
      "                                  cudaDeviceSynchronize         0.00%       6.000us         0.00%       6.000us       6.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 680.461ms\n",
      "Self CUDA time total: 668.360ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-05 18:50:35 2445:2445 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-05 18:50:35 2445:2445 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "wait, warmup = 1,1 # 1 wait cycle to ensure kernel is compiled, 1 warmup cycle to not have overhead of profiler start afterwards\n",
    "runs = 3\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CUDA], schedule=profiler_schedule(wait=1, warmup=1, active=runs)) as p:\n",
    "    for _ in range(wait+warmup+runs):\n",
    "        matmul_2_cudac_notorch.matmul(m,n,k)\n",
    "        p.step()\n",
    "\n",
    "print(p.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540993eb-47d9-407c-8317-0e419aa6cd4c",
   "metadata": {},
   "source": [
    "Cool, still ~200ms. So the removing torch dependencies doesn't change the runtime of the kernel (as expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ea814b-6cc9-4935-a3fe-8b9777968c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
