{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b6df12-ec75-4289-8e26-f56e6c858ed5",
   "metadata": {},
   "source": [
    "Let's do a tiny example of using a custom triton kernel in a PyTorch model's forward and backward.\n",
    "\n",
    "Steps:\n",
    "1. Define tiny model in pure pytorch\n",
    "2. Write custom autograd function that is used in the fwd / bwd\n",
    "2. Write triton kernels for fwd / bwd, and call them from the custom aurograd function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756d2d5-e6bf-45cf-97c8-3d5b8ef5ff89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a776aec-fad0-4b9c-a077-2a7133ff6a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "torch.set_printoptions(linewidth=200, precision=0, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b10dc6-fc00-41c7-a632-4455d036a22c",
   "metadata": {},
   "source": [
    "**1. Define tiny model in pure pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f440ba73-c703-46ec-905a-69b4f80eddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.ones(cout, cin))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weights @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139da9ce-6e25-4586-8df0-ccd4f4599335",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n = 4,2 # out_size, in_size (use powers of 2, as they're easier for triton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42ee18-dad5-4ba1-8da9-4f3a82242365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomLinear()\n",
      "x: tensor([1., 1.], device='cuda:0')\n",
      "y: tensor([2., 2., 2., 2.], device='cuda:0', grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = CustomLinear(n,m).to('cuda')\n",
    "print(lin)\n",
    "x = torch.ones(n, device='cuda')\n",
    "print('x:', x)\n",
    "y = lin(x)\n",
    "print('y:', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911bbd86-30a0-4388-b097-bebd0d4e0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.retain_grad() # retain grad for non-leaf variable, to use it below as input for kernel\n",
    "loss = y.sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc518451-be28-4a03-972a-2990a5803f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_grad = copy(y.grad)\n",
    "y_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e6ad21-a5ef-4888-8fd3-a7a2c352532d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx: None\n",
      "dw: tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('dx:', x.grad)\n",
    "print('dw:',lin.weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052324c9-4651-446d-bfcf-9466030f96b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "243c6d4f-2077-41da-b637-1b167a5ae57c",
   "metadata": {},
   "source": [
    "**2. Write custom autograd function that is used in the fwd / bwd**\n",
    "\n",
    "Now, we'll create a custom `torch.autograd.Function` which manually computes the gradient for our custom operation. This function will be used by the autograd engine, when our operation is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d613b-446f-45ab-9fd5-46c9aa8fd3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, w, x):\n",
    "        print('fwd of LinearFn called')\n",
    "        ctx.save_for_backward(w, x)\n",
    "        return w@x # here, we'll later use a function that runs on gpu\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d):\n",
    "        print('bwd of LinearFn called')\n",
    "        w, x = ctx.saved_tensors\n",
    "        # here, we'll later use a function that runs on gpu:\n",
    "        dx = d@w\n",
    "        dw = d.t()[:,None]@x[None,:]\n",
    "        return dw, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127b808-37d4-43b9-a31e-c7b9666fd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super().__init__()\n",
    "        print('fyi: This module uses a manual autograd function')\n",
    "        self.weights = nn.Parameter(torch.ones(cout, cin))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return LinearFn.apply(self.weights, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46bcaf-4bfb-4fcd-b1cf-973e8d14f733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fyi: This module uses a manual autograd function\n",
      "CustomLinear()\n",
      "x: tensor([1., 1.], device='cuda:0')\n",
      "fwd of LinearFn called\n",
      "y: tensor([2., 2., 2., 2.], device='cuda:0', grad_fn=<LinearFnBackward>)\n"
     ]
    }
   ],
   "source": [
    "lin = CustomLinear(n,m).to('cuda')\n",
    "print(lin)\n",
    "x = torch.ones(n, device='cuda')\n",
    "print('x:', x)\n",
    "y = lin(x)\n",
    "print('y:', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a0643-5921-417a-af0b-07a81af91c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bwd of LinearFn called\n"
     ]
    }
   ],
   "source": [
    "loss = y.sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c15979-0ae9-4293-9a88-d764ca921f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx: None\n",
      "dw: tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('dx:', x.grad)\n",
    "print('dw:',lin.weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaaa796-a3fa-4d44-921c-88841979a94c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "597bdb3e-f0bf-4f1c-9b6e-9d4195cdd641",
   "metadata": {},
   "source": [
    "**3. Write triton kernels for fwd / bwd, and call them from the custom aurograd function**\n",
    "\n",
    "Now, we'll use gpu-backed functions in the manual gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05af61-fafa-44f5-98bb-37b77000d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['TRITON_INTERPRET'] = '1'\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "from triton_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a067851-71a9-46c8-815d-73993f244e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def fwd_kernel(w_ptr, x_ptr, out_ptr, m, n: tl.constexpr, bs: tl.constexpr):\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    offs_m = get_1d_offset(bs, pid)       # split m axis into chunks of size `bs`, take chunk no 'pid'\n",
    "    offs_n = get_1d_offset(n, 0)          # entire n axis\n",
    "\n",
    "    offs_w = get_2d_offset(offs_m, offs_n, stride_0=n)\n",
    "\n",
    "    mask_out    = get_1d_mask(offs_m, m)\n",
    "    mask_x      = get_1d_mask(offs_n, n)\n",
    "    mask_w = get_2d_mask(offs_m, offs_n, m, n)\n",
    "\n",
    "    x = tl.load(x_ptr + offs_n, mask_x) # shape (n)\n",
    "    w = tl.load(w_ptr + offs_w, mask_w) # shape (m,n)\n",
    "\n",
    "    # note: we can't use tl.dot as it require all dims to be >= 16, so let's do manual matmul\n",
    "    out = tl.sum(tl.sum(w[:,:,None] * x[None, :, None],1), 1) # shape (m,n),(n) -> (m,n,1),(1,n,1) -> (m,1) -> (m)\n",
    "\n",
    "    tl.store(out_ptr+offs_m, out, mask_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cd89f-107c-4464-a0a3-4f6342fdd2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_gpu(weight, x):\n",
    "    #shapes: (m,n) @Â (n) -> (m)\n",
    "    m, n = weight.shape\n",
    "    out = torch.zeros(m, device='cuda')\n",
    "    threads = 32\n",
    "    blocks = (cdiv(m,threads),)\n",
    "    assert_tensors_gpu_ready(weight, x, out)\n",
    "    fwd_kernel[blocks](weight, x, out, m, n, threads)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65059dca-cdcd-4fa4-9898-a7e392b0d6d7",
   "metadata": {},
   "source": [
    "Check the gpu-backed fwd producess the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b8096-1d60-4390-82b5-0c2e5279ceae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = fwd_gpu(lin.weights.data, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f12520-dc35-421d-9a09-8b02095355d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9eb1c-1300-498c-982b-e84aee702f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def bwd_kernel(d_ptr, w_ptr, x_ptr, dw_ptr, dx_ptr, m: tl.constexpr, n, bs: tl.constexpr):\n",
    "    # shapes: d = (b,m), w = (m,n), x = (b,n)\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    offs_m = get_1d_offset(m, 0)    # entire m axis\n",
    "    offs_n = get_1d_offset(bs, pid) # split n axis into chunks of size `bs`, take chunk no 'pid'\n",
    "\n",
    "    offs_w = get_2d_offset(offs_m, offs_n, stride_0=n)\n",
    "\n",
    "    mask_d = get_1d_mask(offs_m, m)\n",
    "    mask_x = get_1d_mask(offs_n, n)\n",
    "    mask_w = get_2d_mask(offs_m, offs_n, m, n)\n",
    "\n",
    "    d = tl.load(d_ptr + offs_m, mask_d) # shape (m)\n",
    "    x = tl.load(x_ptr + offs_n, mask_x) # shape (n)\n",
    "    w = tl.load(w_ptr + offs_w, mask_w) # shape (m,n)\n",
    "\n",
    "    # note: we can't use tl.dot as it require all dims to be >= 16, so let's do manual matmul \n",
    "    dx = tl.sum(tl.sum(d[None,:,None] * w[None, :, :], 1), 0) # shape (m),(m,n) -> (1,m,1),(1,m,n) -> (1,n) -> (n) \n",
    "    dw = d[:,None] * x[None, :]                               # shape (m),(n) -> (m,1),(1,n) -> (m,n)\n",
    "\n",
    "    tl.store(dx_ptr+offs_n, dx, mask_x)\n",
    "    tl.store(dw_ptr+offs_w, dw, mask_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a159c12-51ef-417e-87d5-36273a378b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bwd_gpu(d, weight, x):\n",
    "    d = d.contiguous() # autograd can return non-contiguous grads\n",
    "    m, n = weight.shape\n",
    "    dx      = torch.zeros_like(x, device='cuda')\n",
    "    dweight = torch.zeros_like(weight, device='cuda')\n",
    "    threads = 32\n",
    "    blocks = (cdiv(n, threads),)\n",
    "    assert_tensors_gpu_ready(d, weight, x, dweight, dx)\n",
    "    bwd_kernel[blocks](d, weight, x, dweight, dx, m, n, threads)\n",
    "    return dweight, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4cf7ac-61fe-4be0-abf8-a709a8c48d1e",
   "metadata": {},
   "source": [
    "Check the gpu-backed bwd producess the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec0bdd-1976-442a-ab68-a04d8e5b41fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4., 4.], device='cuda:0'),\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw, dx = bwd_gpu(d=y_grad, weight=lin.weights.data, x=x)\n",
    "dx, dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f04246-ba6b-419d-b49b-527e29380215",
   "metadata": {},
   "source": [
    "Now use them in a custom autograd function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a067d4-49a0-4d02-9c75-157164d87e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, w, x):\n",
    "        print('LinearFn.forward with gpu called')\n",
    "        ctx.save_for_backward(w, x)\n",
    "        return fwd_gpu(w, x) # using gpu-backed fwd\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, d):\n",
    "        print('LinearFn.backward with gpu called')\n",
    "        dw, dx = bwd_gpu(d, *ctx.saved_tensors) # using gpu-backed bwd\n",
    "        return dw, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de66820b-1fb7-402d-be1d-535e2c2ce233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super().__init__()\n",
    "        print('fyi: This module uses a manual gpu-backed autograd function')\n",
    "        self.weights = nn.Parameter(torch.ones(cout, cin))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return LinearFn.apply(self.weights, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da66f2-2944-4ed0-8ab7-79bc0e1d60f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fyi: This module uses a manual gpu-backed autograd function\n",
      "CustomLinear()\n",
      "tensor([1., 1.], device='cuda:0')\n",
      "LinearFn.forward with gpu called\n",
      "tensor([2., 2., 2., 2.], device='cuda:0', grad_fn=<LinearFnBackward>)\n"
     ]
    }
   ],
   "source": [
    "lin = CustomLinear(n,m).to('cuda')\n",
    "print(lin)\n",
    "x = torch.ones(n, device='cuda')\n",
    "print(x)\n",
    "y = lin(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a213528-2524-40cd-80a5-81ef39b05cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearFn.backward with gpu called\n"
     ]
    }
   ],
   "source": [
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5d68d-0b22-4c35-8925-d4c67f5b2be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad, lin.weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca773306-267d-4f3d-860e-415fe8303af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27207f5e-1127-4e90-a376-9ffb45819cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
