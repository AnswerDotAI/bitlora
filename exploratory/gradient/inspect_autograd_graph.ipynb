{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d906f0-843d-450a-9d79-8dbec446a026",
   "metadata": {},
   "source": [
    "In this nb, I play around with PyTorch's autograd engine to get a sense of how complicated it would be to get gradient functions out of it (instead of gradients evaluated at a single point).\n",
    "\n",
    "**Edit:** I dediced to manually derive the gradient functions, and verifiy them against the autograd engine. See `verify_derivative.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa7874-5896-4ac1-9272-897214e5da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from fastcore.foundation import L\n",
    "from fastcore.basics import strcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56590b-a10d-467a-abcc-6e2c7692409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModule(nn.Module):\n",
    "    def __init__(self, n,r,m):\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.randn(r,n))\n",
    "        self.b = nn.Parameter(torch.randn(m,r))\n",
    "        self.c = nn.Parameter(torch.randn(m))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (m,r) @ (r,n) @ (n) + (m)\n",
    "        return self.b@self.a@x + self.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198d7b4-8f72-43bb-9072-8bfaa25fd308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapes(*ts): return strcat(L(ts).map(lambda t: list(t.shape)), \"; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c528e-abc2-48c2-843f-0ecd54a8a3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1, 2]; [3, 1]; [3]; [2]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = CustomModule(n=2, r=1, m=3)\n",
    "x = torch.randn(2)\n",
    "\n",
    "shapes(module.a, module.b, module.c, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04e8774-c7b8-4ba7-bfd7-cc7fbd6e98dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[3]', tensor([5.1279, 0.0990, 4.6005], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = module(x)\n",
    "shapes(y), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b78ec-636e-47b0-99d6-a86acf100705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cce42a7-4853-420a-b653-6bf804b00b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = CustomModule(n=2, r=1, m=3)\n",
    "x = torch.randn(2)\n",
    "y = module(x)\n",
    "l = y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966e1ed-92f8-4cc2-a8f5-6dfc1f4e4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SumBackward0 object>\n",
      "    <AddBackward0 object>\n",
      "        <MvBackward0 object>\n",
      "            <MmBackward0 object>\n",
      "                <AccumulateGrad object>\n",
      "                <AccumulateGrad object>\n",
      "        <AccumulateGrad object>\n"
     ]
    }
   ],
   "source": [
    "# Recursively print the backward graph \n",
    "def print_graph(node, indent=0):\n",
    "    print(' ' * indent + str(node))\n",
    "    for next_node, _ in node.next_functions:\n",
    "        if next_node is not None:\n",
    "            print_graph(next_node, indent + 4)\n",
    "\n",
    "# Start from the output's grad_fn\n",
    "print_graph(l.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44736a-824e-4797-b45d-b5eb15433433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m   \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m        SumBackward0\n",
       "\u001b[0;31mString form:\u001b[0m <SumBackward0 object>\n",
       "\u001b[0;31mDocstring:\u001b[0m   <no docstring>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l.grad_fn??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dbf960-61c5-459d-8f32-a6689881098a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86e306-3142-4791-b4bf-a95dd1be918b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.grad_fn._saved_self_sym_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828fad7-ae61-4597-9c69-19163b1e8046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a91a21-2fdd-4fab-9320-95eb7899bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata          (var         ) : {}\n",
      "name              (mth - called) : SumBackward0\n",
      "next_functions    (var         ) : ((<AddBackward0 object>, 0),)\n",
      "register_hook     (mth         ) : <built-in method register_hook of SumBackward0 object>\n",
      "register_prehook  (mth         ) : <built-in method register_prehook of SumBackward0 object>\n",
      "requires_grad     (var         ) : True\n"
     ]
    }
   ],
   "source": [
    "for a in [\n",
    "    'metadata',\n",
    "    'name',\n",
    "    'next_functions',\n",
    "    'register_hook',\n",
    "    'register_prehook',\n",
    "    'requires_grad'\n",
    "]:\n",
    "    obj = getattr(l.grad_fn, a)\n",
    "    if callable(obj):\n",
    "        try:\n",
    "            result = obj()\n",
    "            ty = 'mth - called'\n",
    "        except TypeError:\n",
    "            result = obj\n",
    "            ty = 'mth'\n",
    "    else:\n",
    "        result = obj\n",
    "        ty = 'var'\n",
    "    print(f'{a:<17} ({ty:<12}) : {result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65769e16-7bd1-42e2-a213-160e0d4c2312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c11e7e62-763a-4ed3-9b1e-a92a9eaa852d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67fcc3-1b8f-42bf-a77e-200d156579d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import yaml\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/pytorch/pytorch/a55d63659ad0b9a14cbf5b495464994a9180c988/tools/autograd/derivatives.yaml'\n",
    "response = requests.get(url)\n",
    "data = L(yaml.safe_load(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ae64d-48c6-403c-a52d-5df8e87ba449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9) ['AccumulateGrad','AddBackward0','DivBackward0','MmBackward0','MulBackward0','SqueezeBackward4','SumBackward0','TBackward0','UnsqueezeBackward0']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops = L(\n",
    "    'AccumulateGrad',\n",
    "    'AddBackward0',\n",
    "    'DivBackward0',\n",
    "    'MmBackward0',\n",
    "    'MulBackward0',\n",
    "    'SqueezeBackward4',\n",
    "    'SumBackward0',\n",
    "    'TBackward0',\n",
    "    'UnsqueezeBackward0'\n",
    ")\n",
    "ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08069e-80db-46bb-a2da-8be44c2db227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_name(full_name):\n",
    "    full_name = full_name.lower()\n",
    "    try:    return full_name.split('backward')[0]\n",
    "    except: return full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce7e49-0084-4567-a03d-82770b72334c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9) ['accumulategrad','add','div','mm','mul','squeeze','sum','t','unsqueeze']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops = ops.map(short_name)\n",
    "ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6facee28-646e-464c-a62a-c02942f7847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_names(txt):\n",
    "    for l in data.attrgot('name').filter(lambda o: o.startswith(txt)): print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a248ed-81d0-4dcc-b162-979a3411dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative rules for accumulategrad:\n",
      "\t-none\n",
      "Derivative rules for add:\n",
      "\t- add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n",
      "\t- add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor\n",
      "\t- addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor\n",
      "\t- addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor\n",
      "\t- addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor\n",
      "\t- addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor\n",
      "\t- addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor\n",
      "\t- addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor\n",
      "Derivative rules for div:\n",
      "\t- div.Tensor(Tensor self, Tensor other) -> Tensor\n",
      "\t- div.Scalar(Tensor self, Scalar other) -> Tensor\n",
      "\t- div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor\n",
      "\t- div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor\n",
      "Derivative rules for mm:\n",
      "\t- mm(Tensor self, Tensor mat2) -> Tensor\n",
      "Derivative rules for mul:\n",
      "\t- mul.Tensor(Tensor self, Tensor other) -> Tensor\n",
      "\t- mul.Scalar(Tensor self, Scalar other) -> Tensor\n",
      "\t- multi_margin_loss(Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean) -> Tensor\n",
      "\t- multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -> (Tensor output, Tensor is_target)\n",
      "\t- multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor\n",
      "Derivative rules for squeeze:\n",
      "\t- squeeze(Tensor(a) self) -> Tensor(a)\n",
      "\t- squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)\n",
      "\t- squeeze.dims(Tensor(a) self, int[] dim) -> Tensor(a)\n",
      "\t- squeeze_(Tensor(a!) self) -> Tensor(a!)\n",
      "\t- squeeze_.dim(Tensor(a!) self, int dim) -> Tensor(a!)\n",
      "\t- squeeze_.dims(Tensor(a!) self, int[] dim) -> Tensor(a!)\n",
      "Derivative rules for sum:\n",
      "\t- sum(Tensor self, *, ScalarType? dtype=None) -> Tensor\n",
      "\t- sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor\n",
      "Derivative rules for t:\n",
      "\t- t(Tensor(a) self) -> Tensor(a)\n",
      "\t- t_(Tensor(a!) self) -> Tensor(a!)\n",
      "\t- take(Tensor self, Tensor index) -> Tensor\n",
      "\t- tan(Tensor self) -> Tensor\n",
      "\t- tanh(Tensor self) -> Tensor\n",
      "\t- topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)\n",
      "\t- trace(Tensor self) -> Tensor\n",
      "\t- transpose.int(Tensor(a) self, int dim0, int dim1) -> Tensor(a)\n",
      "\t- transpose_(Tensor(a!) self, int dim0, int dim1) -> Tensor(a!)\n",
      "\t- triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)\n",
      "\t- tril(Tensor self, int diagonal=0) -> Tensor\n",
      "\t- triu(Tensor self, int diagonal=0) -> Tensor\n",
      "\t- trunc(Tensor self) -> Tensor\n",
      "\t- to_mkldnn(Tensor self, ScalarType? dtype=None) -> Tensor\n",
      "\t- threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor\n",
      "\t- threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)\n",
      "\t- threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor\n",
      "\t- tanh_backward(Tensor grad_output, Tensor output) -> Tensor\n",
      "\t- to_padded_tensor(Tensor self, float padding, SymInt[]? output_size=None) -> Tensor\n",
      "Derivative rules for unsqueeze:\n",
      "\t- unsqueeze(Tensor(a) self, int dim) -> Tensor(a)\n",
      "\t- unsqueeze_(Tensor(a!) self, int dim) -> Tensor(a!)\n"
     ]
    }
   ],
   "source": [
    "for op in ops:\n",
    "    print(f'Derivative rules for {op}:')\n",
    "    rules = data.attrgot('name').filter(lambda o: o.startswith(op))\n",
    "    if rules:\n",
    "        for r in rules: print(f'\\t- {r}')\n",
    "    else:\n",
    "        print('\\t-none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89483bcc-9e82-4164-b58b-6eb6b3d72938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c5e8d-c26f-46d6-a57b-5c077328550c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9) ['add.Tensor','div.Tensor','mm','mul.Tensor','mul.Scalar','squeeze.dim','sum','t','unsqueeze']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_op_names = L('add.Tensor div.Tensor mm mul.Tensor mul.Scalar squeeze.dim sum t unsqueeze'.split(' '))\n",
    "op_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525259d4-da59-4609-8622-0c6733f9f304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Derivative rules for add.Tensor:\n",
      "{'name': 'add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor',\n",
      " 'other': 'handle_r_to_c(other.scalar_type(), maybe_multiply(grad, alpha.conj()))',\n",
      " 'result': 'self_t + maybe_multiply(other_t, alpha)',\n",
      " 'self': 'handle_r_to_c(self.scalar_type(), grad)'}\n",
      "\n",
      "--- Derivative rules for div.Tensor:\n",
      "{'name': 'div.Tensor(Tensor self, Tensor other) -> Tensor',\n",
      " 'other': 'div_tensor_other_backward(grad, self, other)',\n",
      " 'result': '(self_t - other_t * result) / other_p',\n",
      " 'self': 'div_tensor_self_backward(grad, other, self.scalar_type())'}\n",
      "\n",
      "--- Derivative rules for mm:\n",
      "{'mat2': 'mm_mat2_backward(grad, self, mat2.sym_sizes(), mat2.sym_strides(), mat2.layout(), 1)',\n",
      " 'name': 'mm(Tensor self, Tensor mat2) -> Tensor',\n",
      " 'result': 'at::mm(self_t, mat2_p) + at::mm(self_p, mat2_t)',\n",
      " 'self': 'mm_mat1_backward(grad, mat2, self.sym_sizes(), self.sym_strides(), self.layout(), 1)'}\n",
      "\n",
      "--- Derivative rules for mul.Tensor:\n",
      "{'name': 'mul.Tensor(Tensor self, Tensor other) -> Tensor',\n",
      " 'other': 'mul_tensor_backward(grad, self, other.scalar_type())',\n",
      " 'result': 'other_t * self_p + self_t * other_p',\n",
      " 'self': 'mul_tensor_backward(grad, other, self.scalar_type())'}\n",
      "\n",
      "--- Derivative rules for mul.Scalar:\n",
      "{'name': 'mul.Scalar(Tensor self, Scalar other) -> Tensor', 'result': 'self_t * other', 'self': 'mul_tensor_backward(grad, other, self.scalar_type())'}\n",
      "\n",
      "--- Derivative rules for squeeze.dim:\n",
      "{'dispatch': {'AutogradNestedTensor': {'self': 'grad.unsqueeze(dim)'}, 'Default': {'result': 'auto_linear', 'self': 'unsqueeze_to(grad, dim, self.sym_sizes())'}},\n",
      " 'name': 'squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)'}\n",
      "\n",
      "--- Derivative rules for sum:\n",
      "{'name': 'sum(Tensor self, *, ScalarType? dtype=None) -> Tensor', 'result': 'auto_linear', 'self': 'grad.expand_symint(self.sym_sizes())'}\n",
      "\n",
      "--- Derivative rules for t:\n",
      "{'name': 't(Tensor(a) self) -> Tensor(a)', 'result': 'auto_linear', 'self': 'grad.t()'}\n",
      "\n",
      "--- Derivative rules for unsqueeze:\n",
      "{'name': 'unsqueeze(Tensor(a) self, int dim) -> Tensor(a)', 'result': 'auto_linear', 'self': 'grad.squeeze(dim)'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for op in relevant_op_names:\n",
    "    print(f'--- Derivative rules for {op}:')\n",
    "    rules = data.filter(lambda o: o['name'].startswith(op+'('))\n",
    "    if len(rules)==0: print('none')\n",
    "    assert len(rules)==1\n",
    "    pprint(rules[0], width=200)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21109ffa-2b37-4049-8c30-7f734fb40eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951f689d-3e03-4a72-ba05-68a516f1590b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0a477f-3741-4d92-83d7-8bc19766f018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
