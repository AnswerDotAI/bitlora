{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82cb0d1c-4157-4936-bf51-b289dbb3ac9f",
   "metadata": {},
   "source": [
    "If model isn't cached:\n",
    "\n",
    "```py\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e7f8ec-55fa-489c-995c-f6dad74278d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval.__main__ import cli_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33831684-74a1-49a2-9081-feb56aa7cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "args = argparse.Namespace(\n",
    "    model='hf',\n",
    "    model_args='pretrained=meta-llama/Meta-Llama-3-8B',\n",
    "    tasks='dharma2',\n",
    "    device='cuda:0',\n",
    "    batch_size='auto',\n",
    "    # set limit to None for full test\n",
    "    limit=None,\n",
    "    # defaults \n",
    "    num_fewshot=None,\n",
    "    max_batch_size=None,\n",
    "    output_path=None,\n",
    "    use_cache=None,\n",
    "    cache_requests=None,\n",
    "    check_integrity=False,\n",
    "    write_out=False,\n",
    "    log_samples=False,\n",
    "    show_config=False,\n",
    "    include_path=None,\n",
    "    gen_kwargs=None,\n",
    "    verbosity='INFO',\n",
    "    wandb_args='',\n",
    "    predict_only=False,\n",
    "    seed=[0, 1234, 1234],  # Adjust if needed\n",
    "    trust_remote_code=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "656ed051-8f55-453e-8c86-be241c79b271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24:20:04:07,141 INFO     [__main__.py:251] Verbosity set to INFO\n",
      "2024-04-24:20:04:12,092 INFO     [__main__.py:335] Selected Tasks: ['dharma2']\n",
      "2024-04-24:20:04:12,094 INFO     [evaluator.py:131] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-04-24:20:04:12,094 INFO     [evaluator.py:177] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B'}\n",
      "2024-04-24:20:04:12,194 INFO     [huggingface.py:164] Using device 'cuda:0'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fb1ab97a2b44889f7270bba75eb3e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-04-24:20:04:16,073 WARNING  [task.py:322] [Task: dharma2] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-04-24:20:04:16,073 WARNING  [task.py:322] [Task: dharma2] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-04-24:20:04:16,083 INFO     [task.py:395] Building contexts for dharma2 on rank 0...\n",
      "100%|██████████| 300/300 [00:00<00:00, 104936.30it/s]\n",
      "2024-04-24:20:04:16,095 INFO     [evaluator.py:381] Running generate_until requests\n",
      "Running generate_until requests:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed argument batch_size = auto. Detecting largest batch size\n",
      "Determined Largest batch size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Running generate_until requests: 100%|██████████| 300/300 [15:32<00:00,  3.11s/it]\n",
      "fatal: not a git repository (or any parent up to mount point /teamspace/studios)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf (pretrained=meta-llama/Meta-Llama-3-8B), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n",
      "| Tasks |Version|     Filter      |n-shot|  Metric   |Value|   |Stderr|\n",
      "|-------|------:|-----------------|-----:|-----------|----:|---|-----:|\n",
      "|dharma2|      1|remove_whitespace|     0|exact_match| 0.34|±  |0.0274|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cli_evaluate(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa132b23-4401-4a18-98a7-5131f4a5a470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5c2da-27b1-4c2d-a7b4-88f1f4d5d050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
