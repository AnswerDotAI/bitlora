{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035941a3-fd58-48e6-a43a-fea00f672b41",
   "metadata": {},
   "source": [
    "**Simplified python reference implementation of hqq-qdora.**\n",
    "\n",
    "(This nb only contains the final module, not the build up to it. For the full build up, see `python_hqq_qdora.ipynb`)\n",
    "\n",
    "Simplifications:\n",
    "- Replaced hqq quanting with simple group-wise quanting, while keeping the hqq dequanting. This is okay, because the fwd only uses the output of quanting, and hqq quanting and simple group-wise quanting return the same kinds of output (namely: quanted data, zeropoints, scales).\n",
    "- Initializing lora_a with 0. Same reason as above.\n",
    "\n",
    "**ToDo:** Compare against hqq implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225deadd-a13a-4146-bfb2-a6d91bf2b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor, cat, int32\n",
    "from torch import float16 as fp16\n",
    "from math import ceil\n",
    "\n",
    "torch.set_printoptions(linewidth=200, precision=2, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f82477-707c-4358-8a2d-acf9202d4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_close(a,b): assert torch.isclose(a,b,atol=1e-2).all()\n",
    "def assert_somehow_close(a,b): assert torch.isclose(a,b,atol=0.1).all() # allow error of 0.1 due to quanting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a6675-7930-48f2-9173-c3f234e34888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d7e70-f910-4f1e-a241-5577c19a9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantedDoraModule(nn.Module):\n",
    "    def __init__(self, linear, bits, group_size, rank, alpha):\n",
    "        super().__init__()\n",
    "        # for quanting\n",
    "        assert base_linear.weight.numel() % group_size ==0, f'group_size {group_size} can\\'t cleanly split weight of base layer ({base_linear.weight.numel()} items)'\n",
    "        self.bits,self.group_size, = bits,group_size\n",
    "        self.quant(linear)\n",
    "        # for dora\n",
    "        self.a = nn.Linear(linear.in_features, rank, bias=False, dtype=fp16)\n",
    "        self.b = nn.Linear(rank, linear.out_features, bias=False, dtype=fp16)\n",
    "        self.alpha = alpha\n",
    "        self.m = nn.Parameter(linear.weight.norm(p=2, dim=1))\n",
    "        # init a & b to 0 -- a should be inited differently, but for sake of simplicity, set it to 0 as well\n",
    "        self.a.weight.data.zero_()\n",
    "        self.b.weight.data.zero_()\n",
    "\n",
    "    def quant(self, linear):\n",
    "        data = linear.weight.data\n",
    "        self.shape = data.shape\n",
    "\n",
    "        # repeat last element, to have a multiple of group_size elements\n",
    "        # note: element to pad with mustn't change any attribute that's use for quanting (eg min & max in a group)\n",
    "        n_pad = data.numel()%self.group_size\n",
    "        data = F.pad(data, (0,n_pad), 'constant', data.flatten()[-1])\n",
    "        assert data.numel()%self.group_size==0\n",
    "\n",
    "        data = data.reshape(-1,self.group_size)\n",
    "        \n",
    "        min_, max_ = data.min(axis=-1, keepdim=True).values, data.max(axis=-1, keepdim=True).values\n",
    "        \n",
    "        self.zero = min_\n",
    "        self.scale = (max_-min_) / (2**self.bits-1) \n",
    "        \n",
    "        # note: can't use shorthand ops like -= as they modify tensor in-place\n",
    "        data = data - self.zero # start at 0\n",
    "        data = data / self.scale # scale to [0, 2**bits-1]\n",
    "        data = data.round().to(int)\n",
    "\n",
    "        # packed quantized data\n",
    "        self.pqdata = self.pack(data.flatten())\n",
    "\n",
    "    # pack 10 3bit values into a 32bit val\n",
    "    @staticmethod\n",
    "    def pack(vals):\n",
    "        for v in vals: assert 0<=v<=7 and v//1==v, f'Value {v} can\\'t be represented by 3 bits or is not an integer'\n",
    "        \n",
    "        n_packs = ceil(len(vals)/10)\n",
    "    \n",
    "        # pad with 0, to have a multiple of pack_size elements\n",
    "        n_pad = n_packs*10 - len(vals)\n",
    "        vals = F.pad(vals, (0,n_pad), 'constant', 0)\n",
    "        assert len(vals)==n_packs*10\n",
    "    \n",
    "        packed = torch.zeros(n_packs, dtype=int32)\n",
    "        for i in range(n_packs):\n",
    "            # pack the 10 vals from 10*i to 10*(i+1) into packed[i]\n",
    "            for x in vals[10*i:10*(i+1)]: packed[i] = (packed[i] << 3) | x # shift right 3 bits, then set last 3 bits to x\n",
    "        return packed\n",
    "\n",
    "    def dequant(self):\n",
    "        data = self.unpack(self.pqdata)[:self.shape.numel()] # unpack & remove padding that was added during packing\n",
    "        data = data.reshape(-1,self.group_size)\n",
    "        data = data*self.scale + self.zero\n",
    "        return data.reshape(self.shape)\n",
    "    \n",
    "    # unpack a 32bit value into 10 3bit vals\n",
    "    @staticmethod\n",
    "    def unpack(packed):\n",
    "        def bin_to_dec(b3,b2,b1): return 4*b3 + 2*b2 + b1\n",
    "        for v in packed: isinstance(v, int), f'Value {v} is not an integer'\n",
    "        unpacked = []\n",
    "        for pack in packed:\n",
    "            for i in reversed(range(10)):\n",
    "                unpacked.append((pack >> (3*i)) & 0b111) # righ-shift 3*i times, so last 3 bits are those we want; then only select those via 0b111            \n",
    "        return tensor(unpacked)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dequant()@x + self.b(self.a(x))\n",
    "        col_norms =  (self.dequant() + self.b.weight @ self.a.weight).norm(p=2, dim=1).detach()\n",
    "        x /= col_norms\n",
    "        x *= self.m * self.alpha\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6153fbb-e480-45a5-bd43-62064b575b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.44,  0.25,  0.37, -0.49],\n",
       "        [ 0.27, -0.07, -0.40,  0.14],\n",
       "        [ 0.10, -0.43, -0.15, -0.02],\n",
       "        [ 0.03,  0.16,  0.09, -0.13],\n",
       "        [ 0.02,  0.31, -0.17, -0.24]], dtype=torch.float16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_linear = nn.Linear(4,5, bias=False, dtype=fp16) # ignore bias for now\n",
    "base_linear.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d83c4d-0b9d-49a8-a13e-b47b13db9a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.75, -0.51,  1.20,  0.54], dtype=torch.float16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tst = torch.randn(4, dtype=fp16); x_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d69580-0c9a-410e-9d7a-257a0dcec829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.28, -0.16,  0.09, -0.02, -0.48], dtype=torch.float16, grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tst = base_linear(x_tst); y_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8667db-744a-4f71-a247-18ad7d406c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee2a8c-6952-4941-842a-4374b6bc6f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantedDoraModule(\n",
       "  (a): Linear(in_features=4, out_features=2, bias=False)\n",
       "  (b): Linear(in_features=2, out_features=5, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdora = QuantedDoraModule(base_linear, bits=3, group_size=5, rank=2, alpha=1); qdora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43823f-f789-44e4-b01f-d905c0b9e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_qdora = qdora(x_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d8899f-8246-4cdb-adf8-8d32ad10d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quanted result (with packing): tensor([-0.30, -0.18,  0.07, -0.06, -0.49], dtype=torch.float16, grad_fn=<MulBackward0>)\n",
      "exact   result               : tensor([-0.28, -0.16,  0.09, -0.02, -0.48], dtype=torch.float16, grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "print(f'quanted result (with packing): {y_qdora}')\n",
    "print(f'exact   result               : {y_tst}')\n",
    "assert_somehow_close(y_qdora, y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98828f0-d6f9-4ea6-90fb-0c0ed9156a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "050c8c0a-5341-4c30-8086-966bdcd9a527",
   "metadata": {},
   "source": [
    "Let's call backwards on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd43544-9e0e-4e1f-9a99-8859d3e4ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert only the dora part is trainable\n",
    "assert {n for n,p in qdora.named_parameters()} == {'m','a.weight','b.weight'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df823415-ba36-4669-8ddc-a8bb74bd0b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89483648-91a2-422b-9cef-bb4138b04055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.97, dtype=torch.float16, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = y_qdora.sum() # abitrary operation to make y_qdora a scalar\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aebc50-a137-4ad0-9f54-4678123c8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631658a8-80e3-4cef-a480-348d2a4cddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss shapes:\n",
      "Shape of grad of m        is [5]    ; shape of  m        is [5]\n",
      "Shape of grad of a.weight is [2, 4] ; shape of  a.weight is [2, 4]\n",
      "Shape of grad of b.weight is [5, 2] ; shape of  b.weight is [5, 2]\n"
     ]
    }
   ],
   "source": [
    "print('Loss shapes:')\n",
    "for n,p in qdora.named_parameters():\n",
    "    print(f'Shape of grad of {n:<8} is {str(list(p.grad.shape)):<7}; shape of  {n:<8} is {list(p.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b3cee-6cdb-4c82-b26b-38899ee8ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- m.grad:\n",
      "tensor([-0.38, -0.37,  0.15, -0.27, -1.15], dtype=torch.float16)\n",
      "--- a.weight.grad:\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], dtype=torch.float16)\n",
      "--- b.weight.grad:\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "for n,p in qdora.named_parameters():\n",
    "    print(f'--- {n}.grad:\\n{p.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e312c2c-c120-43f6-9682-5a82a73c545f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5595d-f240-4adf-9594-7b3ac2a33bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
