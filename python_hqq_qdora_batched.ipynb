{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035941a3-fd58-48e6-a43a-fea00f672b41",
   "metadata": {},
   "source": [
    "**Simplified python reference implementation of hqq-qdora.**\n",
    "\n",
    "This nb only contains the final module, not the build up to it. For the full build up, see `python_hqq_qdora_v2.ipynb` and `python_hqq_qdora.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225deadd-a13a-4146-bfb2-a6d91bf2b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor, cat, int32, float16 as fp16, bfloat16 as bf16\n",
    "from math import ceil\n",
    "\n",
    "from fastcore.foundation import L\n",
    "from fastcore.basics import store_attr, AttrDict\n",
    "\n",
    "from hqq.core.quantize import Quantizer, HQQLinear, BaseQuantizeConfig # Quantizer - optional, only for optimizing during quanting ; HQQLinear & BaseQuantizeConfig to verify our implementation\n",
    "\n",
    "torch.set_printoptions(linewidth=200, precision=2, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f82477-707c-4358-8a2d-acf9202d4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_abs_diff(a,b): return (a-b).abs().max()\n",
    "def assert_close(a,b): assert torch.isclose(a,b,atol=1e-2).all(), f'assert_close failed, max error = {max_abs_diff(a,b)}'\n",
    "def assert_somehow_close(a,b,max_err=0.12): assert torch.isclose(a,b,atol=max_err).all(), f'assert_somehow_close failed, max error = {max_abs_diff(a,b)}' # allow some error due to quanting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78eda9-5478-483b-9391-ab2537aedc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,r,n = 128,32,128\n",
    "axis=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d7e70-f910-4f1e-a241-5577c19a9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from `python_hqq_qdora_clean.ipynb`\n",
    "# changes\n",
    "# - add axis param, to allow using axis=1\n",
    "# - unpack and _dequant keep data on device it's on\n",
    "# - only dequant/unpack ourselves, and use hqq for quanting/packing\n",
    "# - zero/scale are not packed (as is hqq)\n",
    "# - a (of lora) is initialized correctly\n",
    "\n",
    "class QuantedDoraModule(nn.Module):\n",
    "    def __init__(self, hqq_linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.device = hqq_linear.device        \n",
    "        self.hqq_linear = hqq_linear\n",
    "        # save all metadata, we need for dequanting\n",
    "        meta, meta_zero, meta_scale = AttrDict(hqq_linear.meta),  AttrDict(hqq_linear.meta['meta_zero']),  AttrDict(hqq_linear.meta['meta_scale'])\n",
    "        self.qdata = hqq_linear.W_q.data\n",
    "        self.qzero, self.qscale  = (meta.zero_scale[0], meta.zero_scale[1]) if 'zero_scale' in meta else meta.zero_q, meta.scale_q\n",
    "        self.zeros_zero,  self.zeros_scale  = meta_zero.zero, meta_zero.scale        \n",
    "        self.scales_zero, self.scales_scale = meta_scale.zero, meta_scale.scale\n",
    "        self.data_shape, self.zero_shape, self.scale_shape = meta.shape, meta_zero.shape, meta_scale.shape\n",
    "        self.group_size, self.group_size_zero, self.group_size_scale = meta.group_size, meta_zero.group_size, meta_scale.group_size\n",
    "        self.compute_dtype = meta.compute_dtype\n",
    "        self.axis = meta.axis\n",
    "        # for dora\n",
    "        self.a = nn.Linear(hqq_linear.in_features, rank,  bias=False, dtype=self.compute_dtype, device=self.device)\n",
    "        self.b = nn.Linear(rank, hqq_linear.out_features, bias=False, dtype=self.compute_dtype, device=self.device)        \n",
    "        self.alpha = alpha\n",
    "        self.m = nn.Parameter(hqq_linear.dequantize().norm(p=2, dim=1))\n",
    "        # init a & b\n",
    "        self.a.weight.data = torch.randn(rank, hqq_linear.in_features).to(dtype=self.compute_dtype, device=self.device) / (rank**0.5)\n",
    "        self.b.weight.data.zero_()\n",
    "\n",
    "    @staticmethod\n",
    "    def unpack(packed):\n",
    "        def bin_to_dec(b3,b2,b1): return 4*b3 + 2*b2 + b1\n",
    "        assert len(packed.shape)==2 and packed.dtype==int32, 'Pass a 2d tensor of int32s'\n",
    "        n_packs, cols = packed.shape\n",
    "        padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "        for k_up, k_down in zip(range(10), reversed(range(10))): # top-most 3bits vals (k_up=0) are most right-shifted (k_down=9)\n",
    "            padded_vals[k_down*n_packs:(k_down+1)*n_packs,:] = ((packed >> (3*k_up)) & 0b111) # righ-shift 3*k_up times, so last 3 bits are those we want; then only select those via 0b111\n",
    "        return padded_vals.to(packed.device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _dequant(data, zero, scale, shape, group_size, packed, axis):\n",
    "        if packed:\n",
    "            data = QuantedDoraModule.unpack(data)\n",
    "            rows = group_size if axis==0 else shape.numel()//group_size\n",
    "            data = data[:rows,:]  # removed padded rows that were added for packing (which required row num to be multiple of 10) \n",
    "        data = (data-zero)*scale\n",
    "        return data.reshape(shape).to(data.device)\n",
    "\n",
    "    def dequant(self):\n",
    "        zero  = self._dequant(self.qzero,  self.zeros_zero,  self.zeros_scale,  self.zero_shape,  self.group_size_zero,  packed=False, axis=0) # zero/scale are uint8, so don't require unpacking\n",
    "        scale = self._dequant(self.qscale, self.scales_zero, self.scales_scale, self.scale_shape, self.group_size_scale, packed=False, axis=0)\n",
    "        return  self._dequant(self.qdata,  zero,             scale,             self.data_shape,  self.group_size,       packed=True,  axis=self.axis).to(self.compute_dtype)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, self.dequant()) + self.b(self.a(x)) # use F.linear for batched matmul works\n",
    "        col_norms =  (self.dequant() + self.b.weight @ self.a.weight).norm(p=2, dim=1).detach()\n",
    "        x /= col_norms\n",
    "        x *= self.m * self.alpha\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e715d-332b-470c-8494-3d00851a69d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a222bac-eb05-4f40-8316-68b181ea1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 32       # batch size\n",
    "m,n = 16,8 # out, in\n",
    "r = 2       # lora rank\n",
    "\n",
    "gz = 64     # group size\n",
    "\n",
    "base_linear = nn.Linear(n,m,bias=False, dtype=bf16, device='cuda')\n",
    "\n",
    "ngroups = base_linear.weight.numel()//gz\n",
    "\n",
    "# equals BaseQuantizeConfig(nbits=3, group_size=gz, quant_zero=True, quant_scale=True, offload_meta=True, view_as_float=True),\n",
    "# but with group_size for scale & zero set to ngroups, instead of default 128\n",
    "quant_cfg = dict(\n",
    "    # note: Kerem used view_as_float=True, which stores quanted, packed weights as compute_dtype (for us: bf16) instead of int32\n",
    "    weight_quant_params = dict(nbits=3, group_size=gz,      bitpack=True, optimize=True, axis=axis),\n",
    "    # note: hqq sets nbits for scale/zero to 8, regardless of nbits for weights; nbits=3 result in error further below     \n",
    "    scale_quant_params  = dict(nbits=8, group_size=ngroups, bitpack=True, optimize=False), \n",
    "    zero_quant_params   = dict(nbits=8, group_size=ngroups, bitpack=True, optimize=False),\n",
    "    offload_meta = False # note: 1) Kerem used offload_meta=True; 2) offload_meta=True concats meta['zero_q'] & meta['scale_q'] together into meta['zero_scale']\n",
    ")\n",
    "hqq_linear = HQQLinear(base_linear, quant_cfg, compute_dtype=bf16)\n",
    "assert hqq_linear.W_q.dtype==int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03835012-53ae-44d0-9c64-a5424aacce50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee2a8c-6952-4941-842a-4374b6bc6f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantedDoraModule(\n",
       "  (hqq_linear): HQQLinear(in_features=8, out_features=16, bias=False)\n",
       "  (a): Linear(in_features=8, out_features=2, bias=False)\n",
       "  (b): Linear(in_features=2, out_features=16, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdora_linear = QuantedDoraModule(hqq_linear, r, 1.0); qdora_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43823f-f789-44e4-b01f-d905c0b9e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((b, n), device='cuda', dtype=bf16) # batched input\n",
    "y = base_linear(x)\n",
    "y_qdora = qdora_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac1c709-f65d-45e8-b2e2-4dc284667285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: x = torch.Size([32, 8]) ; y = torch.Size([32, 16]) ; y_qdora = torch.Size([32, 16])\n"
     ]
    }
   ],
   "source": [
    "print(f'Shapes: x = {x.shape} ; y = {y.shape} ; y_qdora = {y_qdora.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d8899f-8246-4cdb-adf8-8d32ad10d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quanted result (with packing):\n",
      "tensor([[    -0.18,     -0.18,      0.13,     -0.24,     -0.14,     -0.29,      0.04,     -0.10,     -0.24,      0.27,      0.37,      0.16,     -0.33,     -0.00,      0.17,      0.12],\n",
      "        [    -0.40,     -0.61,     -0.12,     -0.27,     -0.41,     -0.15,      0.04,     -0.06,      0.36,     -0.84,     -0.29,      0.42,     -0.42,     -0.74,      0.53,      0.73],\n",
      "        [     0.38,     -0.03,      0.38,      0.86,      0.37,     -0.72,      0.08,      0.04,     -0.15,     -0.26,     -0.90,      0.25,     -0.19,      0.03,      0.13,      0.34],\n",
      "        [     0.20,      0.15,      0.04,      0.63,      0.81,     -1.13,     -0.48,     -0.41,      0.08,     -0.74,     -0.88,      0.64,     -0.16,     -0.52,      0.47,      0.47],\n",
      "        [    -0.11,     -0.72,     -0.39,      1.24,     -0.76,      0.38,      0.31,      0.25,      0.76,     -1.09,     -1.74,     -0.31,     -0.01,     -0.08,      0.05,      0.89],\n",
      "        [     0.56,      0.39,      1.52,      0.33,      0.58,     -0.73,      0.42,      0.57,     -0.94,      0.43,      1.47,      0.09,     -0.88,      0.63,      0.86,      0.04],\n",
      "        [     0.38,      0.76,      1.03,      0.88,      0.99,     -0.30,      0.04,      0.03,     -0.24,      0.04,     -0.04,      0.67,     -0.07,     -0.01,     -0.26,      0.35],\n",
      "        [    -0.55,     -0.31,     -1.04,      1.00,     -0.66,      0.10,     -0.89,     -0.03,      0.94,     -0.91,      0.13,     -0.80,     -0.57,      0.52,      1.56,      0.40],\n",
      "        [     0.01,     -0.38,     -0.13,     -0.38,     -0.52,     -0.11,      0.43,      0.04,     -0.59,      0.79,      0.55,     -0.68,     -0.04,      0.63,     -0.11,     -0.40],\n",
      "        [    -0.42,     -0.81,     -1.05,     -0.23,     -0.36,     -0.98,     -0.13,     -0.82,     -0.11,     -0.51,     -0.32,     -0.34,     -0.05,     -0.30,      0.49,      0.42],\n",
      "        [     0.53,      0.38,      0.29,      1.21,      0.07,      0.61,      0.02,      0.76,      0.39,     -0.35,     -0.43,     -0.56,      0.07,      0.69,      0.24,     -0.15],\n",
      "        [     0.42,      0.21,      0.61,      0.01,      0.26,      0.07,      0.08,      0.51,      0.01,      0.06,     -0.39,      0.41,     -0.08,     -0.10,     -0.10,     -0.22],\n",
      "        [    -0.69,     -0.39,     -0.98,     -1.84,     -0.83,      0.60,     -0.33,     -0.25,      0.15,      0.70,      0.91,     -0.21,      0.16,     -0.21,     -0.16,     -0.77],\n",
      "        [     0.71,      0.14,      0.37,     -0.22,      0.36,     -0.25,      0.21,      0.38,     -0.35,      0.41,     -0.79,      0.17,      0.30,      0.03,     -0.51,     -0.69],\n",
      "        [     0.37,      0.85,      0.71,      0.24,      0.78,      0.23,     -0.12,      0.17,     -0.10,      0.47,      0.07,      0.50,      0.23,      0.07,     -0.57,     -0.32],\n",
      "        [    -0.62,     -0.37,     -0.00,      0.73,     -0.43,     -0.17,     -0.70,      0.18,      0.93,     -1.09,     -0.16,      0.57,     -1.17,     -0.35,      1.55,      1.05],\n",
      "        [    -0.33,      0.22,     -0.36,     -0.64,     -0.32,      1.34,      0.48,     -0.16,      0.02,      0.28,      1.05,     -0.68,      0.82,      0.11,     -0.94,     -0.24],\n",
      "        [     0.38,      0.30,      0.49,     -0.77,      0.41,      0.20,     -0.25,      0.48,      0.28,      0.12,     -0.89,      1.09,      0.12,     -0.76,     -0.47,     -0.56],\n",
      "        [     0.67,      0.70,      1.36,      1.56,      0.66,      0.58,      0.46,      0.84,      0.28,     -0.71,     -0.57,      0.34,     -0.05,      0.11,     -0.12,      0.59],\n",
      "        [    -0.01,     -0.66,     -0.32,     -0.83,     -0.11,     -0.20,      0.56,     -0.24,      0.05,     -1.16,     -0.70,      0.19,      0.38,     -1.16,     -0.14,      0.49],\n",
      "        [    -0.22,      0.57,     -0.13,      0.17,      0.52,     -0.05,     -0.59,     -0.44,      0.14,      0.08,      0.52,      0.23,      0.08,     -0.02,      0.05,      0.01],\n",
      "        [     0.13,      0.02,      0.12,      0.76,      0.67,     -0.84,     -0.55,     -0.16,      0.61,     -1.34,     -1.54,      1.04,     -0.28,     -0.98,      0.58,      0.82],\n",
      "        [    -0.08,     -0.32,     -0.76,      0.96,     -0.09,     -0.46,     -0.34,     -0.36,      0.44,     -0.84,     -1.11,     -0.32,      0.06,     -0.02,      0.42,      0.48],\n",
      "        [    -0.71,     -0.21,     -0.56,     -0.04,     -0.36,      0.51,     -0.43,     -0.32,      0.89,     -0.71,     -0.52,      0.50,     -0.01,     -0.76,     -0.01,      0.64],\n",
      "        [    -0.41,      0.52,     -0.25,      0.11,      0.41,      0.32,     -0.56,     -0.49,      0.46,     -0.22,      0.23,      0.41,      0.24,     -0.37,     -0.20,      0.25],\n",
      "        [    -0.43,     -0.18,     -0.03,      0.04,     -0.17,      0.11,     -0.16,     -0.19,      0.41,     -0.25,     -0.54,      0.68,     -0.18,     -0.56,     -0.16,      0.57],\n",
      "        [     0.14,      0.57,      0.47,      0.78,      0.35,      0.54,      0.16,      0.16,      0.06,     -0.02,      0.32,     -0.12,      0.18,      0.34,     -0.29,      0.19],\n",
      "        [    -0.66,      0.10,     -0.29,     -0.17,      0.29,     -0.22,     -0.69,     -0.69,      0.55,     -0.54,     -0.15,      1.00,     -0.17,     -0.96,      0.14,      0.68],\n",
      "        [    -0.09,      0.51,      0.31,     -0.58,      0.45,      0.59,      0.47,     -0.35,     -0.31,      0.22,      0.87,      0.15,      0.66,     -0.34,     -1.00,      0.04],\n",
      "        [    -0.06,     -0.39,     -0.20,     -0.44,      0.10,     -1.16,     -0.00,     -0.54,     -0.60,      0.15,      0.21,      0.05,     -0.24,     -0.14,      0.34,      0.08],\n",
      "        [    -0.32,     -0.38,     -0.46,     -0.38,     -0.50,      0.12,      0.06,     -0.08,      0.03,     -0.24,      0.70,     -0.55,     -0.13,      0.09,      0.51,      0.08],\n",
      "        [     0.17,     -0.25,     -0.39,     -0.04,     -0.42,      0.40,     -0.23,      0.52,      0.57,     -0.37,     -0.86,     -0.20,      0.04,     -0.02,      0.27,     -0.38]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "\n",
      "exact result :\n",
      "tensor([[    -0.20,     -0.19,      0.14,     -0.20,     -0.09,     -0.23,      0.03,     -0.12,     -0.27,      0.27,      0.34,      0.13,     -0.31,     -0.01,      0.15,      0.16],\n",
      "        [    -0.36,     -0.55,     -0.11,     -0.24,     -0.29,     -0.19,      0.06,     -0.18,      0.36,     -0.80,     -0.24,      0.49,     -0.34,     -0.70,      0.55,      0.80],\n",
      "        [     0.30,     -0.02,      0.36,      0.78,      0.29,     -0.68,      0.09,      0.08,     -0.08,     -0.22,     -0.84,      0.24,     -0.21,     -0.01,      0.12,      0.41],\n",
      "        [     0.12,      0.10,      0.13,      0.53,      0.66,     -1.14,     -0.41,     -0.37,      0.16,     -0.62,     -0.75,      0.62,     -0.16,     -0.48,      0.39,      0.59],\n",
      "        [    -0.12,     -0.55,     -0.54,      1.09,     -0.68,      0.31,      0.25,      0.22,      0.89,     -1.12,     -1.69,     -0.14,      0.05,     -0.15,      0.22,      0.87],\n",
      "        [     0.62,      0.29,      1.49,      0.48,      0.65,     -0.64,      0.52,      0.46,     -1.02,      0.46,      1.40,     -0.09,     -0.93,      0.57,      0.71,      0.09],\n",
      "        [     0.30,      0.66,      1.02,      0.80,      0.82,     -0.28,      0.06,      0.05,     -0.19,      0.01,     -0.00,      0.60,     -0.14,     -0.01,     -0.33,      0.38],\n",
      "        [    -0.42,     -0.24,     -1.13,      0.88,     -0.58,      0.03,     -0.79,     -0.02,      1.05,     -0.77,      0.08,     -0.82,     -0.50,      0.50,      1.61,      0.28],\n",
      "        [     0.02,     -0.34,     -0.14,     -0.27,     -0.40,     -0.01,      0.36,      0.05,     -0.68,      0.73,      0.44,     -0.70,     -0.02,      0.55,     -0.11,     -0.38],\n",
      "        [    -0.43,     -0.73,     -0.93,     -0.19,     -0.27,     -0.96,     -0.17,     -0.82,     -0.14,     -0.46,     -0.33,     -0.28,      0.08,     -0.32,      0.43,      0.60],\n",
      "        [     0.59,      0.40,      0.14,      1.11,      0.03,      0.54,      0.08,      0.77,      0.50,     -0.33,     -0.41,     -0.57,     -0.00,      0.64,      0.31,     -0.32],\n",
      "        [     0.40,      0.18,      0.58,     -0.01,      0.21,      0.08,      0.13,      0.50,      0.03,      0.10,     -0.32,      0.39,     -0.16,     -0.08,     -0.06,     -0.27],\n",
      "        [    -0.62,     -0.38,     -0.91,     -1.73,     -0.71,      0.61,     -0.36,     -0.24,      0.04,      0.71,      0.84,     -0.20,      0.20,     -0.13,     -0.11,     -0.84],\n",
      "        [     0.64,      0.11,      0.40,     -0.23,      0.27,     -0.21,      0.22,      0.45,     -0.34,      0.45,     -0.71,      0.15,      0.21,      0.02,     -0.48,     -0.70],\n",
      "        [     0.31,      0.75,      0.69,      0.18,      0.61,      0.24,     -0.10,      0.24,     -0.07,      0.44,      0.10,      0.44,      0.12,      0.10,     -0.59,     -0.39],\n",
      "        [    -0.54,     -0.35,     -0.10,      0.62,     -0.36,     -0.20,     -0.58,      0.10,      1.04,     -0.93,     -0.12,      0.55,     -1.12,     -0.30,      1.61,      1.00],\n",
      "        [    -0.24,      0.25,     -0.37,     -0.53,     -0.23,      1.23,      0.37,     -0.22,     -0.09,      0.07,      0.93,     -0.61,      0.86,      0.12,     -0.96,     -0.28],\n",
      "        [     0.32,      0.23,      0.51,     -0.83,      0.27,      0.19,     -0.17,      0.52,      0.31,      0.21,     -0.72,      1.09,      0.00,     -0.65,     -0.39,     -0.64],\n",
      "        [     0.68,      0.67,      1.20,      1.46,      0.57,      0.48,      0.50,      0.76,      0.38,     -0.77,     -0.49,      0.35,     -0.14,      0.08,     -0.09,      0.52],\n",
      "        [     0.04,     -0.59,     -0.19,     -0.69,      0.01,     -0.33,      0.55,     -0.40,     -0.03,     -1.17,     -0.59,      0.34,      0.48,     -1.12,     -0.20,      0.68],\n",
      "        [    -0.24,      0.50,     -0.09,      0.11,      0.40,     -0.07,     -0.56,     -0.37,      0.17,      0.09,      0.50,      0.17,      0.06,      0.03,     -0.02,     -0.01],\n",
      "        [     0.06,     -0.01,      0.16,      0.61,      0.52,     -0.91,     -0.44,     -0.16,      0.74,     -1.20,     -1.34,      1.07,     -0.28,     -0.91,      0.57,      0.90],\n",
      "        [    -0.12,     -0.25,     -0.77,      0.82,     -0.13,     -0.50,     -0.35,     -0.30,      0.55,     -0.78,     -1.06,     -0.25,      0.12,     -0.05,      0.44,      0.51],\n",
      "        [    -0.71,     -0.17,     -0.59,     -0.15,     -0.36,      0.43,     -0.45,     -0.33,      0.95,     -0.70,     -0.48,      0.60,      0.05,     -0.67,      0.07,      0.61],\n",
      "        [    -0.43,      0.46,     -0.23,      0.02,      0.29,      0.25,     -0.56,     -0.45,      0.49,     -0.24,      0.25,      0.40,      0.24,     -0.29,     -0.23,      0.22],\n",
      "        [    -0.49,     -0.16,     -0.05,     -0.04,     -0.20,      0.12,     -0.19,     -0.19,      0.46,     -0.26,     -0.49,      0.73,     -0.15,     -0.52,     -0.09,      0.58],\n",
      "        [     0.15,      0.55,      0.40,      0.74,      0.29,      0.49,      0.14,      0.15,      0.09,     -0.11,      0.28,     -0.13,      0.15,      0.31,     -0.30,      0.13],\n",
      "        [    -0.71,      0.05,     -0.22,     -0.26,      0.19,     -0.26,     -0.68,     -0.68,      0.59,     -0.50,     -0.08,      1.02,     -0.13,     -0.84,      0.11,      0.74],\n",
      "        [    -0.08,      0.45,      0.38,     -0.47,      0.43,      0.52,      0.39,     -0.41,     -0.43,      0.04,      0.83,      0.18,      0.67,     -0.31,     -1.09,      0.11],\n",
      "        [    -0.11,     -0.40,     -0.07,     -0.35,      0.14,     -1.07,     -0.01,     -0.53,     -0.66,      0.20,      0.20,      0.02,     -0.18,     -0.16,      0.24,      0.24],\n",
      "        [    -0.21,     -0.33,     -0.44,     -0.27,     -0.35,      0.08,      0.07,     -0.16,     -0.03,     -0.24,      0.64,     -0.53,     -0.06,      0.08,      0.48,      0.10],\n",
      "        [     0.22,     -0.20,     -0.45,     -0.11,     -0.40,      0.34,     -0.17,      0.54,      0.64,     -0.27,     -0.78,     -0.17,      0.01,      0.00,      0.39,     -0.51]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "\n",
      "Max error is 0.20 ✓\n"
     ]
    }
   ],
   "source": [
    "print('quanted result (with packing):')\n",
    "print(y_qdora.data)\n",
    "print()\n",
    "print('exact result :')\n",
    "print(y.data)\n",
    "print()\n",
    "assert_somehow_close(y_qdora, y, max_err=0.3)\n",
    "print(f'Max error is {max_abs_diff(y_qdora, y):.2f} ✓')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98828f0-d6f9-4ea6-90fb-0c0ed9156a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "050c8c0a-5341-4c30-8086-966bdcd9a527",
   "metadata": {},
   "source": [
    "Let's call backwards on the model, to make sure it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d8e4a-6adc-4bb0-9dd8-54727050af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_params(model): return L((name,p) for name,p in qdora_linear.named_parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd43544-9e0e-4e1f-9a99-8859d3e4ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(trainable_params(qdora_linear).itemgot(0)) == {'m','a.weight','b.weight'} # assert only the dora part is trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89483648-91a2-422b-9cef-bb4138b04055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.19, device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = y_qdora.sum() # arbitrary operation to make y_qdora a scalar\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aebc50-a137-4ad0-9f54-4678123c8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631658a8-80e3-4cef-a480-348d2a4cddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss shapes:\n",
      "Shape of grad of m        is [16]   ; shape of  m        is [16]\n",
      "Shape of grad of a.weight is [2, 8] ; shape of  a.weight is [2, 8]\n",
      "Shape of grad of b.weight is [16, 2]; shape of  b.weight is [16, 2]\n"
     ]
    }
   ],
   "source": [
    "print('Loss shapes:')\n",
    "for name,p in trainable_params(qdora_linear): print(f'Shape of grad of {name:<8} is {str(list(p.grad.shape)):<7}; shape of  {name:<8} is {list(p.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b3cee-6cdb-4c82-b26b-38899ee8ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- m.grad:\n",
      "tensor([ -2.47,   0.41,   0.48,   6.28,   3.83,  -1.70,  -5.94,  -1.81,   9.44, -13.06,  -7.28,   7.66,  -3.36,  -9.12,   4.94,  11.06], device='cuda:0', dtype=torch.bfloat16)\n",
      "--- a.weight.grad:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16)\n",
      "--- b.weight.grad:\n",
      "tensor([[11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06],\n",
      "        [11.00,  8.06]], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "for name,p in trainable_params(qdora_linear): print(f'--- {name}.grad:\\n{p.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e312c2c-c120-43f6-9682-5a82a73c545f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf5369ba-5ba8-4b02-ad8b-0103158feb9d",
   "metadata": {},
   "source": [
    "**Let's verify our implementation against hqq:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9f461-9f5d-4d46-9cfa-bf344b8e0a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0.23,      0.17,      0.08,      0.21,     -0.13,     -0.10,      0.27,      0.07],\n",
       "        [     0.05,     -0.10,      0.16,     -0.10,      0.08,     -0.03,      0.34,      0.03],\n",
       "        [     0.28,      0.09,      0.24,     -0.22,     -0.30,      0.05,      0.35,      0.32],\n",
       "        [    -0.14,     -0.18,      0.32,     -0.01,     -0.34,     -0.30,      0.19,     -0.19],\n",
       "        [     0.12,      0.00,      0.14,     -0.10,     -0.06,     -0.19,      0.24,      0.26],\n",
       "        [    -0.09,      0.06,      0.13,     -0.05,      0.32,      0.29,      0.18,     -0.31],\n",
       "        [     0.08,      0.34,     -0.00,      0.01,     -0.14,     -0.02,      0.17,      0.01],\n",
       "        [     0.20,      0.04,      0.15,      0.17,     -0.11,      0.16,      0.18,     -0.05],\n",
       "        [    -0.15,     -0.18,      0.29,      0.05,      0.25,      0.03,     -0.26,     -0.17],\n",
       "        [     0.26,     -0.09,     -0.34,     -0.10,     -0.07,      0.25,      0.30,     -0.02],\n",
       "        [    -0.31,     -0.23,     -0.21,     -0.14,     -0.13,      0.32,      0.32,      0.18],\n",
       "        [     0.23,     -0.01,      0.25,     -0.34,      0.09,      0.03,     -0.19,      0.32],\n",
       "        [     0.00,      0.25,     -0.07,      0.05,      0.34,     -0.16,      0.10,     -0.16],\n",
       "        [    -0.05,     -0.16,     -0.14,      0.13,     -0.21,      0.06,      0.32,     -0.22],\n",
       "        [    -0.23,     -0.35,      0.06,      0.25,     -0.32,      0.08,     -0.28,      0.10],\n",
       "        [    -0.25,     -0.01,      0.22,     -0.27,     -0.18,     -0.20,     -0.28,      0.09]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = base_linear.weight.data; W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4860fead-743f-4c5b-a88b-298d1d31cf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.27,  0.17,  0.07,  0.17, -0.12, -0.12,  0.27,  0.07],\n",
       "        [ 0.07, -0.12,  0.17, -0.12,  0.07, -0.03,  0.37,  0.07],\n",
       "        [ 0.27,  0.07,  0.27, -0.22, -0.32,  0.07,  0.37,  0.27],\n",
       "        [-0.12, -0.22,  0.37, -0.03, -0.32, -0.32,  0.17, -0.22],\n",
       "        [ 0.17, -0.03,  0.17, -0.12, -0.03, -0.22,  0.27,  0.27],\n",
       "        [-0.12,  0.07,  0.17, -0.03,  0.37,  0.27,  0.17, -0.32],\n",
       "        [ 0.07,  0.37, -0.03, -0.03, -0.12, -0.03,  0.17, -0.03],\n",
       "        [ 0.17,  0.07,  0.17,  0.17, -0.12,  0.17,  0.17, -0.03],\n",
       "        [-0.15, -0.15,  0.25,  0.05,  0.25,  0.05, -0.25, -0.15],\n",
       "        [ 0.25, -0.05, -0.35, -0.15, -0.05,  0.25,  0.35, -0.05],\n",
       "        [-0.35, -0.25, -0.25, -0.15, -0.15,  0.35,  0.35,  0.15],\n",
       "        [ 0.25, -0.05,  0.25, -0.35,  0.05,  0.05, -0.15,  0.35],\n",
       "        [ 0.05,  0.25, -0.05,  0.05,  0.35, -0.15,  0.15, -0.15],\n",
       "        [-0.05, -0.15, -0.15,  0.15, -0.25,  0.05,  0.35, -0.25],\n",
       "        [-0.25, -0.35,  0.05,  0.25, -0.35,  0.05, -0.25,  0.15],\n",
       "        [-0.25, -0.05,  0.25, -0.25, -0.15, -0.15, -0.25,  0.05]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_hqq = hqq_linear.dequantize(); W_hqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366410ef-94fb-4ec3-94a8-4049f6d11a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.27,  0.17,  0.07,  0.17, -0.12, -0.12,  0.27,  0.07],\n",
       "        [ 0.07, -0.12,  0.17, -0.12,  0.07, -0.03,  0.37,  0.07],\n",
       "        [ 0.27,  0.07,  0.27, -0.22, -0.32,  0.07,  0.37,  0.27],\n",
       "        [-0.12, -0.22,  0.37, -0.03, -0.32, -0.32,  0.17, -0.22],\n",
       "        [ 0.17, -0.03,  0.17, -0.12, -0.03, -0.22,  0.27,  0.27],\n",
       "        [-0.12,  0.07,  0.17, -0.03,  0.37,  0.27,  0.17, -0.32],\n",
       "        [ 0.07,  0.37, -0.03, -0.03, -0.12, -0.03,  0.17, -0.03],\n",
       "        [ 0.17,  0.07,  0.17,  0.17, -0.12,  0.17,  0.17, -0.03],\n",
       "        [-0.15, -0.15,  0.25,  0.05,  0.25,  0.05, -0.25, -0.15],\n",
       "        [ 0.25, -0.05, -0.35, -0.15, -0.05,  0.25,  0.35, -0.05],\n",
       "        [-0.35, -0.25, -0.25, -0.15, -0.15,  0.35,  0.35,  0.15],\n",
       "        [ 0.25, -0.05,  0.25, -0.35,  0.05,  0.05, -0.15,  0.35],\n",
       "        [ 0.05,  0.25, -0.05,  0.05,  0.35, -0.15,  0.15, -0.15],\n",
       "        [-0.05, -0.15, -0.15,  0.15, -0.25,  0.05,  0.35, -0.25],\n",
       "        [-0.25, -0.35,  0.05,  0.25, -0.35,  0.05, -0.25,  0.15],\n",
       "        [-0.25, -0.05,  0.25, -0.25, -0.15, -0.15, -0.25,  0.05]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_ours = qdora_linear.dequant(); W_ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86420c29-14a2-4b4b-a809-f7ef0ab62e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max abs diff between: W     and W_hqq : 0.05\n",
      "Max abs diff between: W     and W_ours: 0.05\n",
      "Max abs diff between: W_hqq and W_ours: 0.00\n"
     ]
    }
   ],
   "source": [
    "print(f'Max abs diff between: W     and W_hqq : {max_abs_diff(W, W_hqq):.2f}')\n",
    "print(f'Max abs diff between: W     and W_ours: {max_abs_diff(W, W_ours):.2f}')\n",
    "print(f'Max abs diff between: W_hqq and W_ours: {max_abs_diff(W_hqq, W_ours):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a619425e-0c23-422a-8e3a-967e3f121b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_somehow_close(W_ours, W_hqq, max_err=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e954dd9-ee73-4b71-864a-bc5358869a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4b619-6a13-4914-a51a-c4bb4ddd79e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
