{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035941a3-fd58-48e6-a43a-fea00f672b41",
   "metadata": {},
   "source": [
    "**Simplified python reference implementation of hqq-qdora.**\n",
    "\n",
    "This nb only contains the final module, not the build up to it. For the full build up, see `python_hqq_qdora_v2.ipynb` and `python_hqq_qdora.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225deadd-a13a-4146-bfb2-a6d91bf2b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor, cat, int32, float16 as fp16, bfloat16 as bf16\n",
    "from math import ceil\n",
    "\n",
    "from fastcore.foundation import L\n",
    "from fastcore.basics import store_attr, AttrDict\n",
    "\n",
    "from hqq.core.quantize import Quantizer, HQQLinear, BaseQuantizeConfig # Quantizer - optional, only for optimizing during quanting ; HQQLinear & BaseQuantizeConfig to verify our implementation\n",
    "\n",
    "torch.set_printoptions(linewidth=200, precision=2, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f82477-707c-4358-8a2d-acf9202d4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_abs_diff(a,b): return (a-b).abs().max()\n",
    "def assert_close(a,b): assert torch.isclose(a,b,atol=1e-2).all(), f'assert_close failed, max error = {max_abs_diff(a,b)}'\n",
    "def assert_somehow_close(a,b,max_err=0.12): assert torch.isclose(a,b,atol=max_err).all(), f'assert_somehow_close failed, max error = {max_abs_diff(a,b)}' # allow some error due to quanting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c78eda9-5478-483b-9391-ab2537aedc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,r,n = 128,32,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9d7e70-f910-4f1e-a241-5577c19a9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from `python_hqq_qdora_clean.ipynb`\n",
    "# changes\n",
    "# - unpack and _dequant keep data on device it's on\n",
    "# - only dequant/unpack ourselves, and use hqq for quanting/packing\n",
    "# - zero/scale are not packed (as is hqq)\n",
    "# - a (of lora) is initialized correctly\n",
    "\n",
    "class QuantedDoraModule(nn.Module):\n",
    "    def __init__(self, hqq_linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.device = hqq_linear.device        \n",
    "        self.hqq_linear = hqq_linear\n",
    "        # save all metadata, we need for dequanting\n",
    "        meta, meta_zero, meta_scale = AttrDict(hqq_linear.meta),  AttrDict(hqq_linear.meta['meta_zero']),  AttrDict(hqq_linear.meta['meta_scale'])\n",
    "        self.qdata = hqq_linear.W_q.data\n",
    "        self.qzero, self.qscale  = (meta.zero_scale[0], meta.zero_scale[1]) if 'zero_scale' in meta else meta.zero_q, meta.scale_q\n",
    "        self.zeros_zero,  self.zeros_scale  = meta_zero.zero, meta_zero.scale        \n",
    "        self.scales_zero, self.scales_scale = meta_scale.zero, meta_scale.scale\n",
    "        self.data_shape, self.zero_shape, self.scale_shape = meta.shape, meta_zero.shape, meta_scale.shape\n",
    "        self.group_size, self.group_size_zero, self.group_size_scale = meta.group_size, meta_zero.group_size, meta_scale.group_size\n",
    "        self.compute_dtype = meta.compute_dtype\n",
    "        # for dora\n",
    "        self.a = nn.Linear(hqq_linear.in_features, rank,  bias=False, dtype=self.compute_dtype, device=self.device)\n",
    "        self.b = nn.Linear(rank, hqq_linear.out_features, bias=False, dtype=self.compute_dtype, device=self.device)        \n",
    "        self.alpha = alpha\n",
    "        self.m = nn.Parameter(hqq_linear.dequantize().norm(p=2, dim=1))\n",
    "        # init a & b\n",
    "        self.a.weight.data = torch.randn(rank, hqq_linear.in_features).to(dtype=self.compute_dtype, device=self.device) / (rank**0.5)\n",
    "        self.b.weight.data.zero_()\n",
    "\n",
    "    @staticmethod\n",
    "    def unpack(packed, rows):\n",
    "        def bin_to_dec(b3,b2,b1): return 4*b3 + 2*b2 + b1\n",
    "        assert len(packed.shape)==2 and packed.dtype==int32, 'Pass a 2d tensor of int32s'\n",
    "        n_packs, cols = packed.shape\n",
    "        padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "        for k_up, k_down in zip(range(10), reversed(range(10))): # top-most 3bits vals (k_up=0) are most right-shifted (k_down=9)\n",
    "            padded_vals[k_down*n_packs:(k_down+1)*n_packs,:] = ((packed >> (3*k_up)) & 0b111) # righ-shift 3*k_up times, so last 3 bits are those we want; then only select those via 0b111            \n",
    "        return padded_vals[:rows,:].to(packed.device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _dequant(data, zero, scale, shape, group_size, packed):\n",
    "        if packed: data = QuantedDoraModule.unpack(data, rows=group_size)\n",
    "        data = (data-zero)*scale\n",
    "        return data.reshape(shape).to(data.device)\n",
    "\n",
    "    def dequant(self):\n",
    "        zero  = self._dequant(self.qzero,  self.zeros_zero,  self.zeros_scale,  self.zero_shape,  self.group_size_zero,  packed=False) # zero/scale are uint8, so don't require unpacking\n",
    "        scale = self._dequant(self.qscale, self.scales_zero, self.scales_scale, self.scale_shape, self.group_size_scale, packed=False)\n",
    "        return  self._dequant(self.qdata,  zero,             scale,             self.data_shape,  self.group_size, packed=True).to(self.compute_dtype)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, self.dequant()) + self.b(self.a(x)) # use F.linear for batched matmul works\n",
    "        col_norms =  (self.dequant() + self.b.weight @ self.a.weight).norm(p=2, dim=1).detach()\n",
    "        x /= col_norms\n",
    "        x *= self.m * self.alpha\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e715d-332b-470c-8494-3d00851a69d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a222bac-eb05-4f40-8316-68b181ea1721",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 32       # batch size\n",
    "m,n = 16,8 # out, in\n",
    "r = 2       # lora rank\n",
    "\n",
    "gz = 64     # group size\n",
    "\n",
    "base_linear = nn.Linear(n,m,bias=False, dtype=bf16, device='cuda')\n",
    "\n",
    "ngroups = base_linear.weight.numel()//gz\n",
    "\n",
    "# equals BaseQuantizeConfig(nbits=3, group_size=gz, quant_zero=True, quant_scale=True, offload_meta=True, view_as_float=True),\n",
    "# but with group_size for scale & zero set to ngroups, instead of default 128\n",
    "quant_cfg = dict(\n",
    "    # note: Kerem used view_as_float=True, which stores quanted, packed weights as compute_dtype (for us: bf16) instead of int32\n",
    "    weight_quant_params = dict(nbits=3, group_size=gz,      bitpack=True, optimize=True),\n",
    "    # note: hqq sets nbits for scale/zero to 8, regardless of nbits for weights; nbits=3 result in error further below     \n",
    "    scale_quant_params  = dict(nbits=8, group_size=ngroups, bitpack=True, optimize=False), \n",
    "    zero_quant_params   = dict(nbits=8, group_size=ngroups, bitpack=True, optimize=False),\n",
    "    offload_meta = False # note: 1) Kerem used offload_meta=True; 2) offload_meta=True concats meta['zero_q'] & meta['scale_q'] together into meta['zero_scale']\n",
    ")\n",
    "hqq_linear = HQQLinear(base_linear, quant_cfg, compute_dtype=bf16)\n",
    "assert hqq_linear.W_q.dtype==int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03835012-53ae-44d0-9c64-a5424aacce50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ee2a8c-6952-4941-842a-4374b6bc6f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantedDoraModule(\n",
       "  (hqq_linear): HQQLinear(in_features=8, out_features=16, bias=False)\n",
       "  (a): Linear(in_features=8, out_features=2, bias=False)\n",
       "  (b): Linear(in_features=2, out_features=16, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdora_linear = QuantedDoraModule(hqq_linear, r, 1.0); qdora_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca43823f-f789-44e4-b01f-d905c0b9e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((b, n), device='cuda', dtype=bf16) # batched input\n",
    "y = base_linear(x)\n",
    "y_qdora = qdora_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fac1c709-f65d-45e8-b2e2-4dc284667285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: x = torch.Size([32, 8]) ; y = torch.Size([32, 16]) ; y_qdora = torch.Size([32, 16])\n"
     ]
    }
   ],
   "source": [
    "print(f'Shapes: x = {x.shape} ; y = {y.shape} ; y_qdora = {y_qdora.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0d8899f-8246-4cdb-adf8-8d32ad10d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quanted result (with packing):\n",
      "tensor([[    -0.20,      0.60,      0.04,     -0.60,     -0.40,     -0.07,     -0.64,     -0.74,      0.29,      0.15,     -1.36,     -0.84,     -0.55,     -0.03,      0.89,     -0.72],\n",
      "        [     0.54,      0.48,      0.34,     -0.02,     -0.37,      0.17,     -0.46,      0.13,     -0.51,      0.33,     -0.73,      0.09,      0.56,     -0.01,      1.23,     -0.13],\n",
      "        [    -0.56,     -0.09,     -0.72,      0.77,     -0.95,      0.41,      0.33,      1.02,     -1.45,     -0.88,      0.54,     -1.11,     -0.47,      0.01,     -1.29,     -0.38],\n",
      "        [     1.23,     -0.77,     -0.75,      0.17,     -0.18,     -1.68,      0.88,      0.07,     -0.21,     -0.39,      0.82,      0.40,     -0.39,     -0.93,      0.12,      0.46],\n",
      "        [     0.35,     -0.35,     -0.07,     -0.82,     -0.06,     -0.25,      0.06,     -0.02,      0.54,      0.29,     -0.37,      0.71,      0.17,     -0.29,     -0.61,      0.48],\n",
      "        [    -0.22,      0.10,      0.25,      0.29,      0.24,      0.00,     -0.13,      0.05,      0.21,     -0.11,      0.10,     -0.24,      0.33,      0.23,      0.11,      0.11],\n",
      "        [     0.01,      0.67,     -0.03,      0.96,      0.27,      0.20,      0.38,      0.86,     -0.21,      0.68,      0.59,      0.15,     -0.68,      0.59,     -0.35,      0.01],\n",
      "        [     0.02,     -0.64,      0.13,      0.61,      0.20,      0.19,      0.56,      0.06,     -1.30,     -0.51,      1.14,      0.21,      0.28,     -0.48,      0.57,     -0.43],\n",
      "        [     0.22,      0.17,     -0.74,      0.55,     -0.50,     -0.19,      0.35,      0.57,     -0.50,     -0.15,      0.42,     -0.31,     -0.77,      0.08,     -0.64,      0.01],\n",
      "        [    -1.18,      0.79,     -0.14,      0.96,     -0.42,      1.11,     -0.59,     -0.25,     -0.95,     -0.77,     -0.02,     -1.84,     -0.52,      0.69,      0.59,     -1.20],\n",
      "        [    -0.04,     -1.01,     -0.29,      0.24,      0.14,     -0.24,      0.65,      0.05,     -0.50,     -0.80,      1.19,      0.19,      0.21,     -0.50,     -0.60,      0.17],\n",
      "        [     0.50,     -0.19,      0.08,     -0.41,     -0.39,     -1.22,      0.28,      0.98,      0.30,      0.28,     -0.68,      0.04,      0.29,     -0.51,     -0.85,      0.68],\n",
      "        [     0.39,      0.19,      0.59,      0.32,     -0.10,     -0.70,      0.40,      0.91,     -0.86,      0.52,     -0.33,     -0.05,      0.10,     -0.52,      0.69,     -0.31],\n",
      "        [    -1.02,      0.56,      0.63,     -0.03,     -0.73,      0.24,     -0.69,      0.98,     -0.31,     -0.25,     -1.43,     -1.63,      0.70,      0.23,     -0.58,     -0.26],\n",
      "        [    -0.29,      0.74,     -0.48,      0.09,     -0.62,      0.91,     -0.47,     -0.58,     -0.68,     -0.05,     -0.48,     -0.75,     -1.02,      0.33,      0.72,     -1.05],\n",
      "        [    -0.01,      0.33,      0.02,     -0.02,     -0.09,      0.37,     -0.22,     -0.18,     -0.15,      0.20,     -0.25,     -0.02,     -0.17,      0.16,      0.43,     -0.29],\n",
      "        [    -0.61,     -0.38,     -0.09,     -0.09,      0.32,      0.58,      0.04,     -0.05,      0.48,     -0.27,      0.55,      0.20,      0.20,      0.34,     -1.22,      0.37],\n",
      "        [    -0.62,     -0.16,      0.32,      0.39,      0.48,      0.47,      0.08,     -0.38,     -0.33,     -0.32,      0.59,     -0.25,      0.05,      0.07,      0.42,     -0.52],\n",
      "        [     0.29,      0.05,     -0.49,      0.17,      0.13,      0.64,      0.00,     -0.47,      0.01,      0.06,      0.67,      0.66,     -0.35,      0.35,      0.14,      0.07],\n",
      "        [     0.10,      0.46,     -0.31,      0.74,     -0.02,      0.42,      0.03,     -0.62,     -0.91,     -0.09,      0.50,     -0.34,     -0.92,      0.12,      1.48,     -1.00],\n",
      "        [    -0.51,     -0.28,      0.51,     -1.70,     -0.84,      0.00,     -0.63,      0.05,      0.14,     -0.06,     -2.08,     -0.54,      0.73,     -0.66,     -0.63,     -0.19],\n",
      "        [     0.42,     -0.43,     -0.81,      0.22,     -0.26,     -0.70,      0.57,      0.19,     -0.12,     -0.39,      0.69,      0.03,     -0.61,     -0.31,     -0.71,      0.29],\n",
      "        [    -0.09,     -0.18,      1.48,     -1.00,      0.67,     -0.43,     -0.39,     -0.61,      0.58,      0.52,     -1.20,      0.30,      1.14,     -0.59,      1.48,     -0.24],\n",
      "        [     0.07,      0.15,      0.16,      0.02,      0.43,     -0.44,      0.47,      0.35,      0.40,      0.74,      0.06,      0.40,     -0.62,     -0.01,     -0.39,      0.06],\n",
      "        [    -0.09,     -0.18,     -0.16,      0.89,     -0.40,     -1.30,      0.54,      0.86,     -0.77,     -0.76,      0.31,     -1.36,     -0.22,     -0.51,     -0.15,     -0.19],\n",
      "        [     0.47,     -0.48,     -0.36,     -1.32,     -0.86,     -0.50,     -0.31,      0.27,      0.64,     -0.14,     -1.11,      0.28,      0.67,     -0.44,     -1.26,      0.92],\n",
      "        [     0.61,     -0.19,      0.16,     -0.77,     -0.26,     -0.11,     -0.55,     -0.09,      0.57,      0.03,     -0.71,      0.60,      1.21,     -0.10,      0.12,      0.83],\n",
      "        [    -0.29,      0.05,     -0.03,      0.29,      0.06,      0.24,     -0.09,     -0.59,     -0.43,     -0.38,      0.22,     -0.51,     -0.29,     -0.02,      0.83,     -0.66],\n",
      "        [    -0.41,      0.13,      0.14,      0.91,      0.61,      0.31,      0.21,      0.04,     -0.09,     -0.11,      0.97,     -0.14,     -0.12,      0.45,      0.13,     -0.12],\n",
      "        [    -0.19,     -0.17,      0.14,     -0.42,      0.52,      0.89,     -0.27,     -1.22,      0.24,      0.03,      0.24,      0.62,      0.03,      0.13,      0.78,     -0.36],\n",
      "        [    -0.69,     -0.76,     -0.02,     -0.51,     -0.26,     -0.78,     -0.01,     -0.13,      0.34,     -1.03,     -0.35,     -1.09,      0.36,     -0.56,     -0.88,      0.06],\n",
      "        [     0.17,     -0.48,     -0.07,      0.30,      0.13,     -0.66,      0.43,      0.26,     -0.10,     -0.38,      0.61,     -0.00,      0.21,     -0.32,     -0.24,      0.30]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "\n",
      "exact result :\n",
      "tensor([[-0.21,  0.61,  0.06, -0.63, -0.35, -0.16, -0.65, -0.83,  0.15,  0.20, -1.34, -0.79, -0.40, -0.04,  0.75, -0.68],\n",
      "        [ 0.44,  0.44,  0.38,  0.08, -0.27,  0.11, -0.50,  0.03, -0.46,  0.29, -0.74,  0.07,  0.54, -0.02,  1.23, -0.19],\n",
      "        [-0.59, -0.13, -0.91,  0.72, -1.02,  0.66,  0.24,  1.14, -1.31, -0.93,  0.44, -0.94, -0.51,  0.05, -1.18, -0.46],\n",
      "        [ 1.02, -0.75, -0.56,  0.22, -0.14, -1.75,  0.93,  0.30, -0.08, -0.45,  0.94,  0.40, -0.39, -1.07,  0.12,  0.46],\n",
      "        [ 0.29, -0.33, -0.02, -0.76, -0.07, -0.24,  0.10, -0.02,  0.54,  0.27, -0.33,  0.68,  0.15, -0.30, -0.62,  0.50],\n",
      "        [-0.13,  0.12,  0.24,  0.26,  0.25, -0.05, -0.13,  0.04,  0.19, -0.14,  0.09, -0.26,  0.29,  0.24,  0.16,  0.09],\n",
      "        [ 0.09,  0.75, -0.10,  0.89,  0.12,  0.17,  0.46,  0.90, -0.25,  0.80,  0.55,  0.14, -0.67,  0.57, -0.30, -0.02],\n",
      "        [-0.09, -0.75,  0.12,  0.63,  0.25,  0.35,  0.46,  0.10, -1.15, -0.53,  1.09,  0.21,  0.22, -0.48,  0.58, -0.40],\n",
      "        [ 0.19,  0.20, -0.77,  0.52, -0.57, -0.17,  0.38,  0.70, -0.45, -0.13,  0.42, -0.24, -0.75,  0.06, -0.60, -0.03],\n",
      "        [-1.03,  0.71, -0.31,  0.83, -0.41,  1.17, -0.70, -0.31, -0.99, -0.74, -0.12, -1.69, -0.43,  0.80,  0.57, -1.21],\n",
      "        [-0.10, -1.06, -0.28,  0.25,  0.15, -0.10,  0.61,  0.17, -0.37, -0.86,  1.19,  0.21,  0.13, -0.50, -0.55,  0.19],\n",
      "        [ 0.41, -0.09,  0.12, -0.36, -0.40, -1.24,  0.33,  1.09,  0.33,  0.18, -0.65,  0.03,  0.21, -0.62, -0.76,  0.58],\n",
      "        [ 0.26,  0.22,  0.59,  0.36, -0.10, -0.67,  0.38,  0.93, -0.82,  0.52, -0.38, -0.07,  0.06, -0.64,  0.71, -0.37],\n",
      "        [-0.93,  0.59,  0.44, -0.07, -0.71,  0.33, -0.77,  0.90, -0.34, -0.38, -1.55, -1.53,  0.62,  0.26, -0.45, -0.42],\n",
      "        [-0.31,  0.68, -0.55,  0.05, -0.62,  0.95, -0.53, -0.66, -0.73,  0.06, -0.52, -0.64, -0.86,  0.40,  0.58, -1.00],\n",
      "        [-0.01,  0.30,  0.01, -0.02, -0.09,  0.36, -0.23, -0.24, -0.17,  0.24, -0.27, -0.02, -0.13,  0.18,  0.38, -0.27],\n",
      "        [-0.48, -0.38, -0.17, -0.13,  0.26,  0.66,  0.05, -0.04,  0.47, -0.28,  0.53,  0.20,  0.15,  0.42, -1.16,  0.39],\n",
      "        [-0.55, -0.21,  0.27,  0.33,  0.49,  0.54,  0.02, -0.42, -0.35, -0.28,  0.55, -0.25,  0.05,  0.11,  0.39, -0.46],\n",
      "        [ 0.29,  0.01, -0.46,  0.18,  0.12,  0.61,  0.02, -0.48,  0.03,  0.13,  0.70,  0.64, -0.31,  0.41,  0.10,  0.13],\n",
      "        [ 0.06,  0.39, -0.29,  0.70, -0.00,  0.39, -0.01, -0.64, -0.92,  0.03,  0.50, -0.30, -0.78,  0.13,  1.35, -0.93],\n",
      "        [-0.62, -0.30,  0.43, -1.62, -0.77,  0.19, -0.72, -0.08,  0.11, -0.17, -2.12, -0.48,  0.70, -0.64, -0.66, -0.22],\n",
      "        [ 0.35, -0.40, -0.76,  0.21, -0.30, -0.68,  0.61,  0.36, -0.05, -0.41,  0.75,  0.07, -0.59, -0.36, -0.70,  0.29],\n",
      "        [-0.13, -0.19,  1.56, -0.93,  0.79, -0.49, -0.43, -0.80,  0.48,  0.49, -1.20,  0.17,  1.12, -0.64,  1.39, -0.19],\n",
      "        [ 0.09,  0.25,  0.17, -0.03,  0.30, -0.47,  0.55,  0.38,  0.31,  0.86,  0.08,  0.36, -0.58, -0.09, -0.43,  0.09],\n",
      "        [-0.12, -0.12, -0.17,  0.82, -0.39, -1.30,  0.51,  1.05, -0.70, -0.86,  0.30, -1.27, -0.25, -0.63, -0.05, -0.29],\n",
      "        [ 0.36, -0.46, -0.32, -1.20, -0.79, -0.46, -0.29,  0.29,  0.71, -0.31, -1.06,  0.30,  0.59, -0.44, -1.19,  0.84],\n",
      "        [ 0.55, -0.22,  0.25, -0.64, -0.13, -0.19, -0.54, -0.16,  0.64, -0.12, -0.67,  0.54,  1.10, -0.08,  0.19,  0.76],\n",
      "        [-0.28, -0.00, -0.03,  0.25,  0.10,  0.25, -0.15, -0.61, -0.44, -0.35,  0.21, -0.47, -0.22,  0.00,  0.76, -0.61],\n",
      "        [-0.27,  0.13,  0.10,  0.83,  0.56,  0.28,  0.21,  0.06, -0.11, -0.06,  0.94, -0.16, -0.13,  0.47,  0.16, -0.11],\n",
      "        [-0.17, -0.26,  0.17, -0.41,  0.56,  0.91, -0.30, -1.35,  0.20,  0.11,  0.25,  0.58,  0.10,  0.22,  0.65, -0.23],\n",
      "        [-0.66, -0.74, -0.04, -0.55, -0.21, -0.69, -0.06, -0.05,  0.34, -1.18, -0.34, -1.02,  0.32, -0.58, -0.84,  0.04],\n",
      "        [ 0.15, -0.47, -0.03,  0.30,  0.14, -0.65,  0.44,  0.37, -0.04, -0.44,  0.63, -0.01,  0.15, -0.37, -0.17,  0.28]], device='cuda:0', dtype=torch.bfloat16)\n",
      "\n",
      "Max error is 0.25 ✓\n"
     ]
    }
   ],
   "source": [
    "print('quanted result (with packing):')\n",
    "print(y_qdora.data)\n",
    "print()\n",
    "print('exact result :')\n",
    "print(y.data)\n",
    "print()\n",
    "assert_somehow_close(y_qdora, y, max_err=0.3)\n",
    "print(f'Max error is {max_abs_diff(y_qdora, y):.2f} ✓')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98828f0-d6f9-4ea6-90fb-0c0ed9156a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "050c8c0a-5341-4c30-8086-966bdcd9a527",
   "metadata": {},
   "source": [
    "Let's call backwards on the model, to make sure it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf9d8e4a-6adc-4bb0-9dd8-54727050af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_params(model): return L((name,p) for name,p in qdora_linear.named_parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afd43544-9e0e-4e1f-9a99-8859d3e4ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(trainable_params(qdora_linear).itemgot(0)) == {'m','a.weight','b.weight'} # assert only the dora part is trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89483648-91a2-422b-9cef-bb4138b04055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-28.38, device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = y_qdora.sum() # arbitrary operation to make y_qdora a scalar\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97aebc50-a137-4ad0-9f54-4678123c8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "631658a8-80e3-4cef-a480-348d2a4cddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss shapes:\n",
      "Shape of grad of m        is [16]   ; shape of  m        is [16]\n",
      "Shape of grad of a.weight is [2, 8] ; shape of  a.weight is [2, 8]\n",
      "Shape of grad of b.weight is [16, 2]; shape of  b.weight is [16, 2]\n"
     ]
    }
   ],
   "source": [
    "print('Loss shapes:')\n",
    "for name,p in trainable_params(qdora_linear): print(f'Shape of grad of {name:<8} is {str(list(p.grad.shape)):<7}; shape of  {name:<8} is {list(p.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "284b3cee-6cdb-4c82-b26b-38899ee8ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- m.grad:\n",
      "tensor([ -3.88,  -1.54,  -1.24,   2.14,  -8.12,  -1.60,   0.99,   1.09, -11.69,  -8.00,  -1.25, -10.31,  -1.73,  -5.25,   2.17,  -9.12], device='cuda:0', dtype=torch.bfloat16)\n",
      "--- a.weight.grad:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16)\n",
      "--- b.weight.grad:\n",
      "tensor([[-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69],\n",
      "        [-12.56,  -9.69]], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "for name,p in trainable_params(qdora_linear): print(f'--- {name}.grad:\\n{p.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e312c2c-c120-43f6-9682-5a82a73c545f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf5369ba-5ba8-4b02-ad8b-0103158feb9d",
   "metadata": {},
   "source": [
    "**Let's verify our implementation against hqq:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ee9f461-9f5d-4d46-9cfa-bf344b8e0a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.16, -0.11, -0.08,  0.03,  0.17,  0.11,  0.13, -0.30],\n",
       "        [-0.16,  0.34,  0.16,  0.05,  0.13, -0.26,  0.28, -0.06],\n",
       "        [ 0.07,  0.20,  0.06, -0.08, -0.28,  0.17, -0.11, -0.29],\n",
       "        [-0.20,  0.32, -0.03, -0.03,  0.16,  0.26,  0.24,  0.20],\n",
       "        [-0.08,  0.12, -0.09, -0.25, -0.02,  0.17, -0.22, -0.08],\n",
       "        [ 0.11,  0.34,  0.03,  0.18,  0.31, -0.34, -0.34,  0.29],\n",
       "        [-0.16, -0.06, -0.31, -0.06, -0.11,  0.23,  0.08,  0.03],\n",
       "        [-0.33,  0.10, -0.05,  0.34, -0.31,  0.18,  0.29, -0.02],\n",
       "        [-0.21, -0.27,  0.15, -0.11, -0.02, -0.20, -0.21, -0.14],\n",
       "        [-0.22,  0.29, -0.16,  0.02,  0.00, -0.18,  0.08, -0.34],\n",
       "        [-0.22,  0.01, -0.30, -0.17,  0.27,  0.34, -0.11,  0.33],\n",
       "        [-0.19, -0.06, -0.29,  0.06,  0.16,  0.03, -0.28, -0.30],\n",
       "        [ 0.09, -0.04,  0.24,  0.11, -0.18,  0.20, -0.28, -0.23],\n",
       "        [-0.20,  0.26,  0.18,  0.06,  0.29, -0.28, -0.06,  0.18],\n",
       "        [ 0.30,  0.34,  0.10, -0.30,  0.23,  0.27,  0.16, -0.34],\n",
       "        [-0.28, -0.25,  0.03,  0.11, -0.01,  0.03, -0.15, -0.14]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = base_linear.weight.data; W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4860fead-743f-4c5b-a88b-298d1d31cf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.12, -0.15, -0.12,  0.05,  0.16,  0.15,  0.16, -0.35],\n",
       "        [-0.12,  0.34,  0.16,  0.05,  0.16, -0.25,  0.25, -0.05],\n",
       "        [ 0.07,  0.24,  0.07, -0.05, -0.30,  0.15, -0.12, -0.25],\n",
       "        [-0.21,  0.34, -0.03, -0.05,  0.16,  0.24,  0.25,  0.24],\n",
       "        [-0.12,  0.15, -0.12, -0.25, -0.03,  0.15, -0.21, -0.05],\n",
       "        [ 0.07,  0.34,  0.07,  0.15,  0.34, -0.35, -0.30,  0.24],\n",
       "        [-0.12, -0.05, -0.30, -0.05, -0.12,  0.24,  0.07,  0.05],\n",
       "        [-0.30,  0.15, -0.03,  0.34, -0.30,  0.15,  0.25, -0.05],\n",
       "        [-0.21, -0.25,  0.16, -0.15, -0.03, -0.25, -0.21, -0.15],\n",
       "        [-0.21,  0.24, -0.12,  0.05, -0.03, -0.15,  0.07, -0.35],\n",
       "        [-0.21,  0.05, -0.30, -0.15,  0.25,  0.34, -0.12,  0.34],\n",
       "        [-0.21, -0.05, -0.30,  0.05,  0.16,  0.05, -0.30, -0.35],\n",
       "        [ 0.07, -0.05,  0.25,  0.15, -0.21,  0.24, -0.30, -0.25],\n",
       "        [-0.21,  0.24,  0.16,  0.05,  0.25, -0.25, -0.03,  0.15],\n",
       "        [ 0.34,  0.34,  0.07, -0.35,  0.25,  0.24,  0.16, -0.35],\n",
       "        [-0.30, -0.25,  0.07,  0.15, -0.03,  0.05, -0.12, -0.15]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_hqq = hqq_linear.dequantize(); W_hqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "366410ef-94fb-4ec3-94a8-4049f6d11a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.12, -0.15, -0.12,  0.05,  0.16,  0.15,  0.16, -0.35],\n",
       "        [-0.12,  0.34,  0.16,  0.05,  0.16, -0.25,  0.25, -0.05],\n",
       "        [ 0.07,  0.24,  0.07, -0.05, -0.30,  0.15, -0.12, -0.25],\n",
       "        [-0.21,  0.34, -0.03, -0.05,  0.16,  0.24,  0.25,  0.24],\n",
       "        [-0.12,  0.15, -0.12, -0.25, -0.03,  0.15, -0.21, -0.05],\n",
       "        [ 0.07,  0.34,  0.07,  0.15,  0.34, -0.35, -0.30,  0.24],\n",
       "        [-0.12, -0.05, -0.30, -0.05, -0.12,  0.24,  0.07,  0.05],\n",
       "        [-0.30,  0.15, -0.03,  0.34, -0.30,  0.15,  0.25, -0.05],\n",
       "        [-0.21, -0.25,  0.16, -0.15, -0.03, -0.25, -0.21, -0.15],\n",
       "        [-0.21,  0.24, -0.12,  0.05, -0.03, -0.15,  0.07, -0.35],\n",
       "        [-0.21,  0.05, -0.30, -0.15,  0.25,  0.34, -0.12,  0.34],\n",
       "        [-0.21, -0.05, -0.30,  0.05,  0.16,  0.05, -0.30, -0.35],\n",
       "        [ 0.07, -0.05,  0.25,  0.15, -0.21,  0.24, -0.30, -0.25],\n",
       "        [-0.21,  0.24,  0.16,  0.05,  0.25, -0.25, -0.03,  0.15],\n",
       "        [ 0.34,  0.34,  0.07, -0.35,  0.25,  0.24,  0.16, -0.35],\n",
       "        [-0.30, -0.25,  0.07,  0.15, -0.03,  0.05, -0.12, -0.15]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_ours = qdora_linear.dequant(); W_ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86420c29-14a2-4b4b-a809-f7ef0ab62e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max abs diff between: W     and W_hqq : 0.05\n",
      "Max abs diff between: W     and W_ours: 0.05\n",
      "Max abs diff between: W_hqq and W_ours: 0.00\n"
     ]
    }
   ],
   "source": [
    "print(f'Max abs diff between: W     and W_hqq : {max_abs_diff(W, W_hqq):.2f}')\n",
    "print(f'Max abs diff between: W     and W_ours: {max_abs_diff(W, W_ours):.2f}')\n",
    "print(f'Max abs diff between: W_hqq and W_ours: {max_abs_diff(W_hqq, W_ours):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a619425e-0c23-422a-8e3a-967e3f121b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_somehow_close(W_ours, W_hqq, max_err=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e954dd9-ee73-4b71-864a-bc5358869a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4b619-6a13-4914-a51a-c4bb4ddd79e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
