{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035941a3-fd58-48e6-a43a-fea00f672b41",
   "metadata": {},
   "source": [
    "**Simplified python reference implementation of hqq-qdora.**\n",
    "\n",
    "This nb only contains the final module, not the build up to it. For the full build up, see `python_hqq_qdora_v2.ipynb` and `python_hqq_qdora.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225deadd-a13a-4146-bfb2-a6d91bf2b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor, cat, int32, float16 as fp16\n",
    "from math import ceil\n",
    "\n",
    "from fastcore.basics import store_attr\n",
    "\n",
    "from hqq.core.quantize import Quantizer, HQQLinear, BaseQuantizeConfig # Quantizer - optional, only for optimizing during quanting ; HQQLinear & BaseQuantizeConfig to verify our implementation\n",
    "\n",
    "torch.set_printoptions(linewidth=200, precision=2, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f82477-707c-4358-8a2d-acf9202d4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_abs_diff(a,b): return (a-b).abs().max()\n",
    "def assert_close(a,b): assert torch.isclose(a,b,atol=1e-2).all(), f'assert_close failed, max error = {max_abs_diff(a,b)}'\n",
    "def assert_somehow_close(a,b,max_err=0.12): assert torch.isclose(a,b,atol=max_err).all(), f'assert_somehow_close failed, max error = {max_abs_diff(a,b)}' # allow some error due to quanting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c78eda9-5478-483b-9391-ab2537aedc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,r,n = 128,32,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9d7e70-f910-4f1e-a241-5577c19a9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantedDoraModule(nn.Module):\n",
    "    def __init__(self, linear, bits, group_size, rank, alpha, compute_dtype=fp16, packed=True, optimized=True, group_size_zero=None, group_size_scale=None):\n",
    "        super().__init__()\n",
    "        # for quanting\n",
    "        store_attr('bits,group_size,packed,optimized,compute_dtype',self)\n",
    "        self.group_size_zero, self.group_size_scale = group_size_zero or 128, group_size_scale or 128 # hqq uses group size of 128 for zero & scale\n",
    "        self.quant(linear.weight.data)\n",
    "        # for dora\n",
    "        self.a = nn.Linear(linear.in_features, rank, bias=False, dtype=fp16)\n",
    "        self.b = nn.Linear(rank, linear.out_features, bias=False, dtype=fp16)\n",
    "        self.alpha = alpha\n",
    "        self.m = nn.Parameter(linear.weight.norm(p=2, dim=1))\n",
    "        # init a & b to 0 -- a should be inited differently, but for sake of simplicity, set it to 0 as well\n",
    "        self.a.weight.data.zero_()\n",
    "        self.b.weight.data.zero_()\n",
    "\n",
    "    @staticmethod\n",
    "    def pack(vals):\n",
    "        assert len(vals.shape)==2, 'Pass a 2d tensor'\n",
    "        for v in vals.flatten(): assert 0<=v.item()<=7 and v.item()//1==v.item(), f'Value {v} can\\'t be represented by 3 bits or is not an integer'    \n",
    "        rows, cols = vals.shape\n",
    "        n_packs = ceil(rows/10)\n",
    "        padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "        padded_vals[:rows, :cols] = vals\n",
    "        packed = torch.zeros(n_packs, cols, dtype=int32)\n",
    "        for k in range(10): packed = (packed << 3) | padded_vals[k*n_packs:(k+1)*n_packs,:] # shift right 3 bits, then set last 3 bits to padded_vals[...,...]\n",
    "        return packed\n",
    "\n",
    "    @staticmethod\n",
    "    def unpack(packed, rows):\n",
    "        def bin_to_dec(b3,b2,b1): return 4*b3 + 2*b2 + b1\n",
    "        assert len(packed.shape)==2 and packed.dtype==int32, 'Pass a 2d tensor of int32s'\n",
    "        n_packs, cols = packed.shape\n",
    "        padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "        for k_up, k_down in zip(range(10), reversed(range(10))): # top-most 3bits vals (k_up=0) are most right-shifted (k_down=9)\n",
    "            padded_vals[k_down*n_packs:(k_down+1)*n_packs,:] = ((packed >> (3*k_up)) & 0b111) # righ-shift 3*k_up times, so last 3 bits are those we want; then only select those via 0b111            \n",
    "        return padded_vals[:rows,:]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _quant(data, group_size, bits=3, packed=True, optimize=True):\n",
    "        assert data.numel()%group_size==0, f'group_size {group_size} can\\'t evenly split the data (numel = {data.numel()})'\n",
    "        data = data.float().reshape(group_size,-1)\n",
    "        \n",
    "        min_, max_ = data.min(axis=0, keepdim=True).values, data.max(axis=0, keepdim=True).values\n",
    "    \n",
    "        scale = (2**bits-1) / (max_-min_) # note: hqq clamp to 2e4 to avoid half-precision problems, let's ignore that for now\n",
    "        zero = -min_ * scale\n",
    "    \n",
    "        if optimize: data, scale, zero = Quantizer.optimize_weights(data, scale, zero, min_max=[0, 2**bits-1])\n",
    "        else: data = (data * scale + zero).round()\n",
    "\n",
    "        if packed: data = QuantedDoraModule.pack(data)\n",
    "        return data, zero, 1/scale # invert scale, so in dequanting we multiply instead of divide \n",
    "\n",
    "    @staticmethod\n",
    "    def _dequant(data, zero, scale, shape, group_size, packed=True):\n",
    "        if packed: data = QuantedDoraModule.unpack(data, rows=group_size)\n",
    "        data = (data-zero)*scale\n",
    "        return data.reshape(shape)\n",
    "\n",
    "    def quant(self, data):\n",
    "        qdata,  zero       , scale        = self._quant(data,  self.group_size,       self.bits, self.packed, self.optimized)\n",
    "        qzero,  zeros_zero , zeros_scale  = self._quant(zero,  self.group_size_zero,  self.bits, self.packed, False)\n",
    "        qscale, scales_zero, scales_scale = self._quant(scale, self.group_size_scale, self.bits, self.packed, False)\n",
    "        store_attr('qdata, qzero, qscale, zeros_zero, zeros_scale, scales_zero, scales_scale', self)\n",
    "        self.data_shape,self.zero_shape,self.scale_shape = data.shape, zero.shape, scale.shape\n",
    "\n",
    "    def dequant(self):\n",
    "        zero  = self._dequant(self.qzero,  self.zeros_zero,  self.zeros_scale,  self.zero_shape,  self.group_size_zero,  self.packed)\n",
    "        scale = self._dequant(self.qscale, self.scales_zero, self.scales_scale, self.scale_shape, self.group_size_scale, self.packed)\n",
    "        return  self._dequant(self.qdata,  zero,             scale,             self.data_shape,  self.group_size,       self.packed).to(self.compute_dtype)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dequant()@x + self.b(self.a(x))\n",
    "        col_norms =  (self.dequant() + self.b.weight @ self.a.weight).norm(p=2, dim=1).detach()\n",
    "        x /= col_norms\n",
    "        x *= self.m * self.alpha\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6153fbb-e480-45a5-bd43-62064b575b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.02,  0.05,  0.08,  ...,  0.01, -0.02, -0.08],\n",
       "        [-0.06,  0.06, -0.07,  ...,  0.02, -0.09, -0.07],\n",
       "        [ 0.04,  0.06, -0.04,  ..., -0.08,  0.04,  0.05],\n",
       "        ...,\n",
       "        [ 0.05,  0.01, -0.07,  ..., -0.00, -0.07,  0.07],\n",
       "        [-0.06,  0.03,  0.01,  ..., -0.06,  0.05, -0.06],\n",
       "        [-0.01, -0.05,  0.07,  ...,  0.06,  0.02,  0.04]], dtype=torch.float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_linear = nn.Linear(n,m, bias=False, dtype=fp16) # ignore bias for now\n",
    "base_linear.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d83c4d-0b9d-49a8-a13e-b47b13db9a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.46, -1.52, -0.69,  0.25,  0.24, -1.34,  0.62,  0.97,  0.05, -0.63, -0.70, -0.70, -0.32,  1.02, -0.63, -0.01, -0.14, -1.09,  0.02, -0.64,  0.31, -0.71, -1.03,  0.68, -0.18, -1.04,  0.63,\n",
       "        -1.56,  0.56,  0.48, -0.47, -0.48,  0.91, -0.26, -0.91, -0.36,  1.37,  0.61,  1.27, -1.89,  0.71,  1.18, -0.14,  1.89, -0.59, -2.38, -0.81, -0.79,  0.33, -0.18, -0.53, -1.63,  0.87, -0.52,\n",
       "        -0.28,  0.15, -0.45,  0.22, -0.10,  0.34,  0.76,  0.36,  0.14, -0.82,  0.02,  0.11,  0.12, -0.99,  0.96, -1.13,  0.05, -0.32,  0.50, -1.92, -0.94,  0.40,  0.72,  0.17, -0.12, -0.57, -1.00,\n",
       "         0.32, -0.55, -1.63,  0.55, -1.47, -0.77, -1.09, -2.43, -1.39, -1.45, -0.69, -0.41,  0.40,  0.52, -0.92,  0.58, -0.52, -1.04, -0.51,  0.90, -1.67, -0.12,  1.66,  1.82,  0.20,  0.26, -2.13,\n",
       "         2.00,  0.82, -1.04,  0.66, -0.75, -0.06,  0.74,  1.77,  0.27, -0.43, -0.70,  0.47,  0.45,  0.21,  2.23, -0.52, -1.25,  1.01,  1.11, -0.61], dtype=torch.float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tst = torch.randn(n, dtype=fp16); x_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d69580-0c9a-410e-9d7a-257a0dcec829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    -0.56,      0.68,      0.33,      0.17,      0.04,      0.01,     -0.11,      0.46,     -0.24,      0.21,      0.29,      0.24,     -0.64,      0.39,     -0.05,      0.75,     -0.37,\n",
       "            -0.44,      0.41,     -0.43,      0.13,     -0.95,     -0.69,     -0.65,      0.47,      0.03,     -0.16,     -0.22,      0.26,     -0.35,      0.31,     -0.51,      1.36,      0.15,\n",
       "             0.67,     -0.08,      0.97,      0.41,     -0.54,      0.89,     -1.37,      0.53,      0.98,      0.05,     -0.36,      0.41,      0.26,     -0.95,     -0.15,      0.30,     -0.35,\n",
       "            -0.50,     -1.25,     -0.20,      0.30,      0.22,      0.04,     -0.67,     -0.16,      0.06,      0.88,      0.04,      1.04,      0.81,      0.42,      0.04,      1.38,      0.38,\n",
       "            -0.41,     -0.46,      0.46,     -0.89,      0.39,     -0.51,     -0.38,      0.43,      0.27,      0.07,      0.30,      0.69,      0.16,      0.63,     -0.74,     -0.08,      0.52,\n",
       "            -0.58,      0.25,      0.12,     -0.00,     -0.65,     -0.38,     -0.27,     -0.36,      0.50,     -0.00,     -0.17,     -0.09,     -0.20,     -0.16,      1.19,      0.40,      0.24,\n",
       "             1.12,      0.16,      0.33,      0.04,      0.52,      0.05,     -0.39,     -0.73,     -0.17,     -0.88,     -0.56,      0.65,     -0.29,     -0.28,      0.34,      1.10,     -0.04,\n",
       "             0.35,     -0.92,     -0.74,     -0.11,     -0.40,     -0.66,      0.31,      0.06,     -0.28], dtype=torch.float16, grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tst = base_linear(x_tst); y_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8667db-744a-4f71-a247-18ad7d406c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ee2a8c-6952-4941-842a-4374b6bc6f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantedDoraModule(\n",
       "  (a): Linear(in_features=128, out_features=32, bias=False)\n",
       "  (b): Linear(in_features=32, out_features=128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdora_linear = QuantedDoraModule(base_linear, bits=3, group_size=64, rank=r, alpha=1, compute_dtype=fp16); qdora_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca43823f-f789-44e4-b01f-d905c0b9e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_qdora = qdora_linear(x_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d8899f-8246-4cdb-adf8-8d32ad10d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quanted result (with packing):\n",
      "tensor([[-0.66,  0.69,  0.20,  0.15,  0.06, -0.10, -0.11,  0.41, -0.22,  0.15,  0.31,  0.16, -0.57,  0.42,  0.10,  0.70],\n",
      "        [-0.38, -0.44,  0.55, -0.36,  0.18, -1.02, -0.74, -0.61,  0.52, -0.02, -0.14, -0.33,  0.27, -0.40,  0.42, -0.53],\n",
      "        [ 1.30,  0.23,  0.66, -0.04,  0.98,  0.58, -0.47,  0.92, -1.37,  0.55,  1.00,  0.07, -0.46,  0.40,  0.17, -0.93],\n",
      "        [-0.18,  0.24, -0.46, -0.53, -1.26, -0.21,  0.33,  0.29,  0.06, -0.57, -0.19,  0.13,  0.80,  0.03,  1.04,  0.79],\n",
      "        [ 0.39,  0.15,  1.37,  0.37, -0.43, -0.48,  0.51, -0.97,  0.43, -0.60, -0.40,  0.42,  0.27,  0.18,  0.35,  0.77],\n",
      "        [ 0.15,  0.55, -0.75, -0.07,  0.59, -0.57,  0.13,  0.20,  0.03, -0.63, -0.37, -0.34, -0.36,  0.46,  0.01, -0.13],\n",
      "        [-0.07, -0.14, -0.19,  1.13,  0.44,  0.23,  1.15,  0.16,  0.31,  0.02,  0.56,  0.10, -0.32, -0.65, -0.22, -0.77],\n",
      "        [-0.52,  0.78, -0.29, -0.25,  0.27,  1.00, -0.06,  0.34, -0.94, -0.79, -0.09, -0.40, -0.61,  0.27, -0.14, -0.17]], dtype=torch.float16)\n",
      "\n",
      "exact result :\n",
      "tensor([[    -0.56,      0.68,      0.33,      0.17,      0.04,      0.01,     -0.11,      0.46,     -0.24,      0.21,      0.29,      0.24,     -0.64,      0.39,     -0.05,      0.75],\n",
      "        [    -0.37,     -0.44,      0.41,     -0.43,      0.13,     -0.95,     -0.69,     -0.65,      0.47,      0.03,     -0.16,     -0.22,      0.26,     -0.35,      0.31,     -0.51],\n",
      "        [     1.36,      0.15,      0.67,     -0.08,      0.97,      0.41,     -0.54,      0.89,     -1.37,      0.53,      0.98,      0.05,     -0.36,      0.41,      0.26,     -0.95],\n",
      "        [    -0.15,      0.30,     -0.35,     -0.50,     -1.25,     -0.20,      0.30,      0.22,      0.04,     -0.67,     -0.16,      0.06,      0.88,      0.04,      1.04,      0.81],\n",
      "        [     0.42,      0.04,      1.38,      0.38,     -0.41,     -0.46,      0.46,     -0.89,      0.39,     -0.51,     -0.38,      0.43,      0.27,      0.07,      0.30,      0.69],\n",
      "        [     0.16,      0.63,     -0.74,     -0.08,      0.52,     -0.58,      0.25,      0.12,     -0.00,     -0.65,     -0.38,     -0.27,     -0.36,      0.50,     -0.00,     -0.17],\n",
      "        [    -0.09,     -0.20,     -0.16,      1.19,      0.40,      0.24,      1.12,      0.16,      0.33,      0.04,      0.52,      0.05,     -0.39,     -0.73,     -0.17,     -0.88],\n",
      "        [    -0.56,      0.65,     -0.29,     -0.28,      0.34,      1.10,     -0.04,      0.35,     -0.92,     -0.74,     -0.11,     -0.40,     -0.66,      0.31,      0.06,     -0.28]],\n",
      "       dtype=torch.float16)\n",
      "\n",
      "Max error is 0.20 ✓\n"
     ]
    }
   ],
   "source": [
    "print('quanted result (with packing):')\n",
    "print(y_qdora.reshape(8,16).data) # reshape to easier read & compare\n",
    "print()\n",
    "print('exact result :')\n",
    "print(y_tst.reshape(8,16).data)\n",
    "print()\n",
    "assert_somehow_close(y_qdora, y_tst, max_err=0.3)\n",
    "print(f'Max error is {max_abs_diff(y_qdora, y_tst):.2f} ✓')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98828f0-d6f9-4ea6-90fb-0c0ed9156a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "050c8c0a-5341-4c30-8086-966bdcd9a527",
   "metadata": {},
   "source": [
    "Let's call backwards on the model, to make sure it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd43544-9e0e-4e1f-9a99-8859d3e4ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert {name for name,p in qdora_linear.named_parameters()} == {'m','a.weight','b.weight'} # assert only the dora part is trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89483648-91a2-422b-9cef-bb4138b04055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.90, dtype=torch.float16, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = y_qdora.sum() # arbitrary operation to make y_qdora a scalar\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97aebc50-a137-4ad0-9f54-4678123c8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "631658a8-80e3-4cef-a480-348d2a4cddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss shapes:\n",
      "Shape of grad of m        is [128]  ; shape of  m        is [128]\n",
      "Shape of grad of a.weight is [32, 128]; shape of  a.weight is [32, 128]\n",
      "Shape of grad of b.weight is [128, 32]; shape of  b.weight is [128, 32]\n"
     ]
    }
   ],
   "source": [
    "print('Loss shapes:')\n",
    "for name,p in qdora_linear.named_parameters(): print(f'Shape of grad of {name:<8} is {str(list(p.grad.shape)):<7}; shape of  {name:<8} is {list(p.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "284b3cee-6cdb-4c82-b26b-38899ee8ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- m.grad:\n",
      "tensor([-1.13,  1.16,  0.37,  0.26,  0.11, -0.16, -0.21,  0.69, -0.37,  0.24,  0.56,  0.28, -0.95,  0.71,  0.17,  1.25, -0.67, -0.77,  0.95, -0.64,  0.30, -1.73, -1.41, -1.03,  0.91, -0.03, -0.24,\n",
      "        -0.62,  0.46, -0.72,  0.74, -0.92,  2.36,  0.43,  1.12, -0.07,  1.73,  0.93, -0.82,  1.74, -2.20,  0.93,  1.73,  0.11, -0.81,  0.69,  0.29, -1.51, -0.30,  0.43, -0.78, -0.91, -2.11, -0.39,\n",
      "         0.57,  0.47,  0.12, -1.00, -0.32,  0.24,  1.30,  0.05,  1.80,  1.41,  0.68,  0.25,  2.29,  0.65, -0.72, -0.82,  0.89, -1.60,  0.77, -1.08, -0.71,  0.69,  0.50,  0.30,  0.61,  1.28,  0.27,\n",
      "         0.94, -1.28, -0.11,  0.96, -1.07,  0.22,  0.34,  0.04, -0.97, -0.65, -0.56, -0.60,  0.77,  0.02, -0.23, -0.12, -0.24, -0.34,  1.90,  0.76,  0.44,  2.03,  0.28,  0.53,  0.03,  0.94,  0.16,\n",
      "        -0.54, -1.11, -0.39, -1.34, -0.92,  1.29, -0.49, -0.43,  0.45,  1.85, -0.10,  0.60, -1.64, -1.30, -0.15, -0.72, -1.12,  0.49, -0.25, -0.29], dtype=torch.float16)\n",
      "--- a.weight.grad:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)\n",
      "--- b.weight.grad:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "for name,p in qdora_linear.named_parameters(): print(f'--- {name}.grad:\\n{p.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e312c2c-c120-43f6-9682-5a82a73c545f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf5369ba-5ba8-4b02-ad8b-0103158feb9d",
   "metadata": {},
   "source": [
    "**Let's verify our implementation against hqq:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ee9f461-9f5d-4d46-9cfa-bf344b8e0a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.02,  0.05,  0.08,  ...,  0.01, -0.02, -0.08],\n",
       "        [-0.06,  0.06, -0.07,  ...,  0.02, -0.09, -0.07],\n",
       "        [ 0.04,  0.06, -0.04,  ..., -0.08,  0.04,  0.05],\n",
       "        ...,\n",
       "        [ 0.05,  0.01, -0.07,  ..., -0.00, -0.07,  0.07],\n",
       "        [-0.06,  0.03,  0.01,  ..., -0.06,  0.05, -0.06],\n",
       "        [-0.01, -0.05,  0.07,  ...,  0.06,  0.02,  0.04]], dtype=torch.float16)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = base_linear.weight.data; W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b55f1ac5-e3b4-4001-9b48-e40a269585d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hqq_linear = HQQLinear(\n",
    "    base_linear,\n",
    "    quant_config=BaseQuantizeConfig(nbits=3, group_size=64), #quantization configuration\n",
    "    compute_dtype=torch.float16,\n",
    "    device='cuda',\n",
    "    initialize=True, #Use False to quantize later\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4860fead-743f-4c5b-a88b-298d1d31cf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.02,  0.06,  0.08,  ...,  0.01, -0.01, -0.09],\n",
       "        [-0.06,  0.06, -0.06,  ...,  0.02, -0.09, -0.06],\n",
       "        [ 0.03,  0.06, -0.03,  ..., -0.09,  0.04,  0.06],\n",
       "        ...,\n",
       "        [ 0.06,  0.01, -0.06,  ..., -0.01, -0.06,  0.07],\n",
       "        [-0.06,  0.03,  0.01,  ..., -0.06,  0.04, -0.07],\n",
       "        [-0.01, -0.06,  0.06,  ...,  0.07,  0.01,  0.04]], dtype=torch.float16)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_hqq = hqq_linear.dequantize().cpu(); W_hqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "366410ef-94fb-4ec3-94a8-4049f6d11a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.02,  0.05,  0.08,  ...,  0.01, -0.01, -0.09],\n",
       "        [-0.06,  0.06, -0.06,  ...,  0.01, -0.09, -0.05],\n",
       "        [ 0.03,  0.05, -0.04,  ..., -0.08,  0.04,  0.06],\n",
       "        ...,\n",
       "        [ 0.06,  0.01, -0.06,  ..., -0.01, -0.06,  0.07],\n",
       "        [-0.06,  0.03,  0.01,  ..., -0.06,  0.04, -0.06],\n",
       "        [-0.01, -0.06,  0.06,  ...,  0.06,  0.01,  0.04]], dtype=torch.float16)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_ours = qdora_linear.dequant(); W_ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86420c29-14a2-4b4b-a809-f7ef0ab62e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max abs diff between: W     and W_hqq : 0.01\n",
      "Max abs diff between: W     and W_ours: 0.01\n",
      "Max abs diff between: W_hqq and W_ours: 0.03\n"
     ]
    }
   ],
   "source": [
    "print(f'Max abs diff between: W     and W_hqq : {max_abs_diff(W, W_hqq):.2f}')\n",
    "print(f'Max abs diff between: W     and W_ours: {max_abs_diff(W, W_ours):.2f}')\n",
    "print(f'Max abs diff between: W_hqq and W_ours: {max_abs_diff(W_hqq, W_ours):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a619425e-0c23-422a-8e3a-967e3f121b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_somehow_close(W_est, W_est_hqq, max_err=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e954dd9-ee73-4b71-864a-bc5358869a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4b619-6a13-4914-a51a-c4bb4ddd79e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
