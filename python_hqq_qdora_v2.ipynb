{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf648fa-0c41-4f43-ba84-cee41f438d3d",
   "metadata": {},
   "source": [
    "Hqq uses a different packing method than `python_hqq_qdora.ipynb`. In this nb, I use that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3785c-651a-4b70-a225-07b833f9c9ef",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035941a3-fd58-48e6-a43a-fea00f672b41",
   "metadata": {},
   "source": [
    "In this nb I implement hqq-qdora step by step, so that I have a single PyTorch model with all the functionality in one place. This will serves as a reference for implementing fast kernels for the forward, and for deriving the backward.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "0. Take the reference implementation from `python_hqq_qdora`\n",
    "1. Implement packing as hqq does it\n",
    "2. Implement (de)quanting as hqq does it, without the optimization\n",
    "3. Quant zero and scales\n",
    "4. Use hqq's optimization in quanting\n",
    "5. Put it all toghether!\n",
    "\n",
    "Also note: I'm not using inheritance to produce cleaner code in this nb. The goal is to have a single class with all the functionality in one place. This will make it easier later to write kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "225deadd-a13a-4146-bfb2-a6d91bf2b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor, cat, int32\n",
    "from torch import float16 as fp16\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "from fastcore.foundation import L, Self\n",
    "from fastcore.basics import store_attr\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "torch.set_printoptions(linewidth=200, precision=2, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f82477-707c-4358-8a2d-acf9202d4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_close(a,b): assert torch.isclose(a,b,atol=1e-2).all()\n",
    "def assert_somehow_close(a,b): assert torch.isclose(a,b,atol=0.2).all() # allow error of 0.2 due to quanting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b020e820-d9af-474d-a124-13b6a86c8a71",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59febee5-9a96-463a-8083-a7a17d26021b",
   "metadata": {},
   "source": [
    "## 0. Current reference implementation from `python_hqq_qdora`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16a09a1-51ab-4690-9259-5d8fcd11b247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.47, -0.38, -0.32,  0.18],\n",
       "        [-0.38, -0.13, -0.46,  0.47],\n",
       "        [ 0.31, -0.06, -0.14, -0.49],\n",
       "        [ 0.16,  0.34, -0.27, -0.31],\n",
       "        [ 0.29, -0.12, -0.25, -0.42]], dtype=torch.float16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_linear = nn.Linear(4,5, bias=False, dtype=fp16) # ignore bias for now\n",
    "base_linear.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e4217c-eb1d-45b7-b925-2316f1a6b6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.83,  1.67, -0.13,  0.17], dtype=torch.float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_x = torch.randn(4, dtype=fp16); tst_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b6101f-7e26-47b4-9d10-858a0d4d9f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.17, -0.38,  0.09,  0.69,  0.00], dtype=torch.float16, grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_result = base_linear(tst_x); tst_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0ebbd-066a-4584-955d-a84983c1e122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8aed621c-4b19-4c64-bd01-4afac8433620",
   "metadata": {},
   "source": [
    "This is `QuantedDoraModule` from `python_hqq_qdora.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c9d7e70-f910-4f1e-a241-5577c19a9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantedDoraModule(nn.Module):\n",
    "    def __init__(self, linear, bits, group_size, rank, alpha):\n",
    "        super().__init__()\n",
    "        # for quanting\n",
    "        assert base_linear.weight.numel() % group_size ==0, f'group_size {group_size} can\\'t cleanly split weight of base layer ({base_linear.weight.numel()} items)'\n",
    "        self.bits,self.group_size, = bits,group_size\n",
    "        self.quant(linear)\n",
    "        # for dora\n",
    "        self.a = nn.Linear(linear.in_features, rank, bias=False, dtype=fp16)\n",
    "        self.b = nn.Linear(rank, linear.out_features, bias=False, dtype=fp16)\n",
    "        self.alpha = alpha\n",
    "        self.m = nn.Parameter(linear.weight.norm(p=2, dim=1))\n",
    "        # init a & b to 0 -- a should be inited differently, but for sake of simplicity, set it to 0 as well\n",
    "        self.a.weight.data.zero_()\n",
    "        self.b.weight.data.zero_()\n",
    "\n",
    "    def quant(self, linear):\n",
    "        data = linear.weight.data\n",
    "        self.shape = data.shape\n",
    "\n",
    "        # repeat last element, to have a multiple of group_size elements\n",
    "        # note: element to pad with mustn't change any attribute that's use for quanting (eg min & max in a group)\n",
    "        n_pad = data.numel()%self.group_size\n",
    "        data = F.pad(data, (0,n_pad), 'constant', data.flatten()[-1])\n",
    "        assert data.numel()%self.group_size==0\n",
    "\n",
    "        data = data.reshape(-1,self.group_size)\n",
    "        \n",
    "        min_, max_ = data.min(axis=-1, keepdim=True).values, data.max(axis=-1, keepdim=True).values\n",
    "        \n",
    "        self.zero = min_\n",
    "        self.scale = (max_-min_) / (2**self.bits-1) \n",
    "        \n",
    "        # note: can't use shorthand ops like -= as they modify tensor in-place\n",
    "        data = data - self.zero # start at 0\n",
    "        data = data / self.scale # scale to [0, 2**bits-1]\n",
    "        data = data.round().to(int)\n",
    "\n",
    "        # packed quantized data\n",
    "        self.pqdata = self.pack(data.flatten())\n",
    "\n",
    "    # pack 10 3bit values into a 32bit val\n",
    "    @staticmethod\n",
    "    def pack(vals):\n",
    "        for v in vals: assert 0<=v<=7 and v//1==v, f'Value {v} can\\'t be represented by 3 bits or is not an integer'\n",
    "        \n",
    "        n_packs = ceil(len(vals)/10)\n",
    "    \n",
    "        # pad with 0, to have a multiple of pack_size elements\n",
    "        n_pad = n_packs*10 - len(vals)\n",
    "        vals = F.pad(vals, (0,n_pad), 'constant', 0)\n",
    "        assert len(vals)==n_packs*10\n",
    "    \n",
    "        packed = torch.zeros(n_packs, dtype=int32)\n",
    "        for i in range(n_packs):\n",
    "            # pack the 10 vals from 10*i to 10*(i+1) into packed[i]\n",
    "            for x in vals[10*i:10*(i+1)]: packed[i] = (packed[i] << 3) | x # shift right 3 bits, then set last 3 bits to x\n",
    "        return packed\n",
    "\n",
    "    def dequant(self):\n",
    "        data = self.unpack(self.pqdata)[:self.shape.numel()] # unpack & remove padding that was added during packing\n",
    "        data = data.reshape(-1,self.group_size)\n",
    "        data = data*self.scale + self.zero\n",
    "        return data.reshape(self.shape)\n",
    "    \n",
    "    # unpack a 32bit value into 10 3bit vals\n",
    "    @staticmethod\n",
    "    def unpack(packed):\n",
    "        def bin_to_dec(b3,b2,b1): return 4*b3 + 2*b2 + b1\n",
    "        for v in packed: isinstance(v, int), f'Value {v} is not an integer'\n",
    "        unpacked = []\n",
    "        for pack in packed:\n",
    "            for i in reversed(range(10)):\n",
    "                unpacked.append((pack >> (3*i)) & 0b111) # righ-shift 3*i times, so last 3 bits are those we want; then only select those via 0b111            \n",
    "        return tensor(unpacked)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dequant()@x + self.b(self.a(x))\n",
    "        col_norms =  (self.dequant() + self.b.weight @ self.a.weight).norm(p=2, dim=1).detach()\n",
    "        x /= col_norms\n",
    "        x *= self.m * self.alpha\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ee2a8c-6952-4941-842a-4374b6bc6f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantedDoraModule(\n",
       "  (a): Linear(in_features=4, out_features=2, bias=False)\n",
       "  (b): Linear(in_features=2, out_features=5, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdora = QuantedDoraModule(base_linear, bits=3, group_size=5, rank=2, alpha=1); qdora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0d8899f-8246-4cdb-adf8-8d32ad10d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quanted result (with packing): tensor([-0.14, -0.48,  0.11,  0.65,  0.01], dtype=torch.float16, grad_fn=<MulBackward0>)\n",
      "exact   result               : tensor([-0.17, -0.38,  0.09,  0.69,  0.00], dtype=torch.float16, grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "print(f'quanted result (with packing): {qdora(tst_x)}')\n",
    "print(f'exact   result               : {tst_result}')\n",
    "assert_somehow_close(qdora(tst_x), tst_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98828f0-d6f9-4ea6-90fb-0c0ed9156a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae8d1bb-74de-46eb-a72a-3ec24dabf099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32aefae7-ac82-46d7-a4e5-88938f41c460",
   "metadata": {},
   "source": [
    "## 1. Packing as in hqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b31b3-c427-40d8-80d5-a36c4e5bc5e3",
   "metadata": {},
   "source": [
    "hqq packs column-wise and interwovenly. For example, for `group_siz=3` (so 3 columns) and 2 packs per columns (ie 11-20 elements per column), the data matrix\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "d & e & f \\\\\n",
    "g & h & i \\\\\n",
    "j & k & l \\\\\n",
    "\\vdots & \\vdots & \\vdots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "is packed into:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "Pack(a,g,...) & Pack(b,h,...) & Pack(c,i,...) \\\\\n",
    "Pack(d,j,...) & Pack(e,k,...) & Pack(f,l,...) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "004c27e0-6cbd-4dc9-9e41-ef3d724919c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack(vals):\n",
    "    assert len(vals.shape)==2, 'Pass a 2d tensor'\n",
    "    for v in vals.flatten(): assert 0<=v.item()<=7 and v.item()//1==v.item(), f'Value {v} can\\'t be represented by 3 bits or is not an integer'    \n",
    "    rows, cols = vals.shape\n",
    "    n_packs = ceil(rows/10) # packs per col\n",
    "    padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "    padded_vals[:rows, :cols] = vals\n",
    "    packed = torch.zeros(n_packs, cols, dtype=int32)\n",
    "\n",
    "    for j in range(cols):\n",
    "        for i in range(n_packs):\n",
    "            for k in range(10):\n",
    "                packed[i,j] = (packed[i,j] << 3) | padded_vals[n_packs*k+i,j] # shift right 3 bits, then set last 3 bits to x\n",
    "    return packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12211d54-3dc5-4cdb-be96-ea5ad2b93fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pack = torch.ones(11,1, dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a7eb9e4-2ac3-4fe3-84d3-9e046dfa25f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[153391104],\n",
       "        [153387008]], dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack(to_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fac2756-3228-4ed9-b68a-ce56f2d4969e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[153391104],\n",
       "        [153387008]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hqq.core.bitpack import BitPack\n",
    "BitPack.pack_3bit_32(to_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e004b5-f9b7-4c39-96e4-d65eddd62e85",
   "metadata": {},
   "source": [
    "Matches!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d23b4-f6b1-49bb-aa0c-d250716599f3",
   "metadata": {},
   "source": [
    "We can vectorize the loops. Let's do that next. Hqq also does that.\n",
    "\n",
    "There are 3 loops: `j` over cols, `i` over packs, and `k` over 3bit-values. We'll vectorized the `j` and `i` loops. This means, in each iteration over `k`, we'll bit-shift the current packed tensor (shape `(n_packs, cols)`) 3 bits to the right, then set the left-most bits (which are all 0 now) to `vals[k*n_packs:(k+1)*n_packs,:]`, which is also shape `(n_packs, cols)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6c01a83-fc6b-4afc-b9ce-e0919c43a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack(vals):\n",
    "    assert len(vals.shape)==2, 'Pass a 2d tensor'\n",
    "    for v in vals.flatten(): assert 0<=v.item()<=7 and v.item()//1==v.item(), f'Value {v} can\\'t be represented by 3 bits or is not an integer'    \n",
    "    rows, cols = vals.shape\n",
    "    n_packs = ceil(rows/10)\n",
    "    padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "    padded_vals[:rows, :cols] = vals\n",
    "    packed = torch.zeros(n_packs, cols, dtype=int32)\n",
    "    for k in range(10): packed = (packed << 3) | padded_vals[k*n_packs:(k+1)*n_packs,:] # shift right 3 bits, then set last 3 bits to padded_vals[...,...]\n",
    "    return packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad8561e7-2cac-4387-9b02-cdebba6ecd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[153391104],\n",
       "        [153387008]], dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack(to_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef32ac-f6c3-4cb2-87c8-6cb6e0fcd9ab",
   "metadata": {},
   "source": [
    "Note: hqq also 'vectorizes' the `k` loop by explicitly doing the 10 operations. We'll ignore that for simplicity's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68b2995-d3f4-4dc5-8492-60a1fb69a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_3bit_tensor():\n",
    "    rows, cols = torch.randint(low=1, high=100, size=(2,))\n",
    "    return torch.randint(0, 7, (rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "384b84b7-2f46-49c1-b70e-86bccd741624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested for 50 runs - Looks good ✓\n"
     ]
    }
   ],
   "source": [
    "def test_pack_against_hqq(runs=50):\n",
    "    for _ in range(runs):\n",
    "        t = rand_3bit_tensor()    \n",
    "        assert (pack(t)==BitPack.pack_3bit_32(t)).all()\n",
    "    print(f'Tested for {runs} runs - Looks good ✓')\n",
    "\n",
    "test_pack_against_hqq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd2115-cff2-45d7-93a1-b1d1021957c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84c990c6-e55b-4e40-83d0-c8e189170cd9",
   "metadata": {},
   "source": [
    "Let's now adapt the unpacking function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8432401-b150-49dd-afd7-eac494040355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack a 32bit value into 10 3bit vals\n",
    "def unpack(packed, rows):\n",
    "    def bin_to_dec(b3,b2,b1): return 4*b3 + 2*b2 + b1\n",
    "    assert len(packed.shape)==2 and packed.dtype==int32, 'Pass a 2d tensor of int32s'\n",
    "    n_packs, cols = packed.shape\n",
    "    padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "    for k_up, k_down in zip(range(10), reversed(range(10))): # top-most 3bits vals (k_up=0) are most right-shifted (k_down=9)\n",
    "        padded_vals[k_down*n_packs:(k_down+1)*n_packs,:] = ((packed >> (3*k_up)) & 0b111) # righ-shift 3*k_up times, so last 3 bits are those we want; then only select those via 0b111            \n",
    "    return padded_vals[:rows,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fc5595d-f240-4adf-9594-7b3ac2a33bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(unpack(pack(to_pack), rows=11)==to_pack).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "810ca1ed-ae74-4206-b0d7-c5ebbd628522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested for 50 runs - Looks good ✓\n"
     ]
    }
   ],
   "source": [
    "def test_pack_unpack(runs=50):\n",
    "    for _ in range(runs):\n",
    "        t = rand_3bit_tensor()\n",
    "        assert (unpack(pack(t), rows=t.shape[0])==t).all()\n",
    "    print(f'Tested for {runs} runs - Looks good ✓')\n",
    "\n",
    "test_pack_unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3fdc5-73b0-4e17-a8b6-b558b031dbda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b4272b2-8df1-4747-8233-646a6d5701bf",
   "metadata": {},
   "source": [
    "## 2. (De)quanting as in hqq, without optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1707beaa-27c5-4f44-a4dd-3dba824bb9fb",
   "metadata": {},
   "source": [
    "Differences:\n",
    "- **data type:** hqq converts data to float.\n",
    "    - **ToDo Umer:** Why?\n",
    "- **data shape:** by default hqq reshapes to `(group_size, -1)`, while we did `(-1, group_size)`\n",
    "    - our choice corresponds to their `axis=1` setting, but their default is `axis=0`\n",
    "    - changed shapes mean we need to take the min/max now over axis 0, instead of axis 1\n",
    "- **ensuring group_size evenly splits data:** they raise Error if it doesn't, while we pad.\n",
    "- **order of `zero` & `scale`:**\n",
    "    - hqq first multiplies by `scale`, then subtracts `zero`; we did it the other way round, but we also defined `zero` & `scales` a bit differently.\n",
    "    - Both approaches are mathematically identical, but to use their `scale` / `zero`, we need to use their approach.\n",
    "    - Also, their `scale` is our inverse `scale`, but we divide by it, they multiply; and our zeros have different signs, but they add and we devide. Let's use their definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b6ee0c0-f8b6-4a8d-8693-1a732e5fbe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant(data, group_size, bits=3, do_pack=True):\n",
    "    assert data.numel()%group_size==0, f'group_size {group_size} can\\'t evenly split the data (numel = {data.numel()})'\n",
    "    data = data.float().reshape(group_size,-1)\n",
    "    \n",
    "    min_, max_ = data.min(axis=0, keepdim=True).values, data.max(axis=0, keepdim=True).values\n",
    "\n",
    "    scale = (2**bits-1) / (max_-min_) # note: hqq clamp to 2e4 to avoid half-precision problems, let's ignore that for now\n",
    "    zero = -min_ * scale\n",
    "    \n",
    "    data = (data * scale + zero).round()\n",
    "\n",
    "    # packed quantized data\n",
    "    if do_pack: data = pack(data)\n",
    "\n",
    "    return data, zero, 1/scale # we invert scale, so we can do multiplication instead of division in dequanting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d7adc8b-00dc-496d-9410-338c7ba7a00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = torch.repeat_interleave(tensor([1.,2,3]), 4).reshape(3,-1)\n",
    "some_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a44ac497-e914-42ef-9721-9f87a85a51f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 4., 4.],\n",
       "         [4., 4., 7.],\n",
       "         [7., 7., 7.]]),\n",
       " tensor([[-3.50, -3.50, -3.50]]),\n",
       " tensor([[0.29, 0.29, 0.29]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant(some_data, group_size=4, do_pack=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c19977a8-4f12-4f69-a8d1-a95b1939d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.core.quantize import Quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19407d0e-8238-4c20-9a0a-ab1b5e0bd96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c60b93f-f8a1-4974-8983-ec02f232839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------+---------------------------------+\n",
      "| Name   | HQQ Item                        | Our Item                        |\n",
      "+========+=================================+=================================+\n",
      "| qdata  | tensor([[0., 0., 0.],           | tensor([[0., 0., 0.],           |\n",
      "|        |         [0., 4., 4.],           |         [0., 4., 4.],           |\n",
      "|        |         [4., 4., 7.],           |         [4., 4., 7.],           |\n",
      "|        |         [7., 7., 7.]])          |         [7., 7., 7.]])          |\n",
      "+--------+---------------------------------+---------------------------------+\n",
      "| zero   | tensor([[-3.50, -3.50, -3.50]]) | tensor([[-3.50, -3.50, -3.50]]) |\n",
      "+--------+---------------------------------+---------------------------------+\n",
      "| scale  | tensor([[0.29, 0.29, 0.29]])    | tensor([[0.29, 0.29, 0.29]])    |\n",
      "+--------+---------------------------------+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "hqq_results = Quantizer.quantize(some_data, group_size=4, nbits=3, optimize=False, bitpack=False)\n",
    "items_hqq = [hqq_results[0], hqq_results[1]['zero'], hqq_results[1]['scale']]\n",
    "items_ours = quant(some_data, group_size=4, do_pack=False)\n",
    "names = ['qdata', 'zero', 'scale']\n",
    "print(tabulate(zip(names, items_hqq, items_ours), headers=['Name', 'HQQ Item', 'Our Item'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "597d1885-9011-4585-b6e1-fa90b0cba35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ohqq,oours in zip(items_hqq, items_ours): assert (ohqq==oours).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a59623c1-8748-48d2-97ce-da7d441c0bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested for 50 runs - Looks good ✓\n"
     ]
    }
   ],
   "source": [
    "def test_quant_against_hqq(runs=50):\n",
    "    groups, group_size = torch.randint(low=2, high=100, size=(2,)).tolist()\n",
    "    t = torch.randn(groups, group_size)\n",
    "    assert (quant(t, group_size=group_size, do_pack=False)[0]==Quantizer.quantize(t, group_size=group_size, nbits=3, optimize=False, bitpack=False)[0]).all()\n",
    "    print(f'Tested for {runs} runs - Looks good ✓')\n",
    "\n",
    "test_quant_against_hqq(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b60b86-946f-4c83-9b17-e499c4ab08ee",
   "metadata": {},
   "source": [
    "Let's now adapt the dequant function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08151a7a-eb10-4ddf-832c-bce2fe0ff4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequant(data, zero, scale, shape, group_size, packed=True):\n",
    "    if packed:\n",
    "        data = unpack(data, rows=rows)\n",
    "    data = data.reshape(group_size,-1)\n",
    "    data = (data-zero)*scale\n",
    "    return data.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e87f52bf-6628-47b4-a303-8c62d1727c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.00, 1.00, 1.00, 1.00],\n",
       "        [2.14, 2.14, 2.14, 2.14],\n",
       "        [3.00, 3.00, 3.00, 3.00]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdata, z, s = quant(some_data, group_size=4, do_pack=False)\n",
    "reconstructed_data = dequant(qdata, z, s, some_data.shape, group_size=4, packed=False)\n",
    "reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c94ef12-80ba-4f12-98f0-c6a82193a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_somehow_close(some_data, reconstructed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "201614d2-b447-4c65-889b-c55e3a7a2bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested for 50 runs - Looks good ✓\n"
     ]
    }
   ],
   "source": [
    "def test_quant_dequant(runs=50):\n",
    "    for _ in range(runs):\n",
    "        groups, group_size = torch.randint(low=2, high=100, size=(2,)).tolist()\n",
    "        t = torch.randn(groups, group_size)\n",
    "        qdata, zero, scale = quant(t, group_size=group_size, do_pack=False)\n",
    "        reconstructed_t = dequant(qdata, zero, scale, t.shape, group_size=group_size, packed=False)\n",
    "        assert_somehow_close(some_data, reconstructed_data)\n",
    "    print(f'Tested for {runs} runs - Looks good ✓')\n",
    "\n",
    "test_quant_dequant(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ccc014-f040-48f8-9fe1-6577ef522b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "597ea00d-9dc9-4f15-8fd6-873355e8c0aa",
   "metadata": {},
   "source": [
    "## 3. Quant zero and scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77aab1-dbde-4c50-b59a-b38538e530d1",
   "metadata": {},
   "source": [
    "fdsp-qdora sets `quant_zero` and `quant_scale` to `True`, which after quanting the data, applies quanting to the resulting `zero` and `scale`.\n",
    "\n",
    "Let's so that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31b24c54-ee63-4378-a073-e1e28281342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current quant & dequant will be inner functions, used insie the new quant & dequant \n",
    "_quant = quant\n",
    "_dequant = dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c3981f6-54f6-4920-bf0d-7c1467675d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant(data, group_size, group_size_zero=None, group_size_scale=None, bits=3, do_pack=True):\n",
    "    group_size_zero, group_size_scale = group_size_zero or group_size, group_size_scale or group_size\n",
    "    qdata,  zero       , scale        = _quant(data,  group_size,       bits, do_pack)\n",
    "    qzero,  zeros_zero , zeros_scale  = _quant(zero,  group_size_zero,  bits, do_pack)\n",
    "    qscale, scales_zero, scales_scale = _quant(scale, group_size_scale, bits, do_pack)\n",
    "    return qdata, qzero, qscale, zeros_zero, zeros_scale, scales_zero, scales_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afefe482-2040-47d5-9dfe-042af087b7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11,  1,  7,  4],\n",
       "        [ 7,  5,  6,  2],\n",
       "        [19,  2, 10,  6],\n",
       "        [ 9, 16, 12,  5]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a different example,\n",
    "# - with shape (4,4) as example, so group_size divides rows (for data quanting) & cols (for zero/scale quanting)\n",
    "# - different columsn (otherwise zero/scales are equal, and they themselves then have scale of 1/range = 1/0) \n",
    "some_data = torch.randint(low=0, high=20, size=(4,4))\n",
    "some_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "148808ea-aa1d-454d-bae9-e19774ef075a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 0., 1., 4.],\n",
       "         [0., 2., 0., 0.],\n",
       "         [7., 0., 5., 7.],\n",
       "         [1., 7., 7., 5.]]),\n",
       " tensor([[3.],\n",
       "         [7.],\n",
       "         [0.],\n",
       "         [4.]]),\n",
       " tensor([[5.],\n",
       "         [7.],\n",
       "         [1.],\n",
       "         [0.]]),\n",
       " tensor([[7.50]]),\n",
       " tensor([[0.93]]),\n",
       " tensor([[-2.55]]),\n",
       " tensor([[0.22]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant(some_data, group_size=4, bits=3, do_pack=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c5a6d-cdc3-4d58-a05f-a515db4ca696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87fbc548-7819-4ed3-9aa9-4e71d46f744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.core.quantize import HQQLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af08277b-0bb8-4263-a74c-e053f608740a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.34,  0.32,  0.33,  0.16,  0.08, -0.03, -0.32, -0.27],\n",
       "        [-0.23, -0.09, -0.05,  0.30,  0.03, -0.10,  0.04,  0.32],\n",
       "        [-0.03, -0.04,  0.32,  0.27, -0.31, -0.13,  0.09,  0.20],\n",
       "        [-0.13,  0.05,  0.32, -0.08, -0.22,  0.30,  0.15,  0.21],\n",
       "        [-0.28, -0.02,  0.03,  0.20,  0.19,  0.26,  0.12,  0.02],\n",
       "        [-0.20,  0.08, -0.14,  0.18,  0.12, -0.27,  0.16,  0.21],\n",
       "        [-0.17, -0.14,  0.32, -0.17,  0.30, -0.27,  0.05, -0.24],\n",
       "        [-0.26,  0.28, -0.05, -0.08,  0.11, -0.06,  0.01, -0.24]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_linear = nn.Linear(8,8,bias=False) # for hqq, group_size needs to be multiple of 8\n",
    "base_linear.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba68361a-8e13-43b5-a808-0e72cb87327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hqq_cfg = dict(\n",
    "    weight_quant_params = dict(nbits=3, group_size=8, bitpack=False),\n",
    "    scale_quant_params  = dict(nbits=3, group_size=8, bitpack=False),\n",
    "    # hqq sets channel_wise=False for zeros, which makes the min/max calculation be over all the data, not per group.\n",
    "    # that's okay, because the zeros will all be in range tbd --- todo: umer\n",
    "    # we'll ignore that for now\n",
    "    zero_quant_params   = dict(nbits=3, group_size=8, bitpack=False),\n",
    "    offload_meta = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fc711bb-c8f2-4bc3-808e-99cfdc79f803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HQQLinear(in_features=8, out_features=8, bias=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hqq_linear = HQQLinear(base_linear, hqq_cfg)\n",
    "hqq_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3f2e553-b0cc-4a7d-84e3-ad827eabe1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7., 7., 7., 5., 4., 3., 0., 0.],\n",
       "        [1., 1., 1., 7., 4., 2., 5., 7.],\n",
       "        [3., 2., 7., 7., 0., 2., 6., 6.],\n",
       "        [2., 3., 7., 1., 1., 7., 7., 6.],\n",
       "        [0., 2., 3., 6., 6., 6., 6., 3.],\n",
       "        [1., 3., 0., 5., 5., 0., 7., 6.],\n",
       "        [1., 0., 7., 0., 7., 0., 5., 0.],\n",
       "        [0., 6., 1., 1., 5., 3., 5., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hqq_linear.W_q.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1636e681-86a9-472e-a2f0-f0eb489636ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = 'qdata qzero qscale zeros_zero zeros_scale scales_zero scales_scales'.split(' ')\n",
    "items_hqq = L([\n",
    "    hqq_linear.W_q.data,\n",
    "    hqq_linear.meta['zero_q'],\n",
    "    hqq_linear.meta['scale_q'],\n",
    "    hqq_linear.meta['meta_zero']['zero'],\n",
    "    hqq_linear.meta['meta_zero']['scale'],\n",
    "    hqq_linear.meta['meta_scale']['zero'],\n",
    "    hqq_linear.meta['meta_scale']['scale'],\n",
    "]).map(Self.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0af1b42c-5b3e-495e-b0a8-d6acf3ad98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_ours = quant(base_linear.weight.data, group_size=8, bits=3, do_pack=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10033f46-b6a2-4dfc-9fc3-64280289f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| Name          | HQQ Item                                   | Our Item                                   |\n",
      "+===============+============================================+============================================+\n",
      "| qdata         | tensor([[7., 7., 7., 5., 4., 3., 0., 0.],  | tensor([[7., 7., 7., 5., 4., 3., 0., 0.],  |\n",
      "|               |         [1., 1., 1., 7., 4., 2., 5., 7.],  |         [1., 1., 1., 7., 4., 2., 5., 7.],  |\n",
      "|               |         [3., 2., 7., 7., 0., 2., 6., 6.],  |         [3., 2., 7., 7., 0., 2., 6., 6.],  |\n",
      "|               |         [2., 3., 7., 1., 1., 7., 7., 6.],  |         [2., 3., 7., 1., 1., 7., 7., 6.],  |\n",
      "|               |         [0., 2., 3., 6., 6., 6., 6., 3.],  |         [0., 2., 3., 6., 6., 6., 6., 3.],  |\n",
      "|               |         [1., 3., 0., 5., 5., 0., 7., 6.],  |         [1., 3., 0., 5., 5., 0., 7., 6.],  |\n",
      "|               |         [1., 0., 7., 0., 7., 0., 5., 0.],  |         [1., 0., 7., 0., 7., 0., 5., 0.],  |\n",
      "|               |         [0., 6., 1., 1., 5., 3., 5., 0.]]) |         [0., 6., 1., 1., 5., 3., 5., 0.]]) |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| qzero         | tensor([[3.],                              | tensor([[3.],                              |\n",
      "|               |         [0.],                              |         [0.],                              |\n",
      "|               |         [0.],                              |         [0.],                              |\n",
      "|               |         [1.],                              |         [1.],                              |\n",
      "|               |         [4.],                              |         [4.],                              |\n",
      "|               |         [3.],                              |         [3.],                              |\n",
      "|               |         [7.],                              |         [7.],                              |\n",
      "|               |         [3.]], dtype=torch.float16)        |         [3.]])                             |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| qscale        | tensor([[7.],                              | tensor([[7.],                              |\n",
      "|               |         [0.],                              |         [0.],                              |\n",
      "|               |         [0.],                              |         [0.],                              |\n",
      "|               |         [0.],                              |         [0.],                              |\n",
      "|               |         [7.],                              |         [7.],                              |\n",
      "|               |         [5.],                              |         [5.],                              |\n",
      "|               |         [1.],                              |         [1.],                              |\n",
      "|               |         [6.]], dtype=torch.float16)        |         [6.]])                             |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| zeros_zero    | tensor([[-5.75]], dtype=torch.float16)     | tensor([[-5.76]])                          |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| zeros_scale   | tensor([[0.36]], dtype=torch.float16)      | tensor([[0.36]])                           |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| scales_zero   | tensor([[-20.77]], dtype=torch.float16)    | tensor([[-20.76]])                         |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| scales_scales | tensor([[0.00]], dtype=torch.float16)      | tensor([[0.00]])                           |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "table = zip(names, items_hqq, items_ours)\n",
    "print(tabulate(table, headers=['Name', 'HQQ Item', 'Our Item'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4f527ce-a713-4e6b-b02c-1efee6be3390",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Half did not match Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ohqq,oours \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(items_hqq, items_ours): assert_close(ohqq, oours)\n",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(a, b)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21massert_close\u001b[39m(a,b): \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mall()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Half did not match Float"
     ]
    }
   ],
   "source": [
    "for ohqq,oours in zip(items_hqq, items_ours): assert_close(ohqq, oours)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f510c7db-bc59-42cb-add2-b1df5f0ee388",
   "metadata": {},
   "source": [
    "**Q:** Why are some tensors in fp16?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf08b95-2375-4ed9-b8a3-ed37f7117ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c83dc2-ea68-41cf-b5d5-a39605615f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "009deed4-cb3c-4122-8d8c-70aaf0b5dcbf",
   "metadata": {},
   "source": [
    "## 4. Use hqq's optimization in quanting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21afc77a-45a7-4973-918a-e828143f7820",
   "metadata": {},
   "source": [
    "Finally, let's use hqq's optimizer in our quanting method. We won't implement it ourselves, because for our goal (making qdora faaast), it doesn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "419753ec-b939-4efe-88f9-433cca9fc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _quant(data, group_size, bits=3, do_pack=True, optimize=False):\n",
    "    assert data.numel()%group_size==0, f'group_size {group_size} can\\'t evenly split the data (numel = {data.numel()})'\n",
    "    data = data.float().reshape(group_size,-1)\n",
    "    \n",
    "    min_, max_ = data.min(axis=0, keepdim=True).values, data.max(axis=0, keepdim=True).values\n",
    "\n",
    "    scale = (2**bits-1) / (max_-min_) # note: hqq clamp to 2e4 to avoid half-precision problems, let's ignore that for now\n",
    "    zero = -min_ * scale\n",
    "\n",
    "    if optimize: data, scale, zero = Quantizer.optimize_weights(data, scale, zero, min_max=[0, 2**bits-1])\n",
    "    else: data = (data * scale + zero).round()\n",
    "\n",
    "    # packed quantized data\n",
    "    if do_pack: data = pack(data)\n",
    "\n",
    "    return data, zero, 1/scale # we invert scale, so we can do multiplication instead of division in dequanting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7321cf50-4351-4fbe-9208-6c1db6b09e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+\n",
      "| Optimized                              | Not optimized                          |\n",
      "+========================================+========================================+\n",
      "| tensor([[2., 0., 1., 4.],              | tensor([[2., 0., 1., 4.],              |\n",
      "|         [-0., 2., 0., 0.],             |         [0., 2., 0., 0.],              |\n",
      "|         [7., 0., 5., 7.],              |         [7., 0., 5., 7.],              |\n",
      "|         [1., 7., 7., 5.]])             |         [1., 7., 7., 5.]])             |\n",
      "+----------------------------------------+----------------------------------------+\n",
      "| tensor([[-4.14, -0.46, -7.00, -3.50]]) | tensor([[-4.08, -0.47, -7.00, -3.50]]) |\n",
      "+----------------------------------------+----------------------------------------+\n",
      "| tensor([[1.71, 2.14, 0.86, 0.57]])     | tensor([[1.71, 2.14, 0.86, 0.57]])     |\n",
      "+----------------------------------------+----------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(zip(\n",
    "    _quant(some_data, group_size=4, optimize=True, do_pack=False),\n",
    "    _quant(some_data, group_size=4, optimize=False, do_pack=False)\n",
    "), headers=['Optimized', 'Not optimized'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d03064-6dc8-4616-946e-916c7cff70b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71a870a6-f95a-4b25-b265-1480ae7db3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+----------------------------------------+\n",
      "| HQQ                                                         | Ours                                   |\n",
      "+=============================================================+========================================+\n",
      "| tensor([[2, 0, 1, 4],                                       | tensor([[2., 0., 1., 4.],              |\n",
      "|         [0, 2, 0, 0],                                       |         [-0., 2., 0., 0.],             |\n",
      "|         [7, 0, 5, 7],                                       |         [7., 0., 5., 7.],              |\n",
      "|         [1, 7, 7, 5]])                                      |         [1., 7., 7., 5.]])             |\n",
      "+-------------------------------------------------------------+----------------------------------------+\n",
      "| tensor([[-4.16, -0.46, -7.00, -3.50]], dtype=torch.float16) | tensor([[-4.14, -0.46, -7.00, -3.50]]) |\n",
      "+-------------------------------------------------------------+----------------------------------------+\n",
      "| tensor([[1.71, 2.14, 0.86, 0.57]], dtype=torch.float16)     | tensor([[1.71, 2.14, 0.86, 0.57]])     |\n",
      "+-------------------------------------------------------------+----------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "hqq_results = Quantizer.quantize(some_data, group_size=4, nbits=3, optimize=True, bitpack=False)\n",
    "items_hqq = [hqq_results[0], hqq_results[1]['zero'], hqq_results[1]['scale']]\n",
    "items_ours = _quant(some_data, group_size=4, optimize=True, do_pack=False)\n",
    "\n",
    "print(tabulate(zip(items_hqq, items_ours), headers=['HQQ', 'Ours'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a3099-0160-46c0-8c2b-59b03bd617c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f20faedd-d29b-47f5-b00f-799f2c49f2df",
   "metadata": {},
   "source": [
    "## 6. Put it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424887d1-dfce-4fc1-a8ae-7e634362962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantedDoraModule(nn.Module):\n",
    "    def __init__(self, linear, bits, group_size, rank, alpha, packed=True, optimized=True, group_size_zero=None, group_size_scale=None):\n",
    "        super().__init__()\n",
    "        # for quanting\n",
    "        assert base_linear.weight.numel() % group_size ==0, f'group_size {group_size} can\\'t cleanly split weight of base layer ({base_linear.weight.numel()} items)'\n",
    "        store_attr('bits,group_size,packed,optimized',self)\n",
    "        self.group_size_zero, self.group_size_scale = group_size_zero or 128, group_size_scale or 128 # hqq uses group size of 128 for zero & scale\n",
    "        self.quant(linear.weight.data, optimize)\n",
    "        # for dora\n",
    "        self.a = nn.Linear(linear.in_features, rank, bias=False, dtype=fp16)\n",
    "        self.b = nn.Linear(rank, linear.out_features, bias=False, dtype=fp16)\n",
    "        self.alpha = alpha\n",
    "        self.m = nn.Parameter(linear.weight.norm(p=2, dim=1))\n",
    "        # init a & b to 0 -- a should be inited differently, but for sake of simplicity, set it to 0 as well\n",
    "        self.a.weight.data.zero_()\n",
    "        self.b.weight.data.zero_()\n",
    "\n",
    "    @staticmethod\n",
    "    def _quant(data, group_size, bits=3, packed=True, optimize=True):\n",
    "        assert data.numel()%group_size==0, f'group_size {group_size} can\\'t evenly split the data (numel = {data.numel()})'\n",
    "        data = data.float().reshape(group_size,-1)\n",
    "        \n",
    "        min_, max_ = data.min(axis=0, keepdim=True).values, data.max(axis=0, keepdim=True).values\n",
    "    \n",
    "        scale = (2**bits-1) / (max_-min_) # note: hqq clamp to 2e4 to avoid half-precision problems, let's ignore that for now\n",
    "        zero = -min_ * scale\n",
    "    \n",
    "        if optimize: data, scale, zero = Quantizer.optimize_weights(data, scale, zero, min_max=[0, 2**bits-1])\n",
    "        else: data = (data * scale + zero).round()\n",
    "    \n",
    "        if packed: data = pack(data)\n",
    "        return data, zero, 1/scale # invert scale, so in dequanting we multiply instead of divide \n",
    "\n",
    "    @staticmethod\n",
    "    def _dequant(data, zero, scale, shape, group_size, packed=True):\n",
    "        if packed: data = self.unpack(data, rows=rows)\n",
    "        data = data.reshape(group_size,-1)\n",
    "        data = (data-zero)*scale\n",
    "        return data.reshape(shape)\n",
    "\n",
    "    def quant(self, data):\n",
    "        qdata,  zero       , scale        = self._quant(data,  self.group_size,       self.bits, self.packed, self.optimized)\n",
    "        qzero,  zeros_zero , zeros_scale  = self._quant(zero,  self.group_size_zero,  self.bits, self.packed, False)\n",
    "        qscale, scales_zero, scales_scale = self._quant(scale, self.group_size_scale, self.bits, self.packed, False)\n",
    "        store_attr('qdata, qzero, qscale, zeros_zero, zeros_scale, scales_zero, scales_scale', self)\n",
    "        self.data_shape,self.zero_shape,self.scale_shape = data.shape, zero.shape, scale.shape\n",
    "\n",
    "    # todo: test\n",
    "    def dequant(self):\n",
    "        zero  = self._dequant(self.qzero,  self.zeros_zero,  self.zeros_scale,  self.zero_shape,  self.group_size_zero,  self.packed)\n",
    "        scale = self._dequant(self.qscale, self.scales_zero, self.scales_scale, self.scale_shape, self.group_size_scale, self.packed)\n",
    "        return self._dequant(seld.qdata, zero, scale, self.group_size, self.data_shape, self.packed)\n",
    "\n",
    "    @staticmethod\n",
    "    def pack(vals):\n",
    "        assert len(vals.shape)==2, 'Pass a 2d tensor'\n",
    "        for v in vals.flatten(): assert 0<=v.item()<=7 and v.item()//1==v.item(), f'Value {v} can\\'t be represented by 3 bits or is not an integer'    \n",
    "        rows, cols = vals.shape\n",
    "        n_packs = ceil(rows/10)\n",
    "        padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "        padded_vals[:rows, :cols] = vals\n",
    "        packed = torch.zeros(n_packs, cols, dtype=int32)\n",
    "        for k in range(10): packed = (packed << 3) | padded_vals[k*n_packs:(k+1)*n_packs,:] # shift right 3 bits, then set last 3 bits to padded_vals[...,...]\n",
    "        return packed\n",
    "\n",
    "    @staticmethod\n",
    "    def unpack(packed, rows):\n",
    "        def bin_to_dec(b3,b2,b1): return 4*b3 + 2*b2 + b1\n",
    "        assert len(packed.shape)==2 and packed.dtype==int32, 'Pass a 2d tensor of int32s'\n",
    "        n_packs, cols = packed.shape\n",
    "        padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "        for k_up, k_down in zip(range(10), reversed(range(10))): # top-most 3bits vals (k_up=0) are most right-shifted (k_down=9)\n",
    "            padded_vals[k_down*n_packs:(k_down+1)*n_packs,:] = ((packed >> (3*k_up)) & 0b111) # righ-shift 3*k_up times, so last 3 bits are those we want; then only select those via 0b111            \n",
    "        return padded_vals[:rows,:]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dequant()@x + self.b(self.a(x))\n",
    "        col_norms =  (self.dequant() + self.b.weight @ self.a.weight).norm(p=2, dim=1).detach()\n",
    "        x /= col_norms\n",
    "        x *= self.m * self.alpha\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
