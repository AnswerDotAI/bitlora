{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035941a3-fd58-48e6-a43a-fea00f672b41",
   "metadata": {},
   "source": [
    "Hqq uses a slightly different packing and quanting method than `python_hqq_qdora.ipynb`. In this nb, I use that.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "0. Take the reference implementation from `python_hqq_qdora.ipynb`\n",
    "1. Implement packing as hqq does it\n",
    "2. Implement (de)quanting as hqq does it, without the optimization\n",
    "3. Quant zero and scales\n",
    "4. Use hqq's optimization in quanting\n",
    "5. Put it all toghether!\n",
    "\n",
    "Also note: I'm not using inheritance to produce cleaner code in this nb. The goal is to have a single class with all the functionality in one place. This will make it easier later to write kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225deadd-a13a-4146-bfb2-a6d91bf2b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-r94u9uy7 because the default path (/teamspace/studios/this_studio/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from torch import tensor, cat, int32, float16 as fp16, float32 as fp32\n",
    "from math import ceil\n",
    "\n",
    "from fastcore.foundation import L, Self\n",
    "from fastcore.basics import store_attr\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "from hqq.core.quantize import Quantizer\n",
    "\n",
    "torch.set_printoptions(linewidth=200, precision=2, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e8c97c-a979-482a-9930-17152bf41542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randint(n, lo=2, hi=100):\n",
    "    r = torch.randint(low=2, high=100, size=(n,)).tolist()\n",
    "    return r[0] if len(r)==1 else r\n",
    "\n",
    "def rand(*shape): # normalized random numbers, otherwise quanting error depends on size of data\n",
    "    if len(shape)==1: shape=shape[0] # allow rand(a,b) or rand(shape)\n",
    "    assert len(shape)==2\n",
    "    o,i=shape\n",
    "    return nn.Linear(i,o,bias=False).weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f82477-707c-4358-8a2d-acf9202d4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_abs_diff(a,b): return (a-b).abs().max()\n",
    "def assert_close(a,b): assert torch.isclose(a,b,atol=1e-2).all(), f'assert_close failed, max error = {max_abs_diff(a,b)}'\n",
    "def assert_somehow_close(a,b,max_err=0.12): assert torch.isclose(a,b,atol=max_err).all(), f'assert_somehow_close failed, max error = {max_abs_diff(a,b)}' # allow some error due to quanting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b020e820-d9af-474d-a124-13b6a86c8a71",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59febee5-9a96-463a-8083-a7a17d26021b",
   "metadata": {},
   "source": [
    "## 0. Current reference implementation from `python_hqq_qdora`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d16a09a1-51ab-4690-9259-5d8fcd11b247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.15,  0.42,  0.14,  0.48],\n",
       "        [-0.08,  0.32,  0.31, -0.12],\n",
       "        [-0.27, -0.45, -0.06, -0.34],\n",
       "        [-0.33,  0.01,  0.33, -0.25],\n",
       "        [ 0.29,  0.09, -0.33, -0.09]], dtype=torch.float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_linear = nn.Linear(4,5, bias=False, dtype=fp16) # ignore bias for now\n",
    "base_linear.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e4217c-eb1d-45b7-b925-2316f1a6b6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.90, -0.06,  0.36, -0.53], dtype=torch.float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_x = torch.randn(4, dtype=fp16); tst_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b6101f-7e26-47b4-9d10-858a0d4d9f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.10,  0.09, -0.06, -0.05,  0.19], dtype=torch.float16, grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_result = base_linear(tst_x); tst_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0ebbd-066a-4584-955d-a84983c1e122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8aed621c-4b19-4c64-bd01-4afac8433620",
   "metadata": {},
   "source": [
    "This is `QuantedDoraModule` from `python_hqq_qdora.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c9d7e70-f910-4f1e-a241-5577c19a9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantedDoraModule(nn.Module):\n",
    "    def __init__(self, linear, bits, group_size, rank, alpha):\n",
    "        super().__init__()\n",
    "        # for quanting\n",
    "        assert base_linear.weight.numel() % group_size ==0, f'group_size {group_size} can\\'t cleanly split weight of base layer ({base_linear.weight.numel()} items)'\n",
    "        self.bits,self.group_size, = bits,group_size\n",
    "        self.quant(linear)\n",
    "        # for dora\n",
    "        self.a = nn.Linear(linear.in_features, rank, bias=False, dtype=fp16)\n",
    "        self.b = nn.Linear(rank, linear.out_features, bias=False, dtype=fp16)\n",
    "        self.alpha = alpha\n",
    "        self.m = nn.Parameter(linear.weight.norm(p=2, dim=1))\n",
    "        # init a & b to 0 -- a should be inited differently, but for sake of simplicity, set it to 0 as well\n",
    "        self.a.weight.data.zero_()\n",
    "        self.b.weight.data.zero_()\n",
    "\n",
    "    def quant(self, linear):\n",
    "        data = linear.weight.data\n",
    "        self.shape = data.shape\n",
    "\n",
    "        # repeat last element, to have a multiple of group_size elements\n",
    "        # note: element to pad with mustn't change any attribute that's use for quanting (eg min & max in a group)\n",
    "        n_pad = data.numel()%self.group_size\n",
    "        data = F.pad(data, (0,n_pad), 'constant', data.flatten()[-1])\n",
    "        assert data.numel()%self.group_size==0\n",
    "\n",
    "        data = data.reshape(-1,self.group_size)\n",
    "        \n",
    "        min_, max_ = data.min(axis=-1, keepdim=True).values, data.max(axis=-1, keepdim=True).values\n",
    "        \n",
    "        self.zero = min_\n",
    "        self.scale = (max_-min_) / (2**self.bits-1) \n",
    "        \n",
    "        # note: can't use shorthand ops like -= as they modify tensor in-place\n",
    "        data = data - self.zero # start at 0\n",
    "        data = data / self.scale # scale to [0, 2**bits-1]\n",
    "        data = data.round().to(int)\n",
    "\n",
    "        # packed quantized data\n",
    "        self.pqdata = self.pack(data.flatten())\n",
    "\n",
    "    # pack 10 3bit values into a 32bit val\n",
    "    @staticmethod\n",
    "    def pack(vals):\n",
    "        for v in vals: assert 0<=v<=7 and v//1==v, f'Value {v} can\\'t be represented by 3 bits or is not an integer'\n",
    "        \n",
    "        n_packs = ceil(len(vals)/10)\n",
    "    \n",
    "        # pad with 0, to have a multiple of pack_size elements\n",
    "        n_pad = n_packs*10 - len(vals)\n",
    "        vals = F.pad(vals, (0,n_pad), 'constant', 0)\n",
    "        assert len(vals)==n_packs*10\n",
    "    \n",
    "        packed = torch.zeros(n_packs, dtype=int32)\n",
    "        for i in range(n_packs):\n",
    "            # pack the 10 vals from 10*i to 10*(i+1) into packed[i]\n",
    "            for x in vals[10*i:10*(i+1)]: packed[i] = (packed[i] << 3) | x # shift right 3 bits, then set last 3 bits to x\n",
    "        return packed\n",
    "\n",
    "    def dequant(self):\n",
    "        data = self.unpack(self.pqdata)[:self.shape.numel()] # unpack & remove padding that was added during packing\n",
    "        data = data.reshape(-1,self.group_size)\n",
    "        data = data*self.scale + self.zero\n",
    "        return data.reshape(self.shape)\n",
    "    \n",
    "    # unpack a 32bit value into 10 3bit vals\n",
    "    @staticmethod\n",
    "    def unpack(packed):\n",
    "        def bin_to_dec(b3,b2,b1): return 4*b3 + 2*b2 + b1\n",
    "        for v in packed: isinstance(v, int), f'Value {v} is not an integer'\n",
    "        unpacked = []\n",
    "        for pack in packed:\n",
    "            for i in reversed(range(10)):\n",
    "                unpacked.append((pack >> (3*i)) & 0b111) # righ-shift 3*i times, so last 3 bits are those we want; then only select those via 0b111            \n",
    "        return tensor(unpacked)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dequant()@x + self.b(self.a(x))\n",
    "        col_norms =  (self.dequant() + self.b.weight @ self.a.weight).norm(p=2, dim=1).detach()\n",
    "        x /= col_norms\n",
    "        x *= self.m * self.alpha\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ee2a8c-6952-4941-842a-4374b6bc6f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantedDoraModule(\n",
       "  (a): Linear(in_features=4, out_features=2, bias=False)\n",
       "  (b): Linear(in_features=2, out_features=5, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdora = QuantedDoraModule(base_linear, bits=3, group_size=5, rank=2, alpha=1); qdora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0d8899f-8246-4cdb-adf8-8d32ad10d19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quanted result (with packing): tensor([-0.08,  0.09, -0.02, -0.06,  0.17], dtype=torch.float16, grad_fn=<MulBackward0>)\n",
      "exact   result               : tensor([-0.10,  0.09, -0.06, -0.05,  0.19], dtype=torch.float16, grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "print(f'quanted result (with packing): {qdora(tst_x)}')\n",
    "print(f'exact   result               : {tst_result}')\n",
    "assert_somehow_close(qdora(tst_x), tst_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae8d1bb-74de-46eb-a72a-3ec24dabf099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32aefae7-ac82-46d7-a4e5-88938f41c460",
   "metadata": {},
   "source": [
    "## 1. Packing as in hqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b31b3-c427-40d8-80d5-a36c4e5bc5e3",
   "metadata": {},
   "source": [
    "hqq packs column-wise and interwovenly. For example, for `group_siz=3` (so 3 columns) and 2 packs per columns (ie 11-20 elements per column), the data matrix\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "d & e & f \\\\\n",
    "g & h & i \\\\\n",
    "j & k & l \\\\\n",
    "\\vdots & \\vdots & \\vdots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "is packed into:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "Pack(a,g,...) & Pack(b,h,...) & Pack(c,i,...) \\\\\n",
    "Pack(d,j,...) & Pack(e,k,...) & Pack(f,l,...) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "004c27e0-6cbd-4dc9-9e41-ef3d724919c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack(vals):\n",
    "    assert len(vals.shape)==2, 'Pass a 2d tensor'\n",
    "    for v in vals.flatten(): assert 0<=v.item()<=7 and v.item()//1==v.item(), f'Value {v} can\\'t be represented by 3 bits or is not an integer'    \n",
    "    rows, cols = vals.shape\n",
    "    n_packs = ceil(rows/10) # packs per col\n",
    "    padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "    padded_vals[:rows, :cols] = vals\n",
    "    packed = torch.zeros(n_packs, cols, dtype=int32)\n",
    "\n",
    "    for j in range(cols):\n",
    "        for i in range(n_packs):\n",
    "            for k in range(10):\n",
    "                packed[i,j] = (packed[i,j] << 3) | padded_vals[n_packs*k+i,j] # shift right 3 bits, then set last 3 bits to x\n",
    "    return packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12211d54-3dc5-4cdb-be96-ea5ad2b93fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pack = torch.ones(11,1, dtype=int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7eb9e4-2ac3-4fe3-84d3-9e046dfa25f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[153391104],\n",
       "        [153387008]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack(to_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fac2756-3228-4ed9-b68a-ce56f2d4969e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[153391104],\n",
       "        [153387008]], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hqq.core.bitpack import BitPack\n",
    "BitPack.pack_3bit_32(to_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e004b5-f9b7-4c39-96e4-d65eddd62e85",
   "metadata": {},
   "source": [
    "Matches!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d23b4-f6b1-49bb-aa0c-d250716599f3",
   "metadata": {},
   "source": [
    "We can vectorize the loops. Let's do that next. Hqq also does that.\n",
    "\n",
    "There are 3 loops: `j` over cols, `i` over packs, and `k` over 3bit-values. We'll vectorized the `j` and `i` loops. This means, in each iteration over `k`, we'll bit-shift the current packed tensor (shape `(n_packs, cols)`) 3 bits to the right, then set the left-most bits (which are all 0 now) to `vals[k*n_packs:(k+1)*n_packs,:]`, which is also shape `(n_packs, cols)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6c01a83-fc6b-4afc-b9ce-e0919c43a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack(vals):\n",
    "    assert len(vals.shape)==2, 'Pass a 2d tensor'\n",
    "    for v in vals.flatten(): assert 0<=v.item()<=7 and v.item()//1==v.item(), f'Value {v} can\\'t be represented by 3 bits or is not an integer'    \n",
    "    rows, cols = vals.shape\n",
    "    n_packs = ceil(rows/10)\n",
    "    padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "    padded_vals[:rows, :cols] = vals\n",
    "    packed = torch.zeros(n_packs, cols, dtype=int32)\n",
    "    for k in range(10): packed = (packed << 3) | padded_vals[k*n_packs:(k+1)*n_packs,:] # shift right 3 bits, then set last 3 bits to padded_vals[...,...]\n",
    "    return packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad8561e7-2cac-4387-9b02-cdebba6ecd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[153391104],\n",
       "        [153387008]], dtype=torch.int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack(to_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef32ac-f6c3-4cb2-87c8-6cb6e0fcd9ab",
   "metadata": {},
   "source": [
    "Note: hqq also 'vectorizes' the `k` loop by explicitly doing the 10 operations. We'll ignore that for simplicity's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f68b2995-d3f4-4dc5-8492-60a1fb69a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_3bit_tensor():\n",
    "    rows, cols = torch.randint(low=1, high=100, size=(2,))\n",
    "    return torch.randint(0, 7, (rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "384b84b7-2f46-49c1-b70e-86bccd741624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested for 50 runs - Looks good ✓\n"
     ]
    }
   ],
   "source": [
    "def test_pack_against_hqq(runs=50):\n",
    "    for _ in range(runs):\n",
    "        t = rand_3bit_tensor()    \n",
    "        assert (pack(t)==BitPack.pack_3bit_32(t)).all()\n",
    "    print(f'Tested for {runs} runs - Looks good ✓')\n",
    "\n",
    "test_pack_against_hqq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd2115-cff2-45d7-93a1-b1d1021957c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84c990c6-e55b-4e40-83d0-c8e189170cd9",
   "metadata": {},
   "source": [
    "Let's now adapt the unpacking function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8432401-b150-49dd-afd7-eac494040355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack a 32bit value into 10 3bit vals\n",
    "def unpack(packed, rows):\n",
    "    def bin_to_dec(b3,b2,b1): return 4*b3 + 2*b2 + b1\n",
    "    assert len(packed.shape)==2 and packed.dtype==int32, 'Pass a 2d tensor of int32s'\n",
    "    n_packs, cols = packed.shape\n",
    "    padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "    for k_up, k_down in zip(range(10), reversed(range(10))): # top-most 3bits vals (k_up=0) are most right-shifted (k_down=9)\n",
    "        padded_vals[k_down*n_packs:(k_down+1)*n_packs,:] = ((packed >> (3*k_up)) & 0b111) # righ-shift 3*k_up times, so last 3 bits are those we want; then only select those via 0b111            \n",
    "    return padded_vals[:rows,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fc5595d-f240-4adf-9594-7b3ac2a33bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(unpack(pack(to_pack), rows=11)==to_pack).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "810ca1ed-ae74-4206-b0d7-c5ebbd628522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested for 50 runs - Looks good ✓\n"
     ]
    }
   ],
   "source": [
    "def test_pack_unpack(runs=50):\n",
    "    for _ in range(runs):\n",
    "        t = rand_3bit_tensor()\n",
    "        assert (unpack(pack(t), rows=t.shape[0])==t).all()\n",
    "    print(f'Tested for {runs} runs - Looks good ✓')\n",
    "\n",
    "test_pack_unpack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3fdc5-73b0-4e17-a8b6-b558b031dbda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b4272b2-8df1-4747-8233-646a6d5701bf",
   "metadata": {},
   "source": [
    "## 2. (De)quanting as in hqq, without optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1707beaa-27c5-4f44-a4dd-3dba824bb9fb",
   "metadata": {},
   "source": [
    "Differences:\n",
    "- **data type:** hqq converts data to float.\n",
    "    - **ToDo Umer:** Why?\n",
    "- **data shape:** by default hqq reshapes to `(group_size, -1)`, while we did `(-1, group_size)`\n",
    "    - our choice corresponds to their `axis=1` setting, but their default is `axis=0`\n",
    "    - changed shapes mean we need to take the min/max now over axis 0, instead of axis 1\n",
    "- **ensuring group_size evenly splits data:** they raise Error if it doesn't, while we pad.\n",
    "- **order of `zero` & `scale`:**\n",
    "    - hqq first multiplies by `scale`, then subtracts `zero`; we did it the other way round, but we also defined `zero` & `scales` a bit differently.\n",
    "    - Both approaches are mathematically identical, but to use their `scale` / `zero`, we need to use their approach.\n",
    "    - Also, their `scale` is our inverse `scale`, but we divide by it, they multiply; and our zeros have different signs, but they add and we devide. Let's use their definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b6ee0c0-f8b6-4a8d-8693-1a732e5fbe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant(data, group_size, bits=3, packed=True):\n",
    "    assert data.numel()%group_size==0, f'group_size {group_size} can\\'t evenly split the data (numel = {data.numel()})'\n",
    "    data = data.float().reshape(group_size,-1)\n",
    "    \n",
    "    min_, max_ = data.min(axis=0, keepdim=True).values, data.max(axis=0, keepdim=True).values\n",
    "\n",
    "    scale = (2**bits-1) / (max_-min_) # note: hqq clamp to 2e4 to avoid half-precision problems, let's ignore that for now\n",
    "    zero = -min_ * scale\n",
    "    \n",
    "    data = (data * scale + zero).round()\n",
    "\n",
    "    # packed quantized data\n",
    "    if packed: data = pack(data)\n",
    "\n",
    "    return data, zero, 1/scale # we invert scale, so we can do multiplication instead of division in dequanting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d7adc8b-00dc-496d-9410-338c7ba7a00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = torch.repeat_interleave(tensor([1.,2,3]), 4).reshape(3,-1)\n",
    "some_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a44ac497-e914-42ef-9721-9f87a85a51f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 4., 4.],\n",
       "         [4., 4., 7.],\n",
       "         [7., 7., 7.]]),\n",
       " tensor([[-3.50, -3.50, -3.50]]),\n",
       " tensor([[0.29, 0.29, 0.29]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant(some_data, group_size=4, packed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19407d0e-8238-4c20-9a0a-ab1b5e0bd96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c60b93f-f8a1-4974-8983-ec02f232839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------+---------------------------------+\n",
      "| Name   | HQQ Item                        | Our Item                        |\n",
      "+========+=================================+=================================+\n",
      "| qdata  | tensor([[0., 0., 0.],           | tensor([[0., 0., 0.],           |\n",
      "|        |         [0., 4., 4.],           |         [0., 4., 4.],           |\n",
      "|        |         [4., 4., 7.],           |         [4., 4., 7.],           |\n",
      "|        |         [7., 7., 7.]])          |         [7., 7., 7.]])          |\n",
      "+--------+---------------------------------+---------------------------------+\n",
      "| zero   | tensor([[-3.50, -3.50, -3.50]]) | tensor([[-3.50, -3.50, -3.50]]) |\n",
      "+--------+---------------------------------+---------------------------------+\n",
      "| scale  | tensor([[0.29, 0.29, 0.29]])    | tensor([[0.29, 0.29, 0.29]])    |\n",
      "+--------+---------------------------------+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "hqq_results = Quantizer.quantize(some_data, group_size=4, nbits=3, optimize=False, bitpack=False)\n",
    "items_hqq = [hqq_results[0], hqq_results[1]['zero'], hqq_results[1]['scale']]\n",
    "items_ours = quant(some_data, group_size=4, packed=False)\n",
    "names = ['qdata', 'zero', 'scale']\n",
    "print(tabulate(zip(names, items_hqq, items_ours), headers=['Name', 'HQQ Item', 'Our Item'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "597d1885-9011-4585-b6e1-fa90b0cba35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ohqq,oours in zip(items_hqq, items_ours): assert (ohqq==oours).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a59623c1-8748-48d2-97ce-da7d441c0bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested for 50 runs - Looks good ✓\n"
     ]
    }
   ],
   "source": [
    "def test_quant_against_hqq(runs=50):\n",
    "    groups, group_size = torch.randint(low=2, high=100, size=(2,)).tolist()\n",
    "    t = torch.randn(groups, group_size)\n",
    "    assert (quant(t, group_size=group_size, packed=False)[0]==Quantizer.quantize(t, group_size=group_size, nbits=3, optimize=False, bitpack=False)[0]).all()\n",
    "    print(f'Tested for {runs} runs - Looks good ✓')\n",
    "\n",
    "test_quant_against_hqq(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b60b86-946f-4c83-9b17-e499c4ab08ee",
   "metadata": {},
   "source": [
    "Let's now adapt the dequant function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08151a7a-eb10-4ddf-832c-bce2fe0ff4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequant(data, zero, scale, shape, group_size, packed=True):\n",
    "    if packed: data = unpack(data, rows=rows)\n",
    "    data = (data-zero)*scale\n",
    "    return data.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e87f52bf-6628-47b4-a303-8c62d1727c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.00, 1.00, 1.00, 1.00],\n",
       "        [2.14, 2.14, 2.14, 2.14],\n",
       "        [3.00, 3.00, 3.00, 3.00]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdata, z, s = quant(some_data, group_size=4, packed=False)\n",
    "reconstructed_data = dequant(qdata, z, s, some_data.shape, group_size=4, packed=False)\n",
    "reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c94ef12-80ba-4f12-98f0-c6a82193a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_somehow_close(some_data, reconstructed_data, max_err=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "201614d2-b447-4c65-889b-c55e3a7a2bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested for 50 runs - Looks good ✓\n"
     ]
    }
   ],
   "source": [
    "def test_quant_dequant(runs=50):\n",
    "    for _ in range(runs):\n",
    "        groups, group_size = torch.randint(low=2, high=100, size=(2,)).tolist()\n",
    "        t = rand(groups, group_size)\n",
    "        qdata, zero, scale = quant(t, group_size=group_size, packed=False)\n",
    "        t_est = dequant(qdata, zero, scale, t.shape, group_size=group_size, packed=False)\n",
    "        assert_somehow_close(t, t_est)\n",
    "    print(f'Tested for {runs} runs - Looks good ✓')\n",
    "\n",
    "test_quant_dequant(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445e6ac-917b-4c22-ba17-ae3c6a47f041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15154292-6c29-41dc-9d92-2f090c96859f",
   "metadata": {},
   "source": [
    "Out of curiosity, let's plot the error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28e31107-c44b-4fe4-933f-93f093010172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_reconstruction_error(runs=50):\n",
    "    err = []\n",
    "    for _ in range(runs):\n",
    "        group_size = torch.randint(low=2, high=100, size=(1,)).item()\n",
    "        t = rand(group_size, group_size)\n",
    "        qdata, zero, scale = quant(t, group_size=group_size, packed=False)\n",
    "        t_est = dequant(qdata, zero, scale, t.shape, group_size=group_size, packed=False)\n",
    "        err.extend((t-t_est).abs().flatten().tolist())\n",
    "    return err\n",
    "\n",
    "plt.hist(get_reconstruction_error(), bins=50)\n",
    "plt.title('Reconstruction error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d973b-e361-4600-835a-0b60b36b588e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ccc014-f040-48f8-9fe1-6577ef522b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "597ea00d-9dc9-4f15-8fd6-873355e8c0aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Quant zero and scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77aab1-dbde-4c50-b59a-b38538e530d1",
   "metadata": {},
   "source": [
    "fdsp-qdora sets `quant_zero` and `quant_scale` to `True`, which after quanting the data, applies quanting to the resulting `zero` and `scale`.\n",
    "\n",
    "Let's so that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31b24c54-ee63-4378-a073-e1e28281342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current quant & dequant will be inner functions, used inside the new quant & dequant \n",
    "_quant = quant\n",
    "_dequant = dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c3981f6-54f6-4920-bf0d-7c1467675d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant(data, group_size, group_size_zero=None, group_size_scale=None, bits=3, packed=True):\n",
    "    group_size_zero, group_size_scale = group_size_zero or group_size, group_size_scale or group_size\n",
    "    qdata,  zero       , scale        = _quant(data,  group_size,       bits, packed)\n",
    "    qzero,  zeros_zero , zeros_scale  = _quant(zero,  group_size_zero,  bits, packed)\n",
    "    qscale, scales_zero, scales_scale = _quant(scale, group_size_scale, bits, packed)\n",
    "    return qdata, qzero, qscale, zeros_zero, zeros_scale, scales_zero, scales_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afefe482-2040-47d5-9dfe-042af087b7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18, 19, 12, 10],\n",
       "        [11, 18, 15, 14],\n",
       "        [11, 10,  6, 11],\n",
       "        [ 1,  4,  6, 17]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a different example,\n",
    "# - with shape (4,4) as example, so group_size divides rows (for data quanting) & cols (for zero/scale quanting)\n",
    "# - different columsn (otherwise zero/scales are equal, and they themselves then have scale of 1/range = 1/0) \n",
    "some_data = torch.randint(low=0, high=20, size=(4,4))\n",
    "some_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "148808ea-aa1d-454d-bae9-e19774ef075a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7., 7., 5., 0.],\n",
       "         [4., 7., 7., 4.],\n",
       "         [4., 3., 0., 1.],\n",
       "         [0., 0., 0., 7.]]),\n",
       " tensor([[7.],\n",
       "         [6.],\n",
       "         [4.],\n",
       "         [0.]]),\n",
       " tensor([[7.],\n",
       "         [6.],\n",
       "         [1.],\n",
       "         [0.]]),\n",
       " tensor([[7.30]]),\n",
       " tensor([[1.37]]),\n",
       " tensor([[-4.90]]),\n",
       " tensor([[0.20]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant(some_data, group_size=4, bits=3, packed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c5a6d-cdc3-4d58-a05f-a515db4ca696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87fbc548-7819-4ed3-9aa9-4e71d46f744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hqq.core.quantize import HQQLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af08277b-0bb8-4263-a74c-e053f608740a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.07,  0.16,  0.02,  0.18,  0.10, -0.34, -0.22,  0.17],\n",
       "        [-0.11,  0.28, -0.09, -0.17, -0.26, -0.19,  0.32,  0.33],\n",
       "        [-0.15,  0.21,  0.04, -0.12, -0.15,  0.12,  0.34, -0.01],\n",
       "        [-0.13,  0.25,  0.05,  0.16,  0.10, -0.35,  0.12, -0.19],\n",
       "        [-0.32, -0.18,  0.02, -0.29,  0.08,  0.06,  0.22, -0.09],\n",
       "        [-0.29,  0.27,  0.17,  0.10, -0.35,  0.27, -0.06,  0.20],\n",
       "        [-0.04, -0.02,  0.07, -0.27, -0.06, -0.15, -0.23,  0.04],\n",
       "        [-0.10, -0.32,  0.32, -0.17, -0.20,  0.16, -0.07, -0.33]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_linear = nn.Linear(8,8,bias=False) # for hqq, group_size needs to be multiple of 8\n",
    "base_linear.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba68361a-8e13-43b5-a808-0e72cb87327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hqq_cfg = dict(\n",
    "    weight_quant_params = dict(nbits=3, group_size=8, bitpack=False, optimize=False),\n",
    "    scale_quant_params  = dict(nbits=3, group_size=8, bitpack=False, optimize=False),\n",
    "    # hqq sets channel_wise=False for zeros, which makes the min/max calculation be over all the data, not per group.\n",
    "    # that's okay, because the zeros will all be in range tbd --- todo: umer\n",
    "    # we'll ignore that for now\n",
    "    zero_quant_params   = dict(nbits=3, group_size=8, bitpack=False, optimize=False),\n",
    "    offload_meta = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fc711bb-c8f2-4bc3-808e-99cfdc79f803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HQQLinear(in_features=8, out_features=8, bias=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hqq_linear = HQQLinear(base_linear, hqq_cfg, compute_dtype=fp32) # for simplicity, use fp32; default is fp16\n",
    "hqq_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3f2e553-b0cc-4a7d-84e3-ad827eabe1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7., 6., 2., 7., 7., 0., 0., 5.],\n",
       "        [4., 7., 0., 2., 1., 2., 7., 7.],\n",
       "        [3., 6., 2., 3., 3., 5., 7., 3.],\n",
       "        [3., 7., 2., 7., 7., 0., 4., 1.],\n",
       "        [0., 2., 2., 0., 7., 5., 6., 2.],\n",
       "        [1., 7., 4., 6., 0., 7., 2., 6.],\n",
       "        [5., 3., 3., 0., 4., 2., 0., 4.],\n",
       "        [4., 0., 7., 2., 2., 6., 2., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hqq_linear.W_q.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1636e681-86a9-472e-a2f0-f0eb489636ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = 'qdata qzero qscale zeros_zero zeros_scale scales_zero scales_scales'.split(' ')\n",
    "items_hqq = L([\n",
    "    hqq_linear.W_q.data,\n",
    "    hqq_linear.meta['zero_q'],\n",
    "    hqq_linear.meta['scale_q'],\n",
    "    hqq_linear.meta['meta_zero']['zero'],\n",
    "    hqq_linear.meta['meta_zero']['scale'],\n",
    "    hqq_linear.meta['meta_scale']['zero'],\n",
    "    hqq_linear.meta['meta_scale']['scale'],\n",
    "]).map(Self.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0af1b42c-5b3e-495e-b0a8-d6acf3ad98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_ours = quant(base_linear.weight.data, group_size=8, bits=3, packed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10033f46-b6a2-4dfc-9fc3-64280289f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| Name          | HQQ Item                                   | Our Item                                   |\n",
      "+===============+============================================+============================================+\n",
      "| qdata         | tensor([[7., 6., 2., 7., 7., 0., 0., 5.],  | tensor([[7., 6., 2., 7., 7., 0., 0., 5.],  |\n",
      "|               |         [4., 7., 0., 2., 1., 2., 7., 7.],  |         [4., 7., 0., 2., 1., 2., 7., 7.],  |\n",
      "|               |         [3., 6., 2., 3., 3., 5., 7., 3.],  |         [3., 6., 2., 3., 3., 5., 7., 3.],  |\n",
      "|               |         [3., 7., 2., 7., 7., 0., 4., 1.],  |         [3., 7., 2., 7., 7., 0., 4., 1.],  |\n",
      "|               |         [0., 2., 2., 0., 7., 5., 6., 2.],  |         [0., 2., 2., 0., 7., 5., 6., 2.],  |\n",
      "|               |         [1., 7., 4., 6., 0., 7., 2., 6.],  |         [1., 7., 4., 6., 0., 7., 2., 6.],  |\n",
      "|               |         [5., 3., 3., 0., 4., 2., 0., 4.],  |         [5., 3., 3., 0., 4., 2., 0., 4.],  |\n",
      "|               |         [4., 0., 7., 2., 2., 6., 2., 0.]]) |         [4., 0., 7., 2., 2., 6., 2., 0.]]) |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| qzero         | tensor([[7.],                              | tensor([[7.],                              |\n",
      "|               |         [4.],                              |         [4.],                              |\n",
      "|               |         [0.],                              |         [0.],                              |\n",
      "|               |         [5.],                              |         [5.],                              |\n",
      "|               |         [6.],                              |         [6.],                              |\n",
      "|               |         [4.],                              |         [4.],                              |\n",
      "|               |         [2.],                              |         [2.],                              |\n",
      "|               |         [3.]])                             |         [3.]])                             |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| qscale        | tensor([[0.],                              | tensor([[0.],                              |\n",
      "|               |         [5.],                              |         [5.],                              |\n",
      "|               |         [1.],                              |         [1.],                              |\n",
      "|               |         [2.],                              |         [2.],                              |\n",
      "|               |         [2.],                              |         [2.],                              |\n",
      "|               |         [6.],                              |         [6.],                              |\n",
      "|               |         [5.],                              |         [5.],                              |\n",
      "|               |         [7.]])                             |         [7.]])                             |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| zeros_zero    | tensor([[-2.40]])                          | tensor([[-2.40]])                          |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| zeros_scale   | tensor([[0.62]])                           | tensor([[0.62]])                           |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| scales_zero   | tensor([[-10.18]])                         | tensor([[-10.18]])                         |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n",
      "| scales_scales | tensor([[0.01]])                           | tensor([[0.01]])                           |\n",
      "+---------------+--------------------------------------------+--------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "table = zip(names, items_hqq, items_ours)\n",
    "print(tabulate(table, headers=['Name', 'HQQ Item', 'Our Item'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4f527ce-a713-4e6b-b02c-1efee6be3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ohqq,oours in zip(items_hqq, items_ours): assert_close(ohqq, oours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf08b95-2375-4ed9-b8a3-ed37f7117ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5c83dc2-ea68-41cf-b5d5-a39605615f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested for 50 runs - Looks good ✓\n"
     ]
    }
   ],
   "source": [
    "def test_double_quant_against_hqq(runs=50):\n",
    "    for _ in range(runs):\n",
    "        group_size = torch.randint(low=2, high=100, size=(1,)).item()\n",
    "        lin = nn.Linear(group_size, group_size, bias=False)\n",
    "\n",
    "        cfg = deepcopy(hqq_cfg)\n",
    "        for o in ['weight', 'scale', 'zero']: cfg[o+'_quant_params']['group_size'] = group_size\n",
    "        \n",
    "        hqq_lin = HQQLinear(lin, cfg, compute_dtype=fp32) # for simplicity, use fp32; default is fp16)\n",
    "        \n",
    "        names = 'qdata qzero qscale zeros_zero zeros_scale scales_zero scales_scales'.split(' ')\n",
    "        items_hqq = L([\n",
    "            hqq_lin.W_q.data,\n",
    "            hqq_lin.meta['zero_q'],\n",
    "            hqq_lin.meta['scale_q'],\n",
    "            hqq_lin.meta['meta_zero']['zero'],\n",
    "            hqq_lin.meta['meta_zero']['scale'],\n",
    "            hqq_lin.meta['meta_scale']['zero'],\n",
    "            hqq_lin.meta['meta_scale']['scale'],\n",
    "        ]).map(Self.cpu())\n",
    "        items_ours = quant(lin.weight.data, group_size=group_size, bits=3, packed=False)\n",
    "        \n",
    "        for o1,o2 in zip(items_hqq, items_ours): assert_close(o1,o2)\n",
    "    print(f'Tested for {runs} runs - Looks good ✓')\n",
    "\n",
    "test_double_quant_against_hqq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb651637-599a-449c-8e7d-4964c6fac706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c11b0b78-0f00-4ec4-8e88-fedab897755b",
   "metadata": {},
   "source": [
    "Let's now adapt the dequant function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b20f1854-fa34-45c0-9bf9-9a15c4cc2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequant(qdata, qzero, qscale, zeros_zero, zeros_scale, scales_zero, scales_scale, data_shape, zero_shape, scale_shape, group_size, group_size_zero, group_size_scale, packed):\n",
    "    zero  = _dequant(qzero,  zeros_zero,  zeros_scale,  zero_shape,  group_size_zero,  packed)\n",
    "    scale = _dequant(qscale, scales_zero, scales_scale, scale_shape, group_size_scale, packed)\n",
    "    return _dequant(qdata, zero, scale, data_shape, group_size, packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa3d054a-f94d-421b-8073-6a2b9f793018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group size: 39\n"
     ]
    }
   ],
   "source": [
    "gz = randint(1, lo=2, hi=100) # group_size\n",
    "print(f'Group size: {gz}')\n",
    "data = rand(gz, gz)\n",
    "items = quant(data, gz, packed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75c529-dce7-40fd-b599-aba9d60d11c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e1cbaf5c-a640-42ed-ab9a-63a6302a7ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_est = dequant(\n",
    "    *items,\n",
    "    (gz,gz), (1,gz), (1,gz),\n",
    "    gz, gz, gz,\n",
    "    packed=False\n",
    ")\n",
    "\n",
    "assert_somehow_close(data, W_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d14810-3251-485b-b762-528543375bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ac3885a-4d72-484c-8be8-00b07f4d482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested for 50 runs - Looks good ✓\n"
     ]
    }
   ],
   "source": [
    "def test_quant_dequant(runs=50):\n",
    "    for _ in range(runs):\n",
    "        gz = randint(1, lo=2, hi=100)\n",
    "        t = rand(gz, gz)\n",
    "        items = quant(t, gz, packed=False)        \n",
    "        t_est = dequant(\n",
    "            *items,\n",
    "            (gz,gz), (1,gz), (1,gz), # shapes\n",
    "            gz, gz, gz, # group sizes\n",
    "            packed=False\n",
    "        )\n",
    "        assert_somehow_close(t, t_est)\n",
    "    print(f'Tested for {runs} runs - Looks good ✓')\n",
    "\n",
    "test_quant_dequant(50) # error is random, and can rarely be a bit larger, so if this fails, rerun in and verify this fails only rarely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41efc1-0059-4160-b460-9c808625940b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0b780-4802-45e7-a202-73dd31487af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "009deed4-cb3c-4122-8d8c-70aaf0b5dcbf",
   "metadata": {},
   "source": [
    "## 4. Use hqq's optimization in quanting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21afc77a-45a7-4973-918a-e828143f7820",
   "metadata": {},
   "source": [
    "Finally, let's use hqq's optimizer in our quanting method. We won't implement it ourselves, because for our goal (making qdora faaast), it doesn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "419753ec-b939-4efe-88f9-433cca9fc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _quant(data, group_size, bits=3, packed=True, optimize=False):\n",
    "    assert data.numel()%group_size==0, f'group_size {group_size} can\\'t evenly split the data (numel = {data.numel()})'\n",
    "    data = data.float().reshape(group_size,-1)\n",
    "    \n",
    "    min_, max_ = data.min(axis=0, keepdim=True).values, data.max(axis=0, keepdim=True).values\n",
    "\n",
    "    scale = (2**bits-1) / (max_-min_) # note: hqq clamp to 2e4 to avoid half-precision problems, let's ignore that for now\n",
    "    zero = -min_ * scale\n",
    "\n",
    "    if optimize: data, scale, zero = Quantizer.optimize_weights(data, scale, zero, min_max=[0, 2**bits-1])\n",
    "    else: data = (data * scale + zero).round()\n",
    "\n",
    "    # packed quantized data\n",
    "    if packed: data = pack(data)\n",
    "\n",
    "    return data, zero, 1/scale # we invert scale, so we can do multiplication instead of division in dequanting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7321cf50-4351-4fbe-9208-6c1db6b09e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+--------------------------------------------+\n",
      "| Optimized                                  | Not optimized                              |\n",
      "+============================================+============================================+\n",
      "| tensor([[7., 7., 5., 0.],                  | tensor([[7., 7., 5., 0.],                  |\n",
      "|         [4., 7., 7., 4.],                  |         [4., 7., 7., 4.],                  |\n",
      "|         [4., 3., 0., 1.],                  |         [4., 3., 0., 1.],                  |\n",
      "|         [-0., 0., 0., 7.]])                |         [0., 0., 0., 7.]])                 |\n",
      "+--------------------------------------------+--------------------------------------------+\n",
      "| tensor([[ -0.46,  -1.83,  -4.63, -10.00]]) | tensor([[ -0.41,  -1.87,  -4.67, -10.00]]) |\n",
      "+--------------------------------------------+--------------------------------------------+\n",
      "| tensor([[2.43, 2.14, 1.29, 1.00]])         | tensor([[2.43, 2.14, 1.29, 1.00]])         |\n",
      "+--------------------------------------------+--------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(zip(\n",
    "    _quant(some_data, group_size=4, optimize=True, packed=False),\n",
    "    _quant(some_data, group_size=4, optimize=False, packed=False)\n",
    "), headers=['Optimized', 'Not optimized'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d03064-6dc8-4616-946e-916c7cff70b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71a870a6-f95a-4b25-b265-1480ae7db3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------+--------------------------------------------+\n",
      "| HQQ                                                             | Ours                                       |\n",
      "+=================================================================+============================================+\n",
      "| tensor([[7, 7, 5, 0],                                           | tensor([[7., 7., 5., 0.],                  |\n",
      "|         [4, 7, 7, 4],                                           |         [4., 7., 7., 4.],                  |\n",
      "|         [4, 3, 0, 1],                                           |         [4., 3., 0., 1.],                  |\n",
      "|         [0, 0, 0, 7]])                                          |         [-0., 0., 0., 7.]])                |\n",
      "+-----------------------------------------------------------------+--------------------------------------------+\n",
      "| tensor([[ -0.46,  -1.83,  -4.63, -10.00]], dtype=torch.float16) | tensor([[ -0.46,  -1.83,  -4.63, -10.00]]) |\n",
      "+-----------------------------------------------------------------+--------------------------------------------+\n",
      "| tensor([[2.43, 2.14, 1.29, 1.00]], dtype=torch.float16)         | tensor([[2.43, 2.14, 1.29, 1.00]])         |\n",
      "+-----------------------------------------------------------------+--------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "hqq_results = Quantizer.quantize(some_data, group_size=4, nbits=3, optimize=True, bitpack=False)\n",
    "items_hqq = [hqq_results[0], hqq_results[1]['zero'], hqq_results[1]['scale']]\n",
    "items_ours = _quant(some_data, group_size=4, optimize=True, packed=False)\n",
    "\n",
    "print(tabulate(zip(items_hqq, items_ours), headers=['HQQ', 'Ours'], tablefmt='grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a3099-0160-46c0-8c2b-59b03bd617c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f20faedd-d29b-47f5-b00f-799f2c49f2df",
   "metadata": {},
   "source": [
    "## 6. Put it all together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79639e0b-9fad-449a-9532-069f9df4f193",
   "metadata": {},
   "source": [
    "Put everything together, and added `compute_dtype` param, so `self.dequant()@x` in `forward` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "424887d1-dfce-4fc1-a8ae-7e634362962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantedDoraModule(nn.Module):\n",
    "    def __init__(self, linear, bits, group_size, rank, alpha, compute_dtype=fp16, packed=True, optimized=True, group_size_zero=None, group_size_scale=None):\n",
    "        super().__init__()\n",
    "        # for quanting\n",
    "        store_attr('bits,group_size,packed,optimized,compute_dtype',self)\n",
    "        self.group_size_zero, self.group_size_scale = group_size_zero or 128, group_size_scale or 128 # hqq uses group size of 128 for zero & scale\n",
    "        self.quant(linear.weight.data)\n",
    "        # for dora\n",
    "        self.a = nn.Linear(linear.in_features, rank, bias=False, dtype=fp16)\n",
    "        self.b = nn.Linear(rank, linear.out_features, bias=False, dtype=fp16)\n",
    "        self.alpha = alpha\n",
    "        self.m = nn.Parameter(linear.weight.norm(p=2, dim=1))\n",
    "        # init a & b to 0 -- a should be inited differently, but for sake of simplicity, set it to 0 as well\n",
    "        self.a.weight.data.zero_()\n",
    "        self.b.weight.data.zero_()\n",
    "\n",
    "    @staticmethod\n",
    "    def pack(vals):\n",
    "        assert len(vals.shape)==2, 'Pass a 2d tensor'\n",
    "        for v in vals.flatten(): assert 0<=v.item()<=7 and v.item()//1==v.item(), f'Value {v} can\\'t be represented by 3 bits or is not an integer'    \n",
    "        rows, cols = vals.shape\n",
    "        n_packs = ceil(rows/10)\n",
    "        padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "        padded_vals[:rows, :cols] = vals\n",
    "        packed = torch.zeros(n_packs, cols, dtype=int32)\n",
    "        for k in range(10): packed = (packed << 3) | padded_vals[k*n_packs:(k+1)*n_packs,:] # shift right 3 bits, then set last 3 bits to padded_vals[...,...]\n",
    "        return packed\n",
    "\n",
    "    @staticmethod\n",
    "    def unpack(packed, rows):\n",
    "        def bin_to_dec(b3,b2,b1): return 4*b3 + 2*b2 + b1\n",
    "        assert len(packed.shape)==2 and packed.dtype==int32, 'Pass a 2d tensor of int32s'\n",
    "        n_packs, cols = packed.shape\n",
    "        padded_vals = torch.zeros(n_packs*10, cols, dtype=int32)\n",
    "        for k_up, k_down in zip(range(10), reversed(range(10))): # top-most 3bits vals (k_up=0) are most right-shifted (k_down=9)\n",
    "            padded_vals[k_down*n_packs:(k_down+1)*n_packs,:] = ((packed >> (3*k_up)) & 0b111) # righ-shift 3*k_up times, so last 3 bits are those we want; then only select those via 0b111            \n",
    "        return padded_vals[:rows,:]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _quant(data, group_size, bits=3, packed=True, optimize=True):\n",
    "        assert data.numel()%group_size==0, f'group_size {group_size} can\\'t evenly split the data (numel = {data.numel()})'\n",
    "        data = data.float().reshape(group_size,-1)\n",
    "        \n",
    "        min_, max_ = data.min(axis=0, keepdim=True).values, data.max(axis=0, keepdim=True).values\n",
    "    \n",
    "        scale = (2**bits-1) / (max_-min_) # note: hqq clamp to 2e4 to avoid half-precision problems, let's ignore that for now\n",
    "        zero = -min_ * scale\n",
    "    \n",
    "        if optimize: data, scale, zero = Quantizer.optimize_weights(data, scale, zero, min_max=[0, 2**bits-1])\n",
    "        else: data = (data * scale + zero).round()\n",
    "\n",
    "        if packed: data = QuantedDoraModule.pack(data)\n",
    "        return data, zero, 1/scale # invert scale, so in dequanting we multiply instead of divide \n",
    "\n",
    "    @staticmethod\n",
    "    def _dequant(data, zero, scale, shape, group_size, packed=True):\n",
    "        if packed: data = QuantedDoraModule.unpack(data, rows=group_size)\n",
    "        data = (data-zero)*scale\n",
    "        return data.reshape(shape)\n",
    "\n",
    "    def quant(self, data):\n",
    "        qdata,  zero       , scale        = self._quant(data,  self.group_size,       self.bits, self.packed, self.optimized)\n",
    "        qzero,  zeros_zero , zeros_scale  = self._quant(zero,  self.group_size_zero,  self.bits, self.packed, False)\n",
    "        qscale, scales_zero, scales_scale = self._quant(scale, self.group_size_scale, self.bits, self.packed, False)\n",
    "        store_attr('qdata, qzero, qscale, zeros_zero, zeros_scale, scales_zero, scales_scale', self)\n",
    "        self.data_shape,self.zero_shape,self.scale_shape = data.shape, zero.shape, scale.shape\n",
    "\n",
    "    def dequant(self):\n",
    "        zero  = self._dequant(self.qzero,  self.zeros_zero,  self.zeros_scale,  self.zero_shape,  self.group_size_zero,  self.packed)\n",
    "        scale = self._dequant(self.qscale, self.scales_zero, self.scales_scale, self.scale_shape, self.group_size_scale, self.packed)\n",
    "        return  self._dequant(self.qdata,  zero,             scale,             self.data_shape,  self.group_size,       self.packed).to(self.compute_dtype)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dequant()@x + self.b(self.a(x))\n",
    "        col_norms =  (self.dequant() + self.b.weight @ self.a.weight).norm(p=2, dim=1).detach()\n",
    "        x /= col_norms\n",
    "        x *= self.m * self.alpha\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59ac3bac-f6d5-471d-8e5d-de2ccd65c9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.18,  0.16,  0.32,  0.20,  0.01, -0.23, -0.26, -0.13],\n",
       "        [ 0.18, -0.15,  0.21,  0.12,  0.22,  0.15, -0.02, -0.28],\n",
       "        [ 0.17,  0.18,  0.13, -0.17, -0.12,  0.21, -0.06,  0.17],\n",
       "        [-0.22, -0.21, -0.28,  0.27,  0.13,  0.15,  0.30, -0.27],\n",
       "        [ 0.08,  0.18,  0.01, -0.01,  0.06, -0.12, -0.26,  0.02],\n",
       "        [-0.01,  0.13, -0.29,  0.09, -0.08, -0.19,  0.27, -0.15],\n",
       "        [-0.07, -0.09,  0.27, -0.08, -0.18, -0.01,  0.20,  0.06],\n",
       "        [ 0.12,  0.29, -0.31,  0.13, -0.03,  0.03,  0.13, -0.25]], dtype=torch.float16)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = nn.Linear(8,8, bias=False, dtype=fp16)\n",
    "lin.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "977f22a5-b113-4385-b410-847c0979bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdora_lin = QuantedDoraModule(lin, bits=3, group_size=8, rank=2, alpha=1, packed=True, optimized=True, group_size_zero=8, group_size_scale=8, compute_dtype=fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "26b6c57a-469a-45b2-b587-91b4741491d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    -0.17,      0.13,      0.33,      0.20,     -0.01,     -0.23,     -0.27,     -0.15],\n",
       "        [     0.17,     -0.15,      0.24,      0.14,      0.22,      0.14,     -0.03,     -0.28],\n",
       "        [     0.17,      0.20,      0.15,     -0.17,     -0.12,      0.20,     -0.11,      0.19],\n",
       "        [    -0.23,     -0.23,     -0.30,      0.26,      0.11,      0.14,      0.29,     -0.28],\n",
       "        [     0.06,      0.20,      0.06,      0.02,      0.05,     -0.11,     -0.27,      0.05],\n",
       "        [     0.00,      0.13,     -0.30,      0.08,     -0.07,     -0.17,      0.29,     -0.15],\n",
       "        [    -0.06,     -0.08,      0.24,     -0.11,     -0.18,      0.01,      0.21,      0.05],\n",
       "        [     0.11,      0.27,     -0.30,      0.14,     -0.01,      0.01,      0.13,     -0.28]], dtype=torch.float16)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdora_lin.dequant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "517fbda9-8943-4aed-bda7-4d1d3c7a8782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.05, dtype=torch.float16)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_abs_diff(lin.weight.data, qdora_lin.dequant())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1eed8d-5edb-4b77-80f3-9d6be8dfa4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df2c6dfd-bd7f-42ae-8a88-99dacfdb5ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.33, -0.27,  0.04, -0.31,  0.14, -0.01,  0.10, -0.14], dtype=torch.float16, grad_fn=<SqueezeBackward4>)\n",
      "tensor([ 0.31, -0.24,  0.06, -0.32,  0.17, -0.03,  0.07, -0.15], dtype=torch.float16, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(8, dtype=fp16)\n",
    "y = lin(x)\n",
    "y_qdora = qdora_lin(x)\n",
    "print(f'{y}\\n{y_qdora}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "40cc11ab-4611-4afa-88fe-7799c974495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_somehow_close(y, y_qdora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb73573-6849-427d-b487-4c41739e7091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af3df67-0edb-4a69-84e4-45e7b18a600a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
